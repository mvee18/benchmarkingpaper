{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.ncbi.names import split_bio, map_and_add_tax_ids, standardize_core, generate_names_df, names_db_path, find_tax_id\n",
    "from typing import Tuple\n",
    "from utils.data_paths import *\n",
    "\n",
    "taxonomy_dict = {\"s\": \"species\", \"g\": \"genus\", \"f\": \"family\", \"o\": \"order\", \"c\": \"class\", \"p\": \"phylum\", \"k\": \"kingdom\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = {\"Clostridium_clostridioforme\": \"Clostridium_clostridiiforme\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a csv file with genus, relative abundance.\n",
    "def clean_biobakery_with_taxid(df, rank=\"g\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    data = df.copy()\n",
    "\n",
    "    # Get the indices so we can split then. They are of the form ex: ...|g__Bacteroides|s__Bacteroides_vulgatus\n",
    "    indices = data.index.to_list()\n",
    "    # Tax_ids are of the form #|#|# ...\n",
    "    tax_ids = data[\"TAX_ID\"].to_list()\n",
    "\n",
    "    # This makes a list of lists, where each sublist is the taxonomy split by the rank.\n",
    "    splitted = [i.split(\"|\") for i in indices]\n",
    "    splitted_ids = [i.split(\"|\") for i in tax_ids]\n",
    "\n",
    "    new_index = []\n",
    "    new_ids = []\n",
    "    # Traverse all of the rows.\n",
    "    for c, i in enumerate(splitted):\n",
    "        # Traverse the sublists.\n",
    "        for c2, j in enumerate(i):\n",
    "            # If it matches the rank we want, then append it to the new index.\n",
    "            if j.startswith(f\"{rank}__\"):\n",
    "                new_index.append(j.replace(f\"{rank}__\", \"\"))\n",
    "                # The taxid is the same index as the taxonomy.\n",
    "                new_ids.append(splitted_ids[c][c2])\n",
    "\n",
    "    # Set the new index.\n",
    "    data.index = new_index\n",
    "\n",
    "    # Make a new dataframe with the new index and the new_ids. We will use this to merge since we do not want to sum the tax_ids.\n",
    "    taxid_df = pd.DataFrame(index=new_index, data=new_ids, columns=[\"TAX_ID\"])\n",
    "\n",
    "    # Sum the rows where the genus/species is the same.\n",
    "    grouped = data.groupby(data.index).sum(numeric_only=True)\n",
    "\n",
    "    # Now, we want to rename the columns to be the sample names, since they are of the form sampleid_###.\n",
    "    columns = grouped.columns.to_list()\n",
    "    new_cols = [i.split(\"_\")[0] for i in columns]\n",
    "    grouped.columns = new_cols\n",
    "\n",
    "    # Divide all the values by 100.\n",
    "    grouped = grouped / 100\n",
    "\n",
    "    # We should now remove duplicates in the taxIDs (since genera can be equivalent). \n",
    "    # If there are duplicates, then we should make it equal to the first value.\n",
    "    taxid_df = taxid_df[~taxid_df.index.duplicated(keep=\"first\")]\n",
    "\n",
    "    return grouped, taxid_df\n",
    "\n",
    "# We want to save to csv, but we want a csv for each column.\n",
    "def save_to_csv(df, taxid_df, output_path, rank=\"s\"):\n",
    "    # Get the columns.\n",
    "    columns = df.columns.to_list()\n",
    "\n",
    "    # Iterate over the columns.\n",
    "    for c, i in enumerate(columns):\n",
    "        # Get the column.\n",
    "        col = df[[i]]\n",
    "\n",
    "        # Join the tax_ids to the column.\n",
    "        # Now, we want to add the new_ids to the dataframe by joining on the index.\n",
    "        col = col.join(taxid_df, how=\"left\")\n",
    "\n",
    "        # We need to multiply each column by the reciprocal of the minimum non-zero value.\n",
    "\n",
    "        # Get the minimum non-zero value.\n",
    "        minimum = col[col > 0].min().min()\n",
    "        col[i] = col[i] / minimum\n",
    "\n",
    "        # Round to closest integer.\n",
    "        col[i] = col[i].round()\n",
    "\n",
    "        display(col.head())\n",
    "\n",
    "        # Save to csv.\n",
    "        # col.to_csv(os.path.join(output_path, f\"{i.upper()}_{taxonomy_dict[rank]}_relabund_annotated.csv\"), index_label=f\"{taxonomy_dict[rank]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In their great wisdom, they decided that the taxid should not be incldued in the file. \n",
    "# Therefore, we will need to standardize the names and attach the taxids.\n",
    "def clean_biobakery_merged(df: pd.DataFrame, rank=\"g\"):\n",
    "    data = df.copy()\n",
    "\n",
    "    # Get the indices so we can split then. They are of the form ex: ...|g__Bacteroides|s__Bacteroides_vulgatus\n",
    "    indices = data.index.to_list()\n",
    "\n",
    "    # This makes a list of lists, where each sublist is the taxonomy split by the rank.\n",
    "    splitted = [i.split(\"|\") for i in indices]\n",
    "\n",
    "    new_index = []\n",
    "    # Traverse all of the rows.\n",
    "    for c, i in enumerate(splitted):\n",
    "        # Traverse the sublists.\n",
    "        for c2, j in enumerate(i):\n",
    "            # If it matches the rank we want, then append it to the new index.\n",
    "            if j.startswith(f\"{rank}__\"):\n",
    "                new_index.append(j.replace(f\"{rank}__\", \"\"))\n",
    "\n",
    "    # Set the new index.\n",
    "    data.index = new_index\n",
    "\n",
    "    # Sum the rows where the genus/species is the same.\n",
    "    grouped = data.groupby(data.index).sum(numeric_only=True)\n",
    "\n",
    "    # Now, we want to rename the columns to be the sample names, since they are of the form sampleid_###.\n",
    "    columns = grouped.columns.to_list()\n",
    "    new_cols = [i.split(\"_\")[0] for i in columns]\n",
    "    grouped.columns = new_cols\n",
    "\n",
    "    # Divide all the values by 100.\n",
    "    grouped = grouped / 100\n",
    "\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_merged(df: pd.DataFrame):\n",
    "    \"\"\" \n",
    "    This function takes the merged dataframe (converted to RA with clean_biobakery_merged) \\  \n",
    "    and standardizes the names plus adds the taxids.\n",
    "    \"\"\"\n",
    "    names_df = generate_names_df(names_db_path, load_pickle=True)\n",
    "\n",
    "    # Get the index. Replace names with those in replacement_dict.\n",
    "    index = df.index.to_list()\n",
    "    index = [replacement_dict[i] if i in replacement_dict else i for i in index]\n",
    "    df.index = index\n",
    "\n",
    "    split_names = split_bio(df)\n",
    "\n",
    "    standard_df = standardize_core(input_df=df, split_names=split_names)\n",
    "\n",
    "    # Replace the split_name using the replacement_dict.\n",
    "    standard_df.set_index(\"split_name\", inplace=True)\n",
    "\n",
    "    annotated = map_and_add_tax_ids(df=standard_df, names_df=names_df)\n",
    "\n",
    "    # We want to split the tax_id column off the dataframe.\n",
    "    taxid_df = annotated[[\"tax_id\"]]\n",
    "\n",
    "    # If any rows contain nan, then we raise an error.\n",
    "    if taxid_df.isna().any().any():\n",
    "        # Print the rows that contain nan.\n",
    "        print(taxid_df[taxid_df.isna().any(axis=1)])\n",
    "        raise ValueError(\"There are nan values in the tax_id column.\")\n",
    "\n",
    "    annotated.drop(columns=[\"tax_id\"], inplace=True)\n",
    "\n",
    "    return annotated, taxid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code\n",
    "Run the below function for cleaning of biobakery. Change rank for the desired rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = amos_hilo.biobakery4\n",
    "# data_path = \"pipelines/bmock12/biobakery4/species_relab.txt\"\n",
    "output_dir = \"pipelines/amos/hilo/bio4\"\n",
    "if not os.path.exists(data_path):\n",
    "    raise Exception(\"Data file does not exist!\")\n",
    "# data_path = \"pipelines/bmock12/biobakery4/species_relab.txt\"\n",
    "data = pd.read_csv(data_path, sep=\"\\t\", index_col=0)\n",
    "# display(data.head())\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank = \"s\"\n",
    "# names, taxids = clean_biobakery_with_taxid(data, rank=rank)\n",
    "# save_to_csv(names, taxids, output_dir, rank=rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(rank):\n",
    "    data = pd.read_csv(data_path, sep=\"\\t\", index_col=0)\n",
    "    cleaned = clean_biobakery_merged(data, rank=rank)\n",
    "\n",
    "    annotated, ids = standardize_merged(cleaned)\n",
    "    save_to_csv(annotated, ids, output_path=output_dir, rank=rank)\n",
    "\n",
    "main(\"g\")\n",
    "main(\"s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.pipeline_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ac10c2973d938bf4f101ae2299756abbb7e00dac649f0769819439ff384650d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
