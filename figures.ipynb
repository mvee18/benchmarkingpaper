{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from typing import Tuple, List\n",
            "from python_src.compositions import clr\n",
            "from dataclasses import dataclass\n",
            "from scipy.stats import pearsonr\n",
            "from scipy.spatial.distance import braycurtis, euclidean\n",
            "from skbio.stats.composition import multiplicative_replacement\n",
            "from python_src.figures_utils import get_all_expected, generate_experimental_df, get_relabund_files, fully_combined\n",
            "from python_src.compositions import multiplicative_aitchison, uniform_replace_zeroes\n",
            "import os\n",
            "from scipy.special import rel_entr\n",
            "from sklearn.metrics import mean_squared_error\n",
            "from sklearn.linear_model import LinearRegression\n",
            "from matplotlib.backends.backend_pdf import PdfPages\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import seaborn as sns\n",
            "import matplotlib.pyplot as plt\n",
            "plt.rcParams.update({'figure.max_open_warning': 0})"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Global Variables"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Change this depending on your data.\n",
            "# taxonomic_rank = 'Species'\n",
            "# base_path = \"pipelines/camisimGI/bio4\"\n",
            "# rel_abund_file = \"s1_genus_relabund.csv\"\n",
            "# program_name = \"Biobakery4\"\n",
            "# sample_id = \"s1\"\n",
            "\n",
            "# expected_input = \"pipelines/camisimGI/s1_expected.csv\"\n",
            "%xmode Plain\n",
            "pdf_output = PdfPages(\"figures.pdf\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Colors For Seaborn and MatplotLib"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "cb_palette = sns.color_palette(as_cmap=True)\n",
            "\n",
            "color_palette = {\n",
            "    \"Expected\": cb_palette[0],\n",
            "    \"expected\": cb_palette[0],\n",
            "    \"woltka\": cb_palette[1],\n",
            "    \"wol\": cb_palette[1],\n",
            "    \"jams\": cb_palette[2],\n",
            "    \"wgsa\": cb_palette[3],\n",
            "    \"wgsa2\": cb_palette[3],\n",
            "    \"biobakery3\": cb_palette[4],\n",
            "    \"bio3\": cb_palette[4],\n",
            "    \"biobakery4\": cb_palette[5],\n",
            "    \"bio4\": cb_palette[5],\n",
            "    \"jams202212\": cb_palette[6],\n",
            "    \"sunbeam\": cb_palette[7],\n",
            "}\n",
            "\n",
            "print(color_palette)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# One Sample, One Expected"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def generate_expected(input_path: str, plot: bool = False, rank: str = \"Genus\") -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Parameters:\n",
            "        input_path: str \n",
            "            Path to the expected abundance file.\n",
            "        plot: bool\n",
            "            Whether to plot the expected abundances.\n",
            "        rank: str\n",
            "            The taxonomic rank to use. Default is \"Genus\".\n",
            "    Returns:\n",
            "        expected: pd.DataFrame\n",
            "            The expected abundances in a dataframe. The format is | rank | abundance |.\n",
            "\n",
            "    Generates the expected result from the tsv file. Returns a dataframe. THIS IS ONLY FOR BMOCK12.\n",
            "    \"\"\"\n",
            "    # expected = pd.read_csv(input_path, sep=',', index_col=0, names=['Organism', 'Counts', 'TAX_ID'], header=0)\n",
            "    expected = pd.read_csv(input_path, sep=',', index_col=0, names=[\n",
            "                           'Organism', 'Counts'], header=0)\n",
            "    expected = expected[['Counts']].astype(int)\n",
            "\n",
            "    # Calculate expected relative abundance. If it's already in RA, this won't change anything.\n",
            "    expected['RA'] = expected['Counts'] / expected['Counts'].sum()\n",
            "\n",
            "    if rank == \"Species\":\n",
            "        expected.sort_values(by=['RA'], ascending=False, inplace=True)\n",
            "        return expected\n",
            "\n",
            "    # Let's split the organism index into two columns to find the genera.\n",
            "    orgs = expected.index.to_list()\n",
            "    genus = [org.strip().split(' ')[0] for org in orgs]\n",
            "    # genus = [x.replace('M.', 'Micromonospora') for x in genus]\n",
            "\n",
            "    # Apparently, propionibacterium have been renamed to cutibacterium.\n",
            "    # genus = [x.replace('Propionibact.', 'Cutibacterium') for x in genus]\n",
            "\n",
            "    # Add the columns to the dataframe.\n",
            "    expected['Genus'] = genus\n",
            "    # display(expected.head(12))\n",
            "\n",
            "    # Group by genus and sum the counts for overlapping genera.\n",
            "    exp_genus = expected.groupby('Genus').sum()\n",
            "    exp_genus.sort_values('RA', ascending=False, inplace=True)\n",
            "\n",
            "    if plot:\n",
            "        exp_genus.plot.bar(y='RA', figsize=(8, 5), legend=False,\n",
            "                           title='Expected Relative Abundance')\n",
            "\n",
            "    return exp_genus\n",
            "\n",
            "# Use generate_expected to generate the expected result for the bmock12 data.\n",
            "# exp_genus = generate_expected('pipelines/bmock12/s1_raw_expanded.csv', False, \"Species\")\n",
            "# exp_genus[\"RA\"].to_csv('pipelines/bmock12/s1_expected_species.csv', index_label=\"Species\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# For camisim, we can just read the csv in directly.\n",
            "# exp_genus = pd.read_csv(expected_input, index_col=0, names=['Genus', 'RA'], header=0)\n",
            "# exp_genus = exp_genus.where(exp_genus['RA'] > 0.001).dropna()\n",
            "\n",
            "# (exp_genus.where(exp_genus['RA'] > 0.001).dropna()).to_csv('pipelines/camisimGI/s2_genus_pretty.csv', index_label=\"Genus\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Instead, let's concat the two dataframes into long format and add a column from where it originated.\n",
            "def long_format(df1, df2):\n",
            "    merged = pd.concat([df1, df2], axis=0)\n",
            "\n",
            "    # !!! This is slick\n",
            "    merged['Source'] = ['Expected'] * len(df1) + ['Observed'] * len(df2)\n",
            "\n",
            "    return merged\n",
            "\n",
            "# result_genus = generate_experimental_df(os.path.join(base_path, rel_abund_file), taxonomic_rank)\n",
            "# display(result_genus.head(12))\n",
            "# merged_lf = long_format(exp_genus, result_genus)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Plotting Tools"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def linear_regression_plot(df: pd.DataFrame, exp_df: pd.DataFrame, plot: bool = False, save_path: str = None) -> pd.DataFrame:\n",
            "    # Merge on the genus key for easy plotting. The expected results are on the left, the observed on the right.\n",
            "    # linear_df = exp_genus.merge(df, left_index=True, right_index=True)\n",
            "\n",
            "    # try join because the left merge will drop the genera that are not in the observerd results, but we want to show that the experimental missed it.\n",
            "    linear_df = exp_df.join(df, how='left', lsuffix='_x', rsuffix='_y')\n",
            "    linear_df.fillna(0, inplace=True)\n",
            "\n",
            "    # Linear regression with scikit.\n",
            "    X = linear_df['RA_x'].values.reshape(-1, 1)\n",
            "    Y = linear_df['RA_y'].values.reshape(-1, 1)\n",
            "    reg = LinearRegression().fit(X, Y)\n",
            "    y_pred = reg.predict(X)\n",
            "\n",
            "    # Scatter plot of RA_x vs. RA_y.\n",
            "    if plot:\n",
            "        fig = plt.figure(figsize=(10, 8))\n",
            "        plt.scatter(linear_df['RA_x'], linear_df['RA_y'], color='black')\n",
            "\n",
            "        # Calculate mean absolute error.\n",
            "        mae = np.mean(np.abs(linear_df['RA_x'] - linear_df['RA_y']))\n",
            "\n",
            "        # Regression line.\n",
            "        plt.plot(X, y_pred, color='red', linewidth=2)\n",
            "        # Labels.\n",
            "        plt.xlabel('Expected Relative Abundance')\n",
            "        plt.ylabel('Observed Relative Abundance')\n",
            "        plt.title(\n",
            "            f'Expected vs. Observed Relative Abundance for {program_name} {taxonomic_rank}')\n",
            "\n",
            "        # Add r^2 value.\n",
            "        plt.text(0.1, 0.9, f'r^2 = {reg.score(X,Y):.4f}',\n",
            "                 transform=plt.gca().transAxes)\n",
            "        # Add MAE.\n",
            "        plt.text(0.1, 0.85, f'MAE = {mae:.4f}', transform=plt.gca().transAxes)\n",
            "\n",
            "        # Add line y = x.\n",
            "        plt.plot([0, 1], [0, 1], color='blue', linewidth=2, linestyle='--')\n",
            "\n",
            "        if save_path is not None:\n",
            "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
            "\n",
            "        # plt.show()\n",
            "\n",
            "    return linear_df\n",
            "\n",
            "# linear_regression(result_genus, exp_genus, plot=True, save_path=os.path.join(base_path, f\"{sample_id}_bivariate_{taxonomic_rank}.png\"))\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def fix_x_labels(ax, df, rank):\n",
            "    xticks = ax.get_xticklabels()\n",
            "    # print(xticks)\n",
            "    new_labels = []\n",
            "    for x in xticks:\n",
            "        res = df.loc[int(x.get_text()), rank]\n",
            "        # Get only the first row from the series.\n",
            "        # This is necessary because if it is unique, it will return a string, but if it is not unique, it will return a series.\n",
            "        if isinstance(res, pd.Series):\n",
            "            res = res.iloc[0]\n",
            "        new_labels.append(res)\n",
            "\n",
            "    return new_labels\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Bar plot of RA_x vs. RA_y side by side.\n",
            "# sns.set_style(\"whitegrid\")\n",
            "def bar_plot(df: pd.DataFrame, plot: bool = False, save_path: str = None, program: str = \"\", title: str = f'INSERT TITLE', hue_category: str = 'Source', taxonomic_rank: str = None, threshold: float = 5e-5):\n",
            "    # df.to_csv(\"debug.csv\", index=True)\n",
            "    subset = pd.DataFrame()\n",
            "\n",
            "    # Used to subset by taxonomic rank, but now TAX_ID?\n",
            "    # Need to test further.\n",
            "    for x, y in df.groupby(\"TAX_ID\"):\n",
            "        if y[\"Source\"].values[0] == \"Expected\":\n",
            "            subset = pd.concat([y, subset], axis=0)\n",
            "\n",
            "    # display(subset.head())\n",
            "    subset = subset.where(subset['RA'] > threshold).dropna()\n",
            "    # subset.to_csv(\"subset.csv\", index=True)\n",
            "\n",
            "    if plot:\n",
            "        # Plot a category bar chart with the colors based on the source.\n",
            "        fig = plt.figure(figsize=(10, 8))\n",
            "        ax = sns.barplot(x=subset.index, y='RA', hue=hue_category,\n",
            "                         data=subset, errorbar=None, palette=color_palette)\n",
            "        ax.semilogy()\n",
            "\n",
            "        ax.bar_label(ax.containers[0], fmt='%.2e', label_type='center')\n",
            "        # ax.bar_label(ax.containers[1], fmt='%.2e', label_type='edge')\n",
            "\n",
            "        # Change the x axis labels from the index to the taxonomic rank.\n",
            "        ax.set_xticklabels(fix_x_labels(\n",
            "            ax=ax, df=subset, rank=taxonomic_rank), rotation=45, horizontalalignment='right')\n",
            "\n",
            "        # Later, we can fix the y labels.\n",
            "        # fix_y_labels(ax)\n",
            "\n",
            "        ax.set_title(title)\n",
            "        ax.set_xlabel(taxonomic_rank)\n",
            "        ax.set_ylabel('Relative Abundance')\n",
            "\n",
            "        if save_path is not None:\n",
            "            pdf_output.savefig(fig, dpi=300, bbox_inches='tight')\n",
            "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
            "            plt.close()\n",
            "\n",
            "        plt.show()\n",
            "\n",
            "# bar_plot(merged_lf, plot=True, save_path=os.path.join(base_path, f\"{sample_id}_bars_{taxonomic_rank}.png\"))\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Multisample\n",
            "## Utilities"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def fix_outer_join_df(df: pd.DataFrame, source: str, threshold: float = 0.001) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Parameters:\n",
            "        df: The dataframe to fix.\n",
            "        source: The source of the dataframe.\n",
            "        threshold: The threshold to use for the relative abundance.\n",
            "\n",
            "    Returns:\n",
            "        pd.Dataframe: The fixed dataframe.\n",
            "\n",
            "    This function will fill the missing RA_expected, RA_observed, and Source_observed columns with 0s and the source name. \\\\ \n",
            "    Then, we select rows only where the observed values or the expected values are greater than the threshold. \\\\\n",
            "    \"\"\"\n",
            "    # print(\"the threshold is \", threshold)\n",
            "    df['RA_expected'].fillna(0, inplace=True)\n",
            "    df['RA_observed'].fillna(0, inplace=True)\n",
            "    df['Source_observed'].fillna(source, inplace=True)\n",
            "\n",
            "    # Now, filter any rows with 'RA_observed' less than threshold if the corresponding RA_expected value is 0.\n",
            "    # df = df.loc[(df['RA_expected'] > 0) | (df['RA_observed'] > threshold)]\n",
            "\n",
            "    # # If the RA_expected is greater than zero, if the RA_observed is less than the threshold, then set the RA_observed to 0.\n",
            "    # df.loc[(\n",
            "    #     (df['RA_observed'] < threshold) &\n",
            "    #     (df['RA_expected'] > threshold) &\n",
            "    #     (abs(df['RA_observed'] - df['RA_expected']) > df['RA_expected'] * 0.10)), 'RA_observed'] = 0\n",
            "\n",
            "    # Just filter everything.\n",
            "    # TODO: THIS IS UNCLEAR IF WE SHOULD DO THIS.\n",
            "    new_df_filter = df.loc[(df['RA_observed'] > threshold)\n",
            "                           | (df['RA_expected'] > 0.0)]\n",
            "\n",
            "    return new_df_filter\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Renormalize"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def renormalize_df(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Parameters:\n",
            "        df: The dataframe to renormalize.\n",
            "\n",
            "    Returns:\n",
            "        pd.DataFrame: The renormalized dataframe.\n",
            "\n",
            "    This function will renormalize the relative abundance. \\\\\n",
            "    Specify the column name to renormalize.\n",
            "    \"\"\"\n",
            "    # Renormalize the experimental dataframes.\n",
            "    df[column_name] = df[column_name] / df[column_name].sum()\n",
            "\n",
            "    return df\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Join functions for the bivariate plots"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def join_left(expected: pd.DataFrame, source_df: pd.DataFrame, source_name: str, sampleID: str) -> pd.DataFrame:\n",
            "    \"\"\" \n",
            "    Parameters:\n",
            "        expected: The expected dataframe.\n",
            "        source_df: The source/experimental dataframe.\n",
            "        source_name: The name of the source.\n",
            "        sampleID: The sampleID of the source (can be None).\n",
            "    Returns:\n",
            "        pd.DataFrame: The joined dataframe.\n",
            "\n",
            "    This function will join the expected dataframe to the source dataframe by left join. \\\\\n",
            "    Then, it will fill the missing observed values with 0s and the source name, since there can be no missing expected values. \\\\.\n",
            "\n",
            "    \"\"\"\n",
            "    exp_copy = expected.copy()\n",
            "    src_copy = source_df.copy()\n",
            "\n",
            "    # Only select where the expected RA is greater than 0.\n",
            "    exp_copy = exp_copy.loc[exp_copy['RA'] > 0]\n",
            "\n",
            "    merged = pd.merge(exp_copy, src_copy, on=\"TAX_ID\",\n",
            "                      how='left', suffixes=('_expected', '_observed'))\n",
            "    merged['RA_observed'].fillna(0, inplace=True)\n",
            "    merged['Source_observed'].fillna(source_name, inplace=True)\n",
            "    if sampleID is not None:\n",
            "        merged['SampleID'] = sampleID\n",
            "\n",
            "    return merged\n",
            "\n",
            "\n",
            "def join_left_replicates(expected: pd.DataFrame, source_df: pd.DataFrame, source_name: str, rank: str) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Parameters:\n",
            "        expected: The expected dataframe.\n",
            "        source_df: The source/experimental dataframe.\n",
            "        source_name: The name of the source (pipeline name).\n",
            "        rank: The rank of the dataframe.\n",
            "    Returns:\n",
            "        pd.DataFrame: The joined dataframe.\n",
            "\n",
            "    This function will join the expected dataframe to the source dataframe by left join. \\\\\n",
            "    However, for replicates, the missing observed values will be assigned the sampleID from the group it's missing from. \\\\\n",
            "    This is necessary because previous the groupby function would ignore these without sampleIDs.\n",
            "    \"\"\"\n",
            "    # We want to join the expected dataframe to the source dataframe by left join.\n",
            "    # However, we want to group by SampleID first and join only those with the same SampleID.\n",
            "    # Then, we want to fill the missing observed values with 0s and the source name, since there can be no missing expected values.\n",
            "\n",
            "    exp_copy = expected.copy()\n",
            "    src_copy = source_df.copy()\n",
            "\n",
            "    final_df = pd.DataFrame()\n",
            "\n",
            "    # Only select where the expected RA is greater than 0.\n",
            "    exp_copy = exp_copy.loc[exp_copy['RA'] > 0]\n",
            "\n",
            "    # Group by SampleID and join only those with the same SampleID.\n",
            "    for sampleID, src_df in src_copy.groupby('SampleID'):\n",
            "        # Subset the sampleID in the source dataframe.\n",
            "        merged = pd.merge(exp_copy, src_df, on=\"TAX_ID\",\n",
            "                          how='left', suffixes=('_expected', '_observed'))\n",
            "        merged['RA_observed'].fillna(0, inplace=True)\n",
            "        merged['Source_observed'].fillna(source_name, inplace=True)\n",
            "        merged['SampleID_observed'].fillna(sampleID, inplace=True)\n",
            "\n",
            "        # Find rows with NaN genus, and fill with the corresponding genus from the expected dataframe by querying the TAX_ID.\n",
            "        null = merged.loc[merged[rank].isna()]\n",
            "\n",
            "        # Find the corresponding genus from the expected dataframe by querying the TAX_ID.\n",
            "        for index, row in null.iterrows():\n",
            "            name = exp_copy.loc[exp_copy['TAX_ID'] == row['TAX_ID']].index[0]\n",
            "            merged.at[index, rank] = name\n",
            "\n",
            "        final_df = pd.concat([final_df, merged], axis=0)\n",
            "\n",
            "    return final_df\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def check_and_fill_missing(merged_df: pd.DataFrame, expected_df: pd.DataFrame, col: str):\n",
            "    \"\"\"This function will check if there are any missing values in the \"genus\" or \"species\" columns.\n",
            "    If there are, it will fill them with the corresponding genus or species from the expected dataframe.\n",
            "    \"\"\"\n",
            "    # Find rows with NaN genus, and fill with the corresponding genus from the expected dataframe by querying the TAX_ID.\n",
            "    null = merged_df.loc[merged_df[col].isna()]\n",
            "\n",
            "    # Find the corresponding genus from the expected dataframe by querying the TAX_ID.\n",
            "    for index, row in null.iterrows():\n",
            "        name = expected_df.loc[expected_df['TAX_ID'] == row['TAX_ID']].index[0]\n",
            "        merged_df.at[index, col] = name\n",
            "\n",
            "    return merged_df\n",
            "\n",
            "\n",
            "def check_and_fill_missing_one_to_one(merged_df: pd.DataFrame, expected_df: pd.DataFrame, col: str):\n",
            "    \"\"\"This function will check if there are any missing values in the \"genus\" or \"species\" columns.\n",
            "    If there are, it will fill them with the corresponding genus or species from the expected dataframe.\n",
            "\n",
            "    This is different than the above function because this expected has the TAXID as the index.\n",
            "    \"\"\"\n",
            "    # Find rows with NaN genus, and fill with the corresponding genus from the expected dataframe by querying the TAX_ID.\n",
            "    null = merged_df.loc[merged_df[col].isna()]\n",
            "    rank = col.split(\"_\")[0]\n",
            "\n",
            "    # Find the corresponding genus from the expected dataframe by querying the TAX_ID.\n",
            "    for index, row in null.iterrows():\n",
            "        name = expected_df.loc[index][rank]\n",
            "        merged_df.at[index, col] = name\n",
            "\n",
            "    return merged_df\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def join_outer(expected: pd.DataFrame, source_df: pd.DataFrame, source_name: str, threshold: float, replicates: bool) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    This function will join the expected dataframe to the source dataframe by outer join. \\\\\n",
            "    Then it will call fix_outer_join_df to fix the missing values and filter out the rows with RA_expected less than the threshold.\n",
            "    See: :func:`fix_outer_join_df`\n",
            "\n",
            "    ### Parameters:\n",
            "        expected: The expected dataframe.\n",
            "        source_df: The source/experimental dataframe.\n",
            "        source: The name of the source.\n",
            "        sample: The sampleID of the source (can be None).\n",
            "        threshold: The threshold to use for the relative abundance.\n",
            "        replicates: Whether or not the source is replicates. If so, the renormalization will occur by sampleID.\n",
            "    ### Returns:\n",
            "        pd.DataFrame: The joined dataframe.\n",
            "    \"\"\"\n",
            "\n",
            "    exp_copy = expected.copy()\n",
            "    src_copy = source_df.copy()\n",
            "\n",
            "    # THIS RENORMALIZATION NEEDS TO OCCUR BY SAMPLEID.\n",
            "    if replicates:\n",
            "        replicate_df = pd.DataFrame()\n",
            "        for smpl, smpl_df in src_copy.groupby('SampleID'):\n",
            "            merged = pd.merge(exp_copy, smpl_df, on=\"TAX_ID\",\n",
            "                              how='outer', suffixes=('_expected', '_observed'))\n",
            "            merged = fix_outer_join_df(\n",
            "                merged, source_name, threshold=threshold)\n",
            "            merged = renormalize_df(merged, 'RA_observed')\n",
            "            merged = renormalize_df(merged, 'RA_expected')\n",
            "\n",
            "            # Fill in any missing sampleID_observed values.\n",
            "            merged['SampleID_observed'].fillna(smpl, inplace=True)\n",
            "\n",
            "            replicate_df = pd.concat([replicate_df, merged], axis=0)\n",
            "        try:\n",
            "            replicate_df = check_and_fill_missing(\n",
            "                replicate_df, expected, 'genus')\n",
            "        except KeyError:\n",
            "            replicate_df = check_and_fill_missing(\n",
            "                replicate_df, expected, 'species')\n",
            "\n",
            "        # display(replicate_df.head())\n",
            "        replicate_df.set_index('TAX_ID', inplace=True)\n",
            "        return replicate_df\n",
            "\n",
            "    else:\n",
            "        merged = pd.merge(exp_copy, src_copy, on=\"TAX_ID\",\n",
            "                          how='outer', suffixes=('_expected', '_observed'))\n",
            "\n",
            "        merged = fix_outer_join_df(merged, source_name, threshold=threshold)\n",
            "\n",
            "        merged = renormalize_df(merged, 'RA_observed')\n",
            "        merged = renormalize_df(merged, 'RA_expected')\n",
            "\n",
            "        try:\n",
            "            merged = check_and_fill_missing_one_to_one(\n",
            "                merged, expected, 'Genus_observed')\n",
            "        except KeyError:\n",
            "            merged = check_and_fill_missing_one_to_one(\n",
            "                merged, expected, 'Species_observed')\n",
            "\n",
            "        return merged\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Multisample with Different Expected Values"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def plot_by_sample(root, expected_root, output_dir, taxonomic_rank, hue_category='Source', threshold=5e-5):\n",
            "    \"\"\"\n",
            "    Parameters:\n",
            "        root: The root directory of the data.\n",
            "        output_dir: The output directory to save the plots.\n",
            "        taxonomic_rank: The taxonomic rank to plot.\n",
            "        hue_category: The category to use for the hue.\n",
            "        threshold: The threshold to use for the relative abundance. Values below this will not be plotted in the bar plot.\n",
            "\n",
            "    This function will plot the relative abundance by sample for the specified taxonomic rank in bar format.\n",
            "    \"\"\"\n",
            "    full_df = fully_combined(root, expected_root, rank=taxonomic_rank)\n",
            "\n",
            "    experiment_name = os.path.basename(root)\n",
            "    # print(root, experiment_name)\n",
            "\n",
            "    for sample_id, df in full_df.groupby('SampleID'):\n",
            "        title = f\"Expected vs. Observed Relative Abundance for {sample_id} in Experiment {experiment_name} ({taxonomic_rank})\"\n",
            "        # display(df.head())\n",
            "        if output_dir is not None:\n",
            "            bar_plot(df, plot=True, save_path=os.path.join(\n",
            "                output_dir, f\"{sample_id}_bars_{taxonomic_rank}_all.png\"), title=title, taxonomic_rank=taxonomic_rank, hue_category=hue_category, threshold=threshold)\n",
            "        else:\n",
            "            bar_plot(df, plot=True, save_path=None, title=title,\n",
            "                     taxonomic_rank=taxonomic_rank, hue_category=hue_category, threshold=threshold)\n",
            "\n",
            "# plot_by_sample(root_dir, hue_category='SampleID')\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def linear_regression(X, Y):\n",
            "    \"\"\"\n",
            "    Parameters:\n",
            "        X: The independent variable.\n",
            "        Y: The dependent variable.\n",
            "    Returns:\n",
            "        reg: The linear regression model.\n",
            "        y_pred: The predicted values.\n",
            "\n",
            "    Performs the linear regression on the data using the sklearn library.\n",
            "\n",
            "    \"\"\"\n",
            "    reg = LinearRegression().fit(X, Y)\n",
            "    y_pred = reg.predict(X)\n",
            "\n",
            "    return reg, y_pred"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def non_clr_plot(input_df: pd.DataFrame, hue_category: str, colors: dict, title: str):\n",
            "    \"\"\" Plots the non-CLR data in a bivariate plot. Annotates with r^2 value. \"\"\"\n",
            "    lin_df = input_df.copy()\n",
            "    display(lin_df)\n",
            "    # Drop the rows with 0s in the RA_expected.\n",
            "    lin_df = lin_df[lin_df['RA_expected'] > 0]\n",
            "\n",
            "    lin_df.to_csv(\"lin_df.csv\", index=True)\n",
            "\n",
            "    ax_lin = sns.lmplot(x=\"RA_expected\", y=\"RA_observed\", hue=hue_category, col=hue_category,\n",
            "                        col_wrap=3, data=lin_df, fit_reg=False, height=7, aspect=1, ci=None, palette=colors)\n",
            "\n",
            "    # Add the R^2 value to the plot.\n",
            "    def annotate(data, **kws):\n",
            "        max_x = data['RA_expected'].max()\n",
            "        max_y = data['RA_observed'].max()\n",
            "        max_val = max(max_x, max_y)\n",
            "\n",
            "        x = data[\"RA_expected\"].values.reshape(-1, 1)\n",
            "        y = data[\"RA_observed\"].values.reshape(-1, 1)\n",
            "\n",
            "        reg, y_pred = linear_regression(y, x)\n",
            "        r2 = reg.score(y, x)\n",
            "\n",
            "        mae = np.mean(np.abs(y - x))\n",
            "\n",
            "        ax = plt.gca()\n",
            "\n",
            "        ax.text(.05, .8, 'r2={:.4f}'.format(r2),\n",
            "                transform=ax.transAxes)\n",
            "\n",
            "        # Get the data from the plot.\n",
            "        ax.plot([0, max_val], [0, max_val], color='black', linestyle='--')\n",
            "\n",
            "        # Make it log scale.\n",
            "\n",
            "    ax_lin.map_dataframe(annotate)\n",
            "\n",
            "    ax_lin.fig.suptitle(title, fontsize=16, y=1.05)\n",
            "    plt.close(ax_lin.fig)\n",
            "    pdf_output.savefig(ax_lin.figure, bbox_inches='tight', dpi=300)\n",
            "\n",
            "    # Make this a log-log plot.\n",
            "    # ax.set(xscale=\"log\", yscale=\"log\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def calc_amos_stats(df: pd.DataFrame) -> Tuple[float, float]:\n",
            "    \"\"\" Calculates the Amos statistics (sensitivity and false positive rate) for the given dataframe. \"\"\"\n",
            "    # This is the part that could be changed to keep in line with the 0.01% threshold.\n",
            "    def outer_sensitivity(df: pd.DataFrame):\n",
            "        \"\"\"Sensitivity: (correctly identified / total) * 100\"\"\"\n",
            "        # The correct identified is where both the RA_expected and RA_observed are greater than THRESHOLD (currently 0).\n",
            "        correct = df[(df['RA_expected'] > 0) & (\n",
            "            df['RA_observed'] > 0)].shape[0]\n",
            "\n",
            "        # The total is the number of rows where the RA_expected is greater than 0.\n",
            "        total = df[df['RA_expected'] > 0].shape[0]\n",
            "\n",
            "        # print(f\"Correct: {correct}, Total: {total}\")\n",
            "\n",
            "        return (correct / total) * 100\n",
            "\n",
            "    def outer_join_FPRA(df: pd.DataFrame) -> Tuple[float, float, float]:\n",
            "        \"\"\"False Positive Rate: abundance of incorrectly identified taxa / total abundance of all taxa. Unclassified are not included in the total abundance.\"\"\"\n",
            "        # For the outer join, the incorrectly identified taxa are the rows where the RA_expected is 0.\n",
            "        # We want to sum up the RA_observed column for these rows.\n",
            "        incorrect = df[df['RA_expected'] == 0]['RA_observed']\n",
            "\n",
            "        num_incorrect = incorrect.shape[0]\n",
            "\n",
            "        incorrect_sum = incorrect.sum()\n",
            "\n",
            "        # Try to find a row with TAXID == 12908\n",
            "        unclassified: float = None\n",
            "        try:\n",
            "            unclassified = df.loc[12908]['RA_observed']\n",
            "        except KeyError:\n",
            "            unclassified = 0\n",
            "\n",
            "        # The unclassified abundance should be subtracted from this total, if it exists.\n",
            "        # While it is not expected, it is a different category than the incorrectly identified.\n",
            "        # This is so that we can agree with the pipeline_plotting scripts.\n",
            "        incorrect_sum -= unclassified\n",
            "\n",
            "        # The total may not always be 1. The total is the sum of the RA_observed column.\n",
            "        result = (incorrect_sum / df['RA_observed'].sum()) * 100\n",
            "        return result, unclassified, num_incorrect\n",
            "\n",
            "    fpra, unclassified, num_incorrect = outer_join_FPRA(df)\n",
            "\n",
            "    return outer_sensitivity(df), fpra, unclassified, num_incorrect\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_min_and_max_values(df: pd.DataFrame) -> Tuple[float, float]:\n",
            "    \"\"\"\n",
            "    Gets the minimum and maximum values from the dataframe along the RA_expected_clr and RA_observed_clr columns. \\\\\n",
            "    This is used to set the limits of the plot in the CLR space.\n",
            "    \"\"\"\n",
            "    # Get the max x and max y values.\n",
            "    max_x = df['RA_expected_clr'].max()\n",
            "    max_y = df['RA_observed_clr'].max()\n",
            "    max_val = max(max_x, max_y)\n",
            "\n",
            "    min_x = df['RA_expected_clr'].min()\n",
            "    min_y = df['RA_observed_clr'].min()\n",
            "    min_val = min(min_x, min_y)\n",
            "\n",
            "    return max_val, min_val\n",
            "\n",
            "\n",
            "def linear_plot_stats(df: pd.DataFrame, heading: str, pipeline_offset: float, diversity_value: float) -> pd.DataFrame:\n",
            "    \"\"\" \n",
            "    Parameters:\n",
            "        df: pd.DataFrame\n",
            "            The dataframe to use for the linear plot.\n",
            "        heading: str\n",
            "            The heading to use for the plot.\n",
            "        pipeline_offset: float\n",
            "            The offset to use for the pipeline.\n",
            "        diversity_value: float \n",
            "            The diversity value to use for the pipeline.\n",
            "\n",
            "    Returns:\n",
            "        pd.DataFrame: The dataframe with the stats for the linear plot.\n",
            "            In form of: heading | diversity_value | r2 | mae | a_d | bc | rmse | sens | fpra.  \n",
            "\n",
            "    Calculates the statistics for the linear plot and return them in a dataframe.\n",
            "    \"\"\"\n",
            "    stats_list = [heading, diversity_value]\n",
            "\n",
            "    # The R^2, MAE, RMS, and BC are calculated on the non-transformed data.\n",
            "    x = df[\"RA_expected\"].values.reshape(-1, 1)\n",
            "    y = df[\"RA_observed\"].values.reshape(-1, 1)\n",
            "\n",
            "    # We should probably perturb the x and y the same way we do the Aitchison values.\n",
            "    # For now, we will calculate the linear regression on the original values.\n",
            "    reg, y_pred = linear_regression(y, x)\n",
            "\n",
            "    # Calculate R^2.\n",
            "    r2 = reg.score(y, x)\n",
            "    stats_list.append(r2)\n",
            "\n",
            "    # Calculate MAE.\n",
            "    mae = np.mean(np.abs(y - x))\n",
            "    stats_list.append(mae)\n",
            "\n",
            "    # Add the aitchison distance.\n",
            "    try:\n",
            "        a_d = euclidean(df['RA_expected_clr'].values,\n",
            "                        df['RA_observed_clr'].values)\n",
            "        stats_list.append(a_d)\n",
            "\n",
            "    except ValueError as e:\n",
            "        # If there is a zero in the data, we cannot calculate the aitchison distance.\n",
            "        print(f\"ValueError: {e}\")\n",
            "        stats_list.append(np.NaN)\n",
            "\n",
            "    # Add the bray-curtis distance.\n",
            "    try:\n",
            "        bc = 1 - braycurtis(df['RA_expected'].values, df['RA_observed'].values)\n",
            "        stats_list.append(bc)\n",
            "\n",
            "    except ValueError:\n",
            "        stats_list.append(np.NaN)\n",
            "\n",
            "    # Add the RMSE distance.\n",
            "    rms = mean_squared_error(x, y, squared=False)\n",
            "    stats_list.append(rms)\n",
            "\n",
            "    # Add the amos stats.\n",
            "    # print(heading)\n",
            "    # df.to_csv(\"amos_df.csv\", index=True, header=True, mode='a')\n",
            "    stats_list.extend(calc_amos_stats(df))\n",
            "\n",
            "    stats_df = (pd.DataFrame(stats_list, index=['Source/Pipeline', 'Diversity', 'R^2', 'MAE',\n",
            "                'AD', '1-BC', 'RMSE', 'Sens', 'FPRA', 'Unclassified', 'Num_FP']).T).set_index('Source/Pipeline')\n",
            "\n",
            "    return stats_df\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def make_minimum(s: pd.Series):\n",
            "    minimum = s[s > 0].min()\n",
            "    return minimum / 10"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def linear_plot(input_df: pd.DataFrame, title: str, sample_id: str, diversity_dict: dict, hue_category=\"Source_observed\", save_path=None, inset=False, colors=color_palette, linear=False) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Plot a linear regression of the expected vs. observed relative abundance. Also calculates the R^2 value, MAE, RMSE, and Aitchison distance.\n",
            "    Parameters:\n",
            "        input_df: A dataframe with the expected and observed relative abundance. The third column should be the source of the data.\n",
            "        title: The title of the plot.\n",
            "        sample_id: The sample ID of the plot.\n",
            "        diversity_dict: A dictionary of the diversity values by grouping variable (hue category).\n",
            "        hue_category: The category to use for the hue (default: \"Source_observed\").\n",
            "        save_path: The path to save the plot to (if None, the plot will not be saved).\n",
            "        inset: Whether or not to plot the inset.\n",
            "        linear: Whether add plots of the non-CLR data.\n",
            "    Returns:\n",
            "        A dataframe with the R^2, MAE, Aitchison distance, BC, and RMSE with pipeline and sampleID.\n",
            "\n",
            "    Plots the expected vs. observed relative abundance and plots the CLR transformed data. \\\\\n",
            "        Also calculates the statistics for the plot and returns them in a dataframe.\n",
            "    \"\"\"\n",
            "    # display(input_df.head())\n",
            "    stats_df = pd.DataFrame()\n",
            "\n",
            "    fig = plt.figure(figsize=(10, 10))\n",
            "\n",
            "    # We need to transform the data to clr space before we plot.\n",
            "    plot_df = pd.DataFrame()\n",
            "    for heading, dataframe in input_df.groupby(hue_category):\n",
            "        # The constant value should be one order of magnitude smaller than the smallest NONZERO in the dataframe.\n",
            "        exp_minimum = make_minimum(dataframe['RA_expected'])\n",
            "        obs_minimum = make_minimum(dataframe['RA_observed'])\n",
            "\n",
            "        # print(\"minimums: \")\n",
            "        # print(exp_minimum, \" \", obs_minimum)\n",
            "\n",
            "        append_df = pd.DataFrame({\n",
            "            # \"RA_expected_clr\": clr(multiplicative_replacement(dataframe[\"RA_expected\"], minimum)),\n",
            "            # \"RA_observed_clr\": clr(multiplicative_replacement(dataframe[\"RA_observed\"], minimum)),\n",
            "            \"RA_expected_clr\": clr(multiplicative_replacement(dataframe[\"RA_expected\"], exp_minimum)),\n",
            "            \"RA_observed_clr\": clr(multiplicative_replacement(dataframe[\"RA_observed\"], obs_minimum)),\n",
            "            \"Source_observed\": heading,\n",
            "            \"RA_expected\": dataframe[\"RA_expected\"],\n",
            "            \"RA_observed\": dataframe[\"RA_observed\"],\n",
            "            \"SampleID_observed\": dataframe[\"SampleID_observed\"]}\n",
            "        )\n",
            "\n",
            "        plot_df = pd.concat([plot_df, append_df], axis=0)\n",
            "\n",
            "    ax = sns.lmplot(x=\"RA_expected_clr\", y=\"RA_observed_clr\", hue=hue_category, col=hue_category,\n",
            "                    col_wrap=3, data=plot_df, fit_reg=False, height=7, aspect=1, ci=None, palette=colors)\n",
            "\n",
            "    # Plot the non-CLR data if linear is wanted.\n",
            "    if linear:\n",
            "        non_clr_plot(plot_df, hue_category, colors, title)\n",
            "\n",
            "    pipeline_offset = -0.1\n",
            "    for heading, dataframe in plot_df.groupby(hue_category):\n",
            "        # print(heading, diversity_dict[heading])\n",
            "        # Convert the stats list to a dataframe, transpose it for rows, then concat it to the stats_df.\n",
            "        stats_data = linear_plot_stats(\n",
            "            dataframe, heading, pipeline_offset, diversity_dict[heading])\n",
            "        stats_df = pd.concat([stats_df, stats_data], axis=0)\n",
            "        pipeline_offset -= 0.05\n",
            "\n",
            "    row_colors = None\n",
            "    # Get the colors of the rows by referencing the color palette with the index names.\n",
            "    try:\n",
            "        # Try to assign to the color palette.\n",
            "        row_colors = [colors[x] for x in stats_df.index.tolist()]\n",
            "    except TypeError:\n",
            "        # If we can't (replicate communities), use the default color.\n",
            "        row_colors = colors\n",
            "\n",
            "        # Convert all of the columns to floats.\n",
            "        stats_df = stats_df.astype(float)\n",
            "\n",
            "        # Add the average for the replicates.\n",
            "        stats_df.loc['Average'] = stats_df.mean(numeric_only=True, axis=0)\n",
            "\n",
            "    stats_df.update(stats_df[['R^2', 'MAE', 'AD', '1-BC', 'RMSE', 'Sens',\n",
            "                    'FPRA']].applymap(lambda x: abs(x)).applymap('{:^.4f}'.format))\n",
            "    stats_df[['Diversity']] = stats_df[[\n",
            "        'Diversity']].applymap('{:^.0f}'.format)\n",
            "    plt.table(cellText=stats_df.values,\n",
            "              colLabels=stats_df.columns,\n",
            "              rowLabels=stats_df.index,\n",
            "              rowColours=row_colors,\n",
            "              loc='bottom',\n",
            "              fontsize=14,\n",
            "              bbox=[0.0, -0.65, 1.0, 0.5],\n",
            "              colWidths=[0.50]*len(stats_df.columns))\n",
            "\n",
            "    # Add title.\n",
            "    ax.fig.suptitle(title, fontsize=16, y=1.05)\n",
            "\n",
            "    # Plot a line from (min-0.1, min-0.1) to (max_x+0.1, max_y+0.1) on all facets.\n",
            "    max_val, min_val = get_min_and_max_values(plot_df)\n",
            "    for a in plt.gcf().axes:\n",
            "        a.plot([min_val - 0.01, max_val + 0.01],\n",
            "               [min_val - 0.01, max_val + 0.01], ls=\"--\", c=\".3\")\n",
            "\n",
            "    # Add an inset for the x values between 0 and 0.05.\n",
            "    if inset:\n",
            "        pass\n",
            "        # left, bottom, width, height = [0.65, 0.15, 0.25, 0.25]\n",
            "        # ax2 = ax.fig.add_axes([left, bottom, width, height])\n",
            "        # ax2 = sns.scatterplot(x=\"RA_expected\", y=\"RA_observed\", hue=hue_category, data=input_df, ax=ax2, legend=False, palette=colors)\n",
            "        # ax2.set_xlim(-0.001, 0.02)\n",
            "        # ax2.set_ylim(-0.001, 0.02)\n",
            "        # ax2.set_title(\"Zoomed In\")\n",
            "\n",
            "    # display(stats_df)\n",
            "\n",
            "    if save_path is not None:\n",
            "        pdf_output.savefig(ax.figure, bbox_inches='tight', dpi=300)\n",
            "        plt.close(ax.figure)\n",
            "\n",
            "    return stats_df\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def cleanup_bivariate_stats(df: pd.DataFrame, sampleID: str):\n",
            "    \"\"\"\n",
            "    Parameters:\n",
            "        df: The dataframe to clean up.\n",
            "        sampleID: The sampleID to use for the index.\n",
            "\n",
            "    Since the sampleID and pipeline are changed depending on the hue color (for replicates), we need to change it here.\n",
            "    \"\"\"\n",
            "\n",
            "    # We need to add a column equal to sampleID.\n",
            "    df['SampleID'] = sampleID\n",
            "\n",
            "    # Pop the old index.\n",
            "    df.reset_index(inplace=True)\n",
            "\n",
            "    # Set the index as the sampleID.\n",
            "    df.set_index(['SampleID'], inplace=True)\n",
            "\n",
            "    # We need to rename the \"index\" column to pipeline.\n",
            "    df.rename(columns={'index': 'Pipeline'}, inplace=True)\n",
            "\n",
            "    return df\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Convert from Genus, RA, Source, SampleID to Genus RA_x, RA_y, Source, SampleID.\n",
            "def convert_to_bivariate(root_dir: str, expected_root: str, save_path=None, inset=False, rank=None, threshold=0.0) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Parameters:\n",
            "        root_dir: The root directory of the data.\n",
            "        save_path: The path to save the figure to.\n",
            "        inset: Whether or not to add an inset to the plot.\n",
            "        rank: The rank to use for the plot (taxonomic).\n",
            "        threshold: The threshold to use for filtering of the outer join data. Features with RA below this will be removed.\n",
            "    Returns:\n",
            "        A dataframe with the statistics of the bivariate data.\n",
            "\n",
            "    Converts the fully combined data into a bivariate format. Then, plots the data from a left join and calculates the statistics.\n",
            "    \"\"\"\n",
            "    if rank is None:\n",
            "        raise Exception(\"Rank cannot be None.\")\n",
            "\n",
            "    df = fully_combined(root_dir, expected_root=expected_root, rank=rank)\n",
            "\n",
            "    display(df.head())\n",
            "    # df.to_csv(\"fully_combined.csv\")\n",
            "\n",
            "    diversity_dict = {}\n",
            "\n",
            "    combined_stats = pd.DataFrame()\n",
            "    for sample, sample_df in df.groupby('SampleID'):\n",
            "        bivariate_df = pd.DataFrame()\n",
            "        # Get the expected dataframe.\n",
            "        expected = sample_df[sample_df['Source'] == 'Expected']\n",
            "\n",
            "        # Get the dirname of the root directory.\n",
            "        dirname = root_dir.split('/')[-1]\n",
            "        # print(dirname)\n",
            "\n",
            "        # Get the experimental dataframe.\n",
            "        experimental = sample_df[(sample_df['Source'] != 'Expected') & (\n",
            "            sample_df['Source'] != dirname)]\n",
            "        # display(expected)\n",
            "        # display(experimental)\n",
            "\n",
            "        # We want to join outer on the expected dataframe after grouping by source. This will show the missing values from the expected.\n",
            "        for source, source_df in experimental.groupby('Source'):\n",
            "            # We need the outer merge to calculate total diversity for the amos stat.\n",
            "            outer_merged = join_outer(\n",
            "                expected, source_df, source_name=source, threshold=threshold, replicates=False)\n",
            "\n",
            "            # Now, diversity is number of rows in the outer merged dataframe.\n",
            "            # Add to dictionary. We will pass this to the stats function.\n",
            "            diversity_dict[source] = outer_merged.shape[0]\n",
            "\n",
            "            bivariate_df = pd.concat([bivariate_df, outer_merged], axis=0)\n",
            "\n",
            "        # Add the merged dataframe to the bivariate dataframe.\n",
            "        # bivariate_df.to_csv('bmock12.csv', header=True)\n",
            "\n",
            "        title = f\"Bivariate Linear Regression for Sample {sample} in Experiment {dirname} ({rank} at filter threshold {threshold})\"\n",
            "\n",
            "        # bivariate_df = bivariate_df.sort_values(by='RA_expected', ascending=False).head(30)\n",
            "        if save_path is not None:\n",
            "            stats_df = linear_plot(bivariate_df, title, sample, hue_category=\"Source_observed\", save_path=os.path.join(\n",
            "                root_dir, f\"{sample}_linear_{rank}_all.png\"), inset=inset, linear=True, diversity_dict=diversity_dict)\n",
            "            stats_df = cleanup_bivariate_stats(stats_df, sample)\n",
            "\n",
            "            combined_stats = pd.concat([combined_stats, stats_df], axis=0)\n",
            "        else:\n",
            "            stats_df = linear_plot(bivariate_df, title, sample, hue_category=\"Source_observed\",\n",
            "                                   save_path=None, linear=True, diversity_dict=diversity_dict)\n",
            "            stats_df = cleanup_bivariate_stats(stats_df, sample)\n",
            "\n",
            "            combined_stats = pd.concat([combined_stats, stats_df], axis=0)\n",
            "\n",
            "    return combined_stats\n",
            "# bivariate_df = convert_to_bivariate(fully_combined())\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Multiple Samples (Replicates) with One Expected"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def plot_many_versus_expected(root_dir: str, output_dir: str, in_df: pd.DataFrame, rank: str, threshold: float):\n",
            "    \"\"\"\n",
            "    Parameters:\n",
            "        root_dir: str\n",
            "            The root directory of the data.\n",
            "        output_dir: str\n",
            "            The output directory to save the plots to.\n",
            "        in_df: pd.DataFrame\n",
            "            The dataframe to use for plotting.\n",
            "        rank: str\n",
            "            The rank to use for the plot (taxonomic).\n",
            "        threshold: float\n",
            "            The threshold to use for filtering of the outer join data. Features with RA below this will be removed.\n",
            "\n",
            "    This function is used for multiple samples from the same pipeline against the same expected and plots in bar charts. \\\\\n",
            "    Ex. S1_expected compared to S1_jams, S1_biobakery, etc. Then, S2_expected compared to S2_jams, S2_biobakery, etc.\n",
            "    \"\"\"\n",
            "    print(\"bar plots\")\n",
            "    expected = in_df[in_df['Source'] == 'Expected']\n",
            "    experimental = in_df[in_df['Source'] != 'Expected']\n",
            "\n",
            "    dirname = root_dir.split('/')[-1]\n",
            "    for pipeline, df in experimental.groupby('Source'):\n",
            "        # print(pipeline)\n",
            "        if pipeline == 'Expected':\n",
            "            raise Exception(\"Pipeline should not be Expected.\")\n",
            "\n",
            "        if pipeline != dirname:\n",
            "            fig = plt.figure(figsize=(15, 12))\n",
            "\n",
            "            df = df.where(df['RA'] > threshold).dropna()\n",
            "\n",
            "            # display(df.head())\n",
            "\n",
            "            # Add the expected dataframe to the combined dataframe.\n",
            "            merged = pd.concat([expected, df], axis=0)\n",
            "\n",
            "            # Sample names change, so we can just use the same set of colors rather than having to change them.\n",
            "            ax = sns.barplot(x=merged.index, y='RA', hue=\"SampleID\",\n",
            "                             data=merged, errorbar=None, log=True, palette=cb_palette)\n",
            "            ticks = [0.001, 0.01, 0.10]\n",
            "            ax.set_yticks(ticks)\n",
            "            ax.set_yticklabels(ticks)\n",
            "            # ax.bar_label(ax.containers[0], fmt='%.2e', label_type='center')\n",
            "            # ax.bar_label(ax.containers[1], fmt='%.2e', label_type='edge')\n",
            "\n",
            "            ax.set_xticklabels(fix_x_labels(\n",
            "                ax=ax, df=merged, rank=rank), rotation=45, horizontalalignment='right')\n",
            "            # ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
            "\n",
            "            title = f\"Expected vs. Observed Relative Abundance for {rank} using {pipeline} in Experiment {dirname}\"\n",
            "            ax.set_title(title)\n",
            "            ax.set_xlabel(rank)\n",
            "            ax.set_ylabel('Relative Abundance')\n",
            "\n",
            "            pdf_output.savefig(fig, bbox_inches='tight', dpi=300)\n",
            "            plt.close(fig)\n",
            "            # plt.savefig(os.path.join(output_dir, f\"{pipeline}\", f\"{pipeline}_bars.png\"), dpi=300, bbox_inches='tight')\n",
            "        else:\n",
            "            continue\n",
            "\n",
            "# plot_many_versus_expected(root_dir, fully_combined(), \"genus\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# This function will plot the expected vs. observed for each sample in each pipeline.\n",
            "# There is only one expected value for each sample since they are replicates.\n",
            "# This will be a bivariate plot.\n",
            "def plot_many_versus_expected_bivariate(root_dir, observed_df, expected_df, rank, inset, threshold) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    This function will plot the expected vs. observed for each sample in each pipeline. \n",
            "    There is only one expected value for each sample since they are replicates. \n",
            "    This will be a bivariate plot that will include MAE, r^2 and Aitchison distance.\n",
            "\n",
            "    Parameters:\n",
            "        root_dir : str\n",
            "            The root directory of the pipeline.\n",
            "        observed_df : pd.DataFrame\n",
            "            The observed dataframe.\n",
            "        expected_df : pd.DataFrame\n",
            "            The expected dataframe.\n",
            "        rank : str\n",
            "            The taxonomic rank.\n",
            "        inset : bool\n",
            "            Whether or not to include an inset plot.\n",
            "    Returns:\n",
            "        pd.DataFrame\n",
            "            A dataframe containing the statistics for each sample.\n",
            "    \"\"\"\n",
            "    # First, we need to make a df of observed vs. expected for each sample.\n",
            "    combined_stats = pd.DataFrame()\n",
            "\n",
            "    for pipeline, pipeline_df in observed_df.groupby(\"Source\"):\n",
            "        print(pipeline)\n",
            "        # Plotting expected vs expected is useless.\n",
            "        if pipeline == \"tourlousse\" or pipeline == \"Expected\":\n",
            "            continue\n",
            "\n",
            "        # We are going to merge outer so that we can see which organisms are missing from the expected or are not supposed to be there.\n",
            "        # merged = join_left_replicates(expected=expected_df, source_df=pipeline_df, source_name=pipeline, rank=rank)\n",
            "        # merged.to_csv(\"left_debug.csv\", mode='a', header=True)\n",
            "\n",
            "        # Make the diversity dictionary.\n",
            "        # display(expected_df.head())\n",
            "        # display(pipeline_df.head())\n",
            "        outer_merged = join_outer(\n",
            "            expected_df, pipeline_df, source_name=pipeline, threshold=threshold, replicates=True)\n",
            "        diversity_dict = {}\n",
            "        for sample, sample_df in outer_merged.groupby(\"SampleID_observed\"):\n",
            "            diversity_dict[sample] = sample_df.shape[0]\n",
            "\n",
            "        save_path = os.path.join(\n",
            "            root_dir, pipeline, f\"{pipeline}_bivariate_{rank}_all_samples.png\")\n",
            "        # merged.to_csv(os.path.join(root_dir, pipeline, f\"{pipeline}_bivariate_{rank}_all_samples.csv\"))\n",
            "\n",
            "        exp_name = os.path.basename(root_dir)\n",
            "\n",
            "        # Add amos to the mixed or hilo samples since they are nested.\n",
            "        if exp_name == \"mixed\" or exp_name == \"hilo\":\n",
            "            exp_name = \"Amos \" + exp_name\n",
            "\n",
            "        # print(pipeline)\n",
            "        pipeline_stats = linear_plot(outer_merged, f\"Expected vs. Observed Relative Abundance for {rank} using {pipeline} in Experiment {exp_name} with filter {threshold}\",\n",
            "                                     pipeline, hue_category=\"SampleID_observed\", save_path=save_path, inset=inset, colors=cb_palette[1:], diversity_dict=diversity_dict)\n",
            "\n",
            "        # Add pipeline column to the stats dataframe equal to the pipeline name.\n",
            "        pipeline_stats['Pipeline'] = pipeline\n",
            "\n",
            "        # Add the pipeline stats to the combined stats dataframe.\n",
            "        combined_stats = pd.concat([combined_stats, pipeline_stats], axis=0)\n",
            "\n",
            "    return combined_stats\n",
            "\n",
            "# plot_many_versus_expected_bivariate(get_relabund_files(root_dir), get_all_expected(root_dir, \"genus\"), \"genus\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Run the code here: MAIN."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# We can initialize the dataclasses to hold the parameters for each pipeline.\n",
            "@dataclass\n",
            "class Pipeline:\n",
            "    \"\"\"\n",
            "    This class will hold the parameters for each pipeline.\n",
            "    Variables:\n",
            "        root: str\n",
            "            The root directory of the pipeline.\n",
            "        inset: bool\n",
            "            Whether or not to include an inset plot.\n",
            "    \"\"\"\n",
            "    root: str\n",
            "    inset: bool\n",
            "\n",
            "    def __init__(self, root: str, inset: bool):\n",
            "        self.root = root\n",
            "        self.inset = inset\n",
            "\n",
            "\n",
            "# No ending slash for these directories.\n",
            "bmock12 = Pipeline(\"pipelines/bmock12\", False)\n",
            "camisim = Pipeline(\"pipelines/camisimGI\", False)\n",
            "tourlousse = Pipeline(\"pipelines/tourlousse\", False)\n",
            "amos_hilo = Pipeline(\"pipelines/amos/hilo\", False)\n",
            "amos_mixed = Pipeline(\"pipelines/amos/mixed\", False)\n",
            "nist = Pipeline(\"pipelines/nist\", False)\n",
            "mbarc = Pipeline(\"pipelines/mbarc\", False)\n",
            "\n",
            "# root_dirs_one_to_one = [bmock12, camisim, nist, mbarc]\n",
            "root_dirs_one_to_one = [mbarc]\n",
            "root_dirs_one_to_many = [tourlousse, amos_hilo, amos_mixed]\n",
            "# root_dirs_one_to_many = [tourlousse]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def main():\n",
            "    # This is the minimum RA abundance.\n",
            "    # filtering_threshold = [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
            "    filtering_threshold = [0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
            "    filtering_with_bars = [1e-4]\n",
            "    # john_filter = [1e-3]\n",
            "    # john_filter = [1e-4]\n",
            "\n",
            "    def one_to_one(thresh: float, bars: bool = False):\n",
            "        for exp in root_dirs_one_to_one:\n",
            "            expected_root = exp.root.replace(\"pipelines\", \"expected_pipelines\")\n",
            "            print(\"Experiment: \", exp.root)\n",
            "            if bars:\n",
            "                # plot_by_sample(root=exp.root, expected_root=expected_root, output_dir=exp.root,\n",
            "                #                hue_category=\"Source\", taxonomic_rank=\"Genus\", threshold=thresh)\n",
            "                plot_by_sample(root=exp.root, expected_root=expected_root, output_dir=exp.root,\n",
            "                               hue_category=\"Source\", taxonomic_rank=\"Species\", threshold=thresh)\n",
            "\n",
            "            final_stats_genus = convert_to_bivariate(\n",
            "                exp.root, expected_root, save_path=exp.root, inset=exp.inset, rank=\"Genus\", threshold=thresh)\n",
            "            final_stats_genus.to_csv(os.path.join(\n",
            "                exp.root, f\"{str(thresh)}_all_stats_replicates_genus.csv\"), index_label=\"SampleID\")\n",
            "            final_stats = convert_to_bivariate(\n",
            "                exp.root, expected_root, save_path=exp.root, inset=exp.inset, rank=\"Species\", threshold=thresh)\n",
            "            final_stats.to_csv(os.path.join(\n",
            "                exp.root, f\"{str(thresh)}_all_stats_replicates_species.csv\"), index_label=\"SampleID\")\n",
            "\n",
            "    def many_to_one(thresh: float, bars: bool = False):\n",
            "        for exp in root_dirs_one_to_many:\n",
            "            expected_root = exp.root.replace(\"pipelines\", \"expected_pipelines\")\n",
            "            print(\"Experiment: \", exp.root)\n",
            "            if bars:\n",
            "                # plot_many_versus_expected(root_dir=exp.root, output_dir=exp.root, in_df=fully_combined(\n",
            "                    # exp.root, expected_root, rank=\"genus\"), rank=\"genus\", threshold=thresh)\n",
            "                plot_many_versus_expected(exp.root, exp.root, fully_combined(\n",
            "                    exp.root, rank=\"species\"), \"species\", thresh)\n",
            "\n",
            "            final_stats_genus = plot_many_versus_expected_bivariate(exp.root, fully_combined(\n",
            "                exp.root, expected_root=expected_root, rank=\"genus\"), get_all_expected(expected_root, \"genus\"), \"genus\", inset=exp.inset, threshold=thresh)\n",
            "            final_stats_genus.to_csv(os.path.join(\n",
            "                exp.root, f\"{str(thresh)}_all_stats_replicates_genus.csv\"), index_label=\"SampleID\")\n",
            "            final_stats = plot_many_versus_expected_bivariate(exp.root, fully_combined(\n",
            "                exp.root, expected_root=expected_root, rank=\"species\"), get_all_expected(expected_root, \"species\"), \"species\", inset=exp.inset, threshold=thresh)\n",
            "            final_stats.to_csv(os.path.join(\n",
            "                exp.root, f\"{str(thresh)}_all_stats_replicates_species.csv\"), index_label=\"SampleID\")\n",
            "\n",
            "    for thresh in filtering_with_bars:\n",
            "        print(\"Threshhold: \", thresh)\n",
            "        one_to_one(thresh=thresh)\n",
            "        # many_to_one(thresh=thresh)\n",
            "\n",
            "    pdf_output.close()\n",
            "\n",
            "\n",
            "main()\n",
            "\n",
            "# DONE: Recode the sources to be wgsa2, wotlka, bio4, jams.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def gather_expected(pipeline_list: List[Pipeline]) -> None:\n",
            "    for exp in pipeline_list:\n",
            "        expected = get_all_expected(exp.root, rank=\"Species\")\n",
            "        expected[\"Pipeline\"] = os.path.basename(exp.root)\n",
            "        expected.to_csv(\"expected_combined.csv\", mode='a', header=False)\n",
            "\n",
            "# gather_expected(root_dirs_one_to_one)\n",
            "# gather_expected(root_dirs_one_to_many)\n",
            "\n",
            "# for exp in root_dirs_one_to_many:\n",
            "#     expected = get_all_expected(exp.root, rank = \"Species\")\n",
            "\n",
            "# fc = fully_combined(\"pipelines/bmock12/\")\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3.10.6 ('.pipeline_venv': venv)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.6"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "3ac10c2973d938bf4f101ae2299756abbb7e00dac649f0769819439ff384650d"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
