{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 253,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "import numpy as np\n",
            "import seaborn as sns\n",
            "import matplotlib.pyplot as plt\n",
            "plt.rcParams.update({'figure.max_open_warning': 0})\n",
            "\n",
            "from matplotlib.backends.backend_pdf import PdfPages\n",
            "from sklearn.linear_model import LinearRegression\n",
            "from sklearn.metrics import mean_squared_error\n",
            "from scipy.special import rel_entr\n",
            "import os\n",
            "# from python_src.compositions import replace_zero_aitchison\n",
            "# from python_src.compositions import replace_zeroes\n",
            "# from python_src.compositions import constant_aitchison\n",
            "# from python_src.compositions import add_constant\n",
            "from python_src.compositions import multiplicative_aitchison, uniform_replace_zeroes\n",
            "from skbio.stats.composition import multiplicative_replacement\n",
            "from scipy.spatial.distance import braycurtis, euclidean\n",
            "from scipy.stats import pearsonr\n",
            "from dataclasses import dataclass\n",
            "from python_src.compositions import clr\n",
            "from typing import Tuple"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Global Variables"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 254,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Change this depending on your data.\n",
            "# taxonomic_rank = 'Species'\n",
            "base_path = \"pipelines/camisimGI/bio4\"\n",
            "rel_abund_file = \"s1_genus_relabund.csv\"\n",
            "program_name = \"Biobakery4\"\n",
            "sample_id = \"s1\"\n",
            "\n",
            "expected_input = \"pipelines/camisimGI/s1_expected.csv\"\n",
            "\n",
            "pdf_output = PdfPages(\"figures.pdf\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Utils"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 255,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Use this once scipy fixes the bug.\n",
            "# from skbio.stats.composition import clr\n",
            "# from scipy.spatial.distance import euclidean\n",
            "\n",
            "# def aitchinson_distance(x, y):\n",
            "#     return euclidean(clr(x), clr(y))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Colors For Seaborn and MatplotLib"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 256,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
                  "{'Expected': '#1f77b4', 'expected': '#1f77b4', 'woltka': '#ff7f0e', 'wol': '#ff7f0e', 'jams': '#2ca02c', 'wgsa': '#d62728', 'wgsa2': '#d62728', 'biobakery3': '#9467bd', 'bio3': '#9467bd', 'biobakery4': '#8c564b', 'bio4': '#8c564b'}\n"
               ]
            }
         ],
         "source": [
            "cb_palette = sns.color_palette(as_cmap=True)\n",
            "\n",
            "# cb_palette = [\n",
            "# \"#e31a1c\",\n",
            "# \"#1f78b4\",\n",
            "# \"#b2df8a\",\n",
            "# \"#a6cee3\",\n",
            "# \"#fb9a99\",\n",
            "# \"#33a02c\",\n",
            "# ]\n",
            "\n",
            "print(cb_palette)\n",
            "\n",
            "color_palette = {\n",
            "    \"Expected\": cb_palette[0], \n",
            "    \"expected\": cb_palette[0], \n",
            "    \"woltka\": cb_palette[1], \n",
            "    \"wol\": cb_palette[1], \n",
            "    \"jams\": cb_palette[2], \n",
            "    \"wgsa\": cb_palette[3], \n",
            "    \"wgsa2\": cb_palette[3], \n",
            "    \"biobakery3\": cb_palette[4], \n",
            "    \"bio3\": cb_palette[4], \n",
            "    \"biobakery4\": cb_palette[5], \n",
            "    \"bio4\": cb_palette[5]\n",
            "}\n",
            "\n",
            "print(color_palette)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# One Sample, One Expected"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 257,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Generate the expected result from the tsv file. Returns a dataframe.\n",
            "def generate_expected(input_path: str, plot: bool = False, rank : str = \"Genus\") -> pd.DataFrame:\n",
            "    expected = pd.read_csv(input_path, sep=',', index_col=0, names=['Organism', 'Counts', 'TAX_ID'], header=0)\n",
            "    expected = expected[['Counts']].astype(int)\n",
            "\n",
            "    # Calculate expected relative abundance. If it's already in RA, this won't change anything.\n",
            "    expected['RA'] = expected['Counts'] / expected['Counts'].sum()\n",
            "\n",
            "    if rank == \"Species\":\n",
            "        expected.sort_values(by=['RA'], ascending=False, inplace=True)\n",
            "        return expected\n",
            "\n",
            "    # Let's split the organism index into two columns to find the genera.\n",
            "    orgs = expected.index.to_list()\n",
            "    genus = [org.strip().split(' ')[0] for org in orgs]\n",
            "    genus = [x.replace('M.', 'Micromonospora') for x in genus]\n",
            "\n",
            "    # Apparently, propionibacterium have been renamed to cutibacterium.\n",
            "    genus = [x.replace('Propionibact.', 'Cutibacterium') for x in genus]\n",
            "\n",
            "    # Add the columns to the dataframe.\n",
            "    expected['Genus'] = genus\n",
            "    # display(expected.head(12))\n",
            "\n",
            "    # Group by genus and sum the counts for overlapping genera.\n",
            "    exp_genus = expected.groupby('Genus').sum()\n",
            "    exp_genus.sort_values('RA', ascending=False, inplace=True)\n",
            "\n",
            "    if plot:\n",
            "        exp_genus.plot.bar(y='RA', figsize=(8, 5), legend=False, title='Expected Relative Abundance')\n",
            "    \n",
            "    return exp_genus\n",
            "\n",
            "# Use generate_expected to generate the expected result for the bmock12 data.\n",
            "# exp_genus = generate_expected('pipelines/bmock12/s1_expected_species.csv', False, \"Species\")\n",
            "# exp_genus[\"RA\"].to_csv('pipelines/bmock12/s1_expected_species.csv', index_label=\"Species\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 258,
         "metadata": {},
         "outputs": [],
         "source": [
            "# For camisim, we can just read the csv in directly.\n",
            "# exp_genus = pd.read_csv(expected_input, index_col=0, names=['Genus', 'RA'], header=0)\n",
            "# exp_genus = exp_genus.where(exp_genus['RA'] > 0.001).dropna()\n",
            "\n",
            "# (exp_genus.where(exp_genus['RA'] > 0.001).dropna()).to_csv('pipelines/camisimGI/s2_genus_pretty.csv', index_label=\"Genus\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 259,
         "metadata": {},
         "outputs": [],
         "source": [
            "def generate_experimental_df(input_path: str, index_name: str) -> pd.DataFrame:\n",
            "    # Now, load in the experimental values.\n",
            "    r_genus = pd.read_csv(input_path, index_col=0, names=[index_name, \"RA\", \"TAX_ID\"], header=0)\n",
            "    # display(r_genus.head(12))\n",
            "    r_genus = r_genus.astype({'RA': 'float64', 'TAX_ID': 'int64'})\n",
            "\n",
            "    return r_genus\n",
            "\n",
            "# Instead, let's concat the two dataframes into long format and add a column from where it originated.\n",
            "def long_format(df1, df2):\n",
            "    merged = pd.concat([df1, df2], axis=0)\n",
            "\n",
            "    # !!! This is slick\n",
            "    merged['Source'] = ['Expected'] * len(df1) + ['Observed'] * len(df2)\n",
            "\n",
            "    return merged\n",
            "\n",
            "# result_genus = generate_experimental_df(os.path.join(base_path, rel_abund_file), taxonomic_rank)\n",
            "# display(result_genus.head(12))\n",
            "# merged_lf = long_format(exp_genus, result_genus)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Plotting Tools"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 260,
         "metadata": {},
         "outputs": [],
         "source": [
            "def linear_regression(df: pd.DataFrame, exp_df: pd.DataFrame, plot: bool = False, save_path: str = None) -> pd.DataFrame:\n",
            "    # Merge on the genus key for easy plotting. The expected results are on the left, the observed on the right.\n",
            "    # linear_df = exp_genus.merge(df, left_index=True, right_index=True)\n",
            "\n",
            "    # try join because the left merge will drop the genera that are not in the observerd results, but we want to show that the experimental missed it.\n",
            "    linear_df = exp_df.join(df, how='left', lsuffix='_x', rsuffix='_y')\n",
            "    linear_df.fillna(0, inplace=True)\n",
            "\n",
            "    # Linear regression with scikit.\n",
            "    X = linear_df['RA_x'].values.reshape(-1, 1)\n",
            "    Y = linear_df['RA_y'].values.reshape(-1, 1)\n",
            "    reg = LinearRegression().fit(X, Y)\n",
            "    y_pred = reg.predict(X)\n",
            "\n",
            "    # Scatter plot of RA_x vs. RA_y.\n",
            "    if plot:\n",
            "        fig = plt.figure(figsize=(10, 8))\n",
            "        plt.scatter(linear_df['RA_x'], linear_df['RA_y'], color='black')\n",
            "\n",
            "        # Calculate mean absolute error.\n",
            "        mae = np.mean(np.abs(linear_df['RA_x'] - linear_df['RA_y']))\n",
            "\n",
            "        # Regression line.\n",
            "        plt.plot(X, y_pred, color='red', linewidth=2)\n",
            "        # Labels.\n",
            "        plt.xlabel('Expected Relative Abundance')\n",
            "        plt.ylabel('Observed Relative Abundance')\n",
            "        plt.title(f'Expected vs. Observed Relative Abundance for {program_name} {taxonomic_rank}')\n",
            "\n",
            "        # Add r^2 value.\n",
            "        plt.text(0.1, 0.9, f'r^2 = {reg.score(X,Y):.4f}', transform=plt.gca().transAxes)\n",
            "        # Add MAE.\n",
            "        plt.text(0.1, 0.85, f'MAE = {mae:.4f}', transform=plt.gca().transAxes)\n",
            "\n",
            "        # Add line y = x.\n",
            "        plt.plot([0, 1], [0, 1], color='blue', linewidth=2, linestyle='--')\n",
            "\n",
            "        if save_path is not None:\n",
            "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
            "\n",
            "        # plt.show()\n",
            "\n",
            "    return linear_df\n",
            "    \n",
            "# linear_regression(result_genus, exp_genus, plot=True, save_path=os.path.join(base_path, f\"{sample_id}_bivariate_{taxonomic_rank}.png\"))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 261,
         "metadata": {},
         "outputs": [],
         "source": [
            "def fix_x_labels(ax, df, rank):\n",
            "    xticks = ax.get_xticklabels()\n",
            "    # print(xticks)\n",
            "    new_labels = []\n",
            "    for x in xticks:\n",
            "        res = df.loc[int(x.get_text()), rank]\n",
            "        # Get only the first row from the series.\n",
            "        # This is necessary because if it is unique, it will return a string, but if it is not unique, it will return a series.\n",
            "        if isinstance(res, pd.Series):\n",
            "            res = res.iloc[0]\n",
            "        new_labels.append(res)\n",
            "\n",
            "    return new_labels"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 262,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Bar plot of RA_x vs. RA_y side by side.\n",
            "# sns.set_style(\"whitegrid\")\n",
            "def bar_plot(df: pd.DataFrame, plot: bool = False, save_path: str = None, program: str = program_name, title: str =f'INSERT TITLE', hue_category: str ='Source', taxonomic_rank: str = None, threshold: float = 5e-5):\n",
            "    # df.to_csv(\"debug.csv\", index=True)\n",
            "    subset = pd.DataFrame()\n",
            "\n",
            "    # Used to subset by taxonomic rank, but now TAX_ID?\n",
            "    # Need to test further.\n",
            "    for x, y in df.groupby(\"TAX_ID\"):\n",
            "        if y[\"Source\"].values[0] == \"Expected\":\n",
            "            subset = pd.concat([y, subset], axis=0)\n",
            "\n",
            "    # display(subset.head())\n",
            "    subset = subset.where(subset['RA'] > threshold).dropna()\n",
            "    # subset.to_csv(\"subset.csv\", index=True)\n",
            "\n",
            "    if plot:\n",
            "        # Plot a category bar chart with the colors based on the source.\n",
            "        fig = plt.figure(figsize=(10, 8))\n",
            "        ax = sns.barplot(x=subset.index, y='RA', hue=hue_category, data=subset, errorbar=None, palette=color_palette)\n",
            "        ax.semilogy()\n",
            "\n",
            "        ax.bar_label(ax.containers[0], fmt='%.2e', label_type='center')\n",
            "        # ax.bar_label(ax.containers[1], fmt='%.2e', label_type='edge')\n",
            "\n",
            "        # Change the x axis labels from the index to the taxonomic rank.\n",
            "        ax.set_xticklabels(fix_x_labels(ax=ax, df=subset, rank=taxonomic_rank), rotation=45, horizontalalignment='right')\n",
            "\n",
            "        # Later, we can fix the y labels.\n",
            "        # fix_y_labels(ax)\n",
            "\n",
            "        ax.set_title(title)\n",
            "        ax.set_xlabel(taxonomic_rank)\n",
            "        ax.set_ylabel('Relative Abundance')\n",
            "\n",
            "        if save_path is not None:\n",
            "            pdf_output.savefig(fig, dpi=300, bbox_inches='tight')\n",
            "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
            "            plt.close()\n",
            "\n",
            "        plt.show()\n",
            "\n",
            "# bar_plot(merged_lf, plot=True, save_path=os.path.join(base_path, f\"{sample_id}_bars_{taxonomic_rank}.png\"))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Multisample\n",
            "## Utilities"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 263,
         "metadata": {},
         "outputs": [],
         "source": [
            "# We will now aggregate the differences by pipeline. \n",
            "# To do this, we will start at the root and walk down, looking for \"relabund\" files and \"expected\" files.\n",
            "\n",
            "# root_dir = \"pipelines/camisimGI/\"\n",
            "\n",
            "def get_all_expected(root_dir: str, rank=\"Genus\"):\n",
            "    combined_expected = pd.DataFrame()\n",
            "    for root, dirs, files in os.walk(root_dir):\n",
            "        for f in files:\n",
            "            # print(\"files: \", files)\n",
            "            if f\"expected_{rank.lower()}_annotated\" in f and f.endswith(\".csv\"):\n",
            "                # print(f)\n",
            "                df = pd.read_csv(os.path.join(root, f), index_col=0, names=[rank, 'RA', \"TAX_ID\"], header=0)\n",
            "                df[\"Source\"] = \"Expected\"\n",
            "\n",
            "                # Files are of s#_expected.csv, so we can split on the underscore and take the first part.\n",
            "                df[\"SampleID\"] = f.split(\"_\")[0]\n",
            "                combined_expected = pd.concat([combined_expected, df], axis=0)\n",
            "\n",
            "    # combined_expected.reset_index(inplace=True)\n",
            "    # combined_expected.rename({\"index\": rank}, axis=1, inplace=True)\n",
            "    # combined_expected = combined_expected.set_index(\"TAX_ID\")\n",
            "\n",
            "    return combined_expected"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 264,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_relabund_files(root_dir: str, rank=\"genus\"):\n",
            "    combined_df = pd.DataFrame()\n",
            "    for root, dirs, files in os.walk(root_dir):\n",
            "        for f in files:\n",
            "            if f\"{rank.lower()}_relabund_annotated\" in f and f.endswith(\".csv\"):\n",
            "                # print(root, f)\n",
            "                p = os.path.join(root, f)\n",
            "                exp = generate_experimental_df(p, rank)\n",
            "\n",
            "                # Add a column to the experimental dataframe with the pipeline name.\n",
            "                exp['Source'] = os.path.dirname(p).split('/')[-1]\n",
            "\n",
            "                # Add sampleID to the experimental dataframe.\n",
            "                exp['SampleID'] = os.path.basename(p).split('_')[0]\n",
            "                # display(exp.head(10))\n",
            "\n",
            "                # Add the experimental dataframe to the combined dataframe.\n",
            "                combined_df = pd.concat([combined_df, exp], axis=0)\n",
            "\n",
            "    # Ensure that the RA column is a float.\n",
            "    combined_df['RA'] = combined_df['RA'].astype(float)\n",
            "\n",
            "    return combined_df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 265,
         "metadata": {},
         "outputs": [],
         "source": [
            "def fully_combined(root_dir, rank) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Gathers all expected and experimental dataframes and combines them into a single dataframe.\n",
            "    \"\"\"\n",
            "    if rank is None:\n",
            "        raise Exception(\"Rank is not defined.\")\n",
            "\n",
            "    combined_df = get_relabund_files(root_dir, rank=rank)\n",
            "\n",
            "    combined_expected = get_all_expected(root_dir, rank=rank)\n",
            "\n",
            "    # Merge the expected and experimental dataframes.\n",
            "    merged = pd.concat([combined_expected, combined_df], axis=0)\n",
            "    merged = merged.reset_index()\n",
            "    merged = merged.rename(columns={'index': rank})\n",
            "\n",
            "    # merged = merged.astype({'TAX_ID': 'int64'})\n",
            "    merged = merged.set_index(\"TAX_ID\")\n",
            "\n",
            "    # display(merged.head())\n",
            "    # merged.to_csv(\"combined.csv\", index=True, index_label=\"TAX_ID\")\n",
            "\n",
            "    return merged"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 266,
         "metadata": {},
         "outputs": [],
         "source": [
            "def fix_outer_join_df(df: pd.DataFrame, source: str, threshold: float = 0.001) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Parameters:\n",
            "        df: The dataframe to fix.\n",
            "        source: The source of the dataframe.\n",
            "        threshold: The threshold to use for the relative abundance.\n",
            "    \n",
            "    Returns:\n",
            "        pd.Dataframe: The fixed dataframe.\n",
            "\n",
            "    This function will fill the missing RA_expected, RA_observed, and Source_observed columns with 0s and the source name. \\\\ \n",
            "    We then filter out the rows where the RA_expected is less than the threshold. \\\\\n",
            "    We are doing this to penalize the pipeline for adding taxa that are not in the expected.\n",
            "    \"\"\"\n",
            "    # print(\"the threshold is \", threshold)\n",
            "    df['RA_expected'].fillna(0, inplace=True)\n",
            "    df['RA_observed'].fillna(0, inplace=True)\n",
            "    df['Source_observed'].fillna(source, inplace=True)\n",
            "\n",
            "    # Now, filter any rows with 'RA_observed' less than threshold if the corresponding RA_expected value is 0.\n",
            "    df = df.loc[(df['RA_expected'] > 0) | (df['RA_observed'] > threshold)]\n",
            "\n",
            "    return df"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Renormalize"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 267,
         "metadata": {},
         "outputs": [],
         "source": [
            "def renormalize_df(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Parameters:\n",
            "        df: The dataframe to renormalize.\n",
            "\n",
            "    Returns:\n",
            "        pd.DataFrame: The renormalized dataframe.\n",
            "\n",
            "    This function will renormalize the relative abundance. \\\\\n",
            "    Specify the column name to renormalize.\n",
            "    \"\"\"\n",
            "    # Renormalize the experimental dataframes.\n",
            "    df[column_name] = df[column_name] / df[column_name].sum()\n",
            "\n",
            "    return df\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Join functions for the bivariate plots"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 268,
         "metadata": {},
         "outputs": [],
         "source": [
            "def join_left(expected: pd.DataFrame, source_df: pd.DataFrame, source_name: str, sampleID: str) -> pd.DataFrame:\n",
            "    \"\"\" \n",
            "    Parameters:\n",
            "        expected: The expected dataframe.\n",
            "        source_df: The source/experimental dataframe.\n",
            "        source_name: The name of the source.\n",
            "        sampleID: The sampleID of the source (can be None).\n",
            "    Returns:\n",
            "        pd.DataFrame: The joined dataframe.\n",
            "\n",
            "    This function will join the expected dataframe to the source dataframe by left join. \\\\\n",
            "    Then, it will fill the missing observed values with 0s and the source name, since there can be no missing expected values. \\\\.\n",
            "\n",
            "    \"\"\"\n",
            "    exp_copy = expected.copy()\n",
            "    src_copy = source_df.copy()\n",
            "\n",
            "    # Only select where the expected RA is greater than 0.\n",
            "    exp_copy = exp_copy.loc[exp_copy['RA'] > 0]\n",
            "\n",
            "    merged = pd.merge(exp_copy, src_copy, on=\"TAX_ID\", how='left', suffixes=('_expected', '_observed'))\n",
            "    merged['RA_observed'].fillna(0, inplace=True)\n",
            "    merged['Source_observed'].fillna(source_name, inplace=True)\n",
            "    if sampleID is not None:\n",
            "        merged['SampleID'] = sampleID\n",
            "\n",
            "    return merged\n",
            "\n",
            "def join_left_replicates(expected: pd.DataFrame, source_df: pd.DataFrame, source_name: str) -> pd.DataFrame:\n",
            "   raise Exception(\"Not implemented yet.\")\n",
            "\n",
            "def join_outer(expected: pd.DataFrame, source_df: pd.DataFrame, source_name: str, threshold: float) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    This function will join the expected dataframe to the source dataframe by outer join. \\\\\n",
            "    Then it will call fix_outer_join_df to fix the missing values and filter out the rows with RA_expected less than the threshold.\n",
            "    See: :func:`fix_outer_join_df`\n",
            "\n",
            "    ### Parameters:\n",
            "        expected: The expected dataframe.\n",
            "        source_df: The source/experimental dataframe.\n",
            "        source: The name of the source.\n",
            "        sample: The sampleID of the source (can be None).\n",
            "        threshold: The threshold to use for the relative abundance.\n",
            "    ### Returns:\n",
            "        pd.DataFrame: The joined dataframe.\n",
            "    \"\"\"\n",
            "        \n",
            "    exp_copy = expected.copy()\n",
            "    src_copy = source_df.copy()\n",
            "\n",
            "    merged = pd.merge(exp_copy, src_copy, on=\"TAX_ID\", how='outer', suffixes=('_expected', '_observed'))\n",
            "    merged = fix_outer_join_df(merged, source_name, threshold=threshold)\n",
            "\n",
            "    # Renormalize the dataframe since we filtered out some rows.\n",
            "    merged = renormalize_df(merged, 'RA_observed')\n",
            "    merged = renormalize_df(merged, 'RA_expected')\n",
            "\n",
            "    return merged\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Multisample with Different Expected Values"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 269,
         "metadata": {},
         "outputs": [],
         "source": [
            "def plot_by_sample(root, output_dir, taxonomic_rank, hue_category='Source', threshold=5e-5): \n",
            "    full_df = fully_combined(root, rank=taxonomic_rank)\n",
            "    \n",
            "    experiment_name = os.path.basename(root)\n",
            "    # print(root, experiment_name)\n",
            "\n",
            "    for sample_id, df in full_df.groupby('SampleID'):\n",
            "        title = f\"Expected vs. Observed Relative Abundance for {sample_id} in Experiment {experiment_name} ({taxonomic_rank})\"\n",
            "        # display(df.head())\n",
            "        if output_dir is not None:\n",
            "            bar_plot(df, plot=True, save_path=os.path.join(output_dir, f\"{sample_id}_bars_{taxonomic_rank}_all.png\"), title=title, taxonomic_rank=taxonomic_rank, hue_category=hue_category, threshold=threshold)\n",
            "        else:\n",
            "            bar_plot(df, plot=True, save_path=None, title=title, taxonomic_rank=taxonomic_rank, hue_category=hue_category, threshold=threshold)\n",
            "\n",
            "# plot_by_sample(root_dir, hue_category='SampleID')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 270,
         "metadata": {},
         "outputs": [],
         "source": [
            "def non_clr_plot(input_df: pd.DataFrame, hue_category: str, colors: dict, title: str):\n",
            "    lin_df = input_df.copy()\n",
            "    # Drop the rows with 0s in the RA_expected.\n",
            "    lin_df = lin_df[lin_df['RA_expected'] > 0]\n",
            "\n",
            "    # lin_df.to_csv(\"lin_df.csv\", index=True)\n",
            "\n",
            "    ax_lin = sns.lmplot(x=\"RA_expected\", y=\"RA_observed\", hue=hue_category, col=hue_category, col_wrap=3, data=lin_df, fit_reg=False, height=7, aspect=1, ci=None, palette=colors)\n",
            "\n",
            "    # Add the R^2 value to the plot.\n",
            "    def annotate(data, **kws):\n",
            "        max_x = data['RA_expected'].max()\n",
            "        max_y = data['RA_observed'].max()\n",
            "        max_val = max(max_x, max_y)\n",
            "\n",
            "        x = data[\"RA_expected\"].values.reshape(-1, 1)\n",
            "        y = data[\"RA_observed\"].values.reshape(-1, 1)\n",
            "\n",
            "        reg, y_pred = linear_regression(y,x)\n",
            "        r2 = reg.score(y, x)\n",
            "\n",
            "        mae = np.mean(np.abs(y - x))\n",
            "\n",
            "        ax = plt.gca()\n",
            "\n",
            "        ax.text(.05, .8, 'r2={:.4f}'.format(r2),\n",
            "                transform=ax.transAxes)\n",
            "\n",
            "        # Get the data from the plot.\n",
            "        ax.plot([0, max_val], [0, max_val], color='black', linestyle='--')\n",
            "\n",
            "        # Make it log scale.\n",
            "\n",
            "    ax_lin.map_dataframe(annotate)\n",
            "\n",
            "    ax_lin.fig.suptitle(title, fontsize=16, y=1.05)\n",
            "    plt.close(ax_lin.fig)\n",
            "    pdf_output.savefig(ax_lin.figure, bbox_inches='tight', dpi=300)\n",
            "    \n",
            "    # Make this a log-log plot.\n",
            "    # ax.set(xscale=\"log\", yscale=\"log\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 271,
         "metadata": {},
         "outputs": [],
         "source": [
            "def calc_amos_stats(df: pd.DataFrame) -> Tuple[float, float]:\n",
            "    df.to_csv(\"amos_debug.csv\", index=True, mode='a')\n",
            "    def sensitivity(df: pd.DataFrame) -> float:\n",
            "        \"\"\"Sensitivity: (correctly identified / total) * 100\"\"\"\n",
            "        # The correctly identified is the number of rows where the RA_observed is greater than 0.\n",
            "        correct = df[df['RA_observed'] > 0].shape[0]\n",
            "\n",
            "        # Since the data is left joined, the RA_expected column's length is the total.\n",
            "        total = df['RA_expected'].shape[0]\n",
            "\n",
            "        return (correct / total) * 100 \n",
            "\n",
            "    def false_positive_RA(df: pd.DataFrame) -> float:\n",
            "        \"\"\"False Positive Rate: abundance of incorrectly identified taxa / total abundance of all taxa\"\"\"\n",
            "        # The abundance of incorrectly identified (i.e., not present in the expected) is the sum of the RA_observed column subtracted from 1.\n",
            "        # This is because the left joined data means that non-correctly identified taxa are not included in the dataframe, but the RA are not renormalized.\n",
            "        incorrect = 1 - df['RA_observed'].sum()\n",
            "\n",
            "        # The total is 100% (1).\n",
            "        return (incorrect / 1) * 100\n",
            "\n",
            "    return sensitivity(df), false_positive_RA(df)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 272,
         "metadata": {},
         "outputs": [],
         "source": [
            "def linear_regression(X, Y):\n",
            "    reg = LinearRegression().fit(X, Y)\n",
            "    y_pred = reg.predict(X)\n",
            "\n",
            "    return reg, y_pred\n",
            "\n",
            "def get_min_and_max_values(df: pd.DataFrame) -> Tuple[float, float]:\n",
            "    # Get the max x and max y values.\n",
            "    max_x = df['RA_expected_clr'].max()\n",
            "    max_y = df['RA_observed_clr'].max()\n",
            "    max_val = max(max_x, max_y)\n",
            "\n",
            "    min_x = df['RA_expected_clr'].min()\n",
            "    min_y = df['RA_observed_clr'].min()\n",
            "    min_val = min(min_x, min_y)\n",
            "\n",
            "    return max_val, min_val\n",
            "\n",
            "\n",
            "def linear_plot_stats(df: pd.DataFrame, heading: str, pipeline_offset: float, diversity_value: float) -> pd.DataFrame:\n",
            "    \"\"\" \n",
            "    This function will calculate the R^2, RMSE, MAE, BC, and Aitchison distance for the given dataframe.\n",
            "    \"\"\"\n",
            "    stats_list = [heading, diversity_value]\n",
            "\n",
            "    # The R^2, MAE, RMS, and BC are calculated on the non-transformed data.\n",
            "    x = df[\"RA_expected\"].values.reshape(-1, 1)\n",
            "    y = df[\"RA_observed\"].values.reshape(-1, 1)\n",
            "\n",
            "    # We should probably perturb the x and y the same way we do the Aitchison values. \n",
            "    # For now, we will calculate the linear regression on the original values.\n",
            "    reg, y_pred = linear_regression(y, x)\n",
            "\n",
            "    # Calculate R^2.\n",
            "    r2 = reg.score(y, x)\n",
            "\n",
            "    # Add r^2\n",
            "    # plt.text(-0.1, pipeline_offset, f'r\\u00b2 = {r2:.4f} for {heading}', transform=plt.gca().transAxes)\n",
            "    stats_list.append(r2)\n",
            "\n",
            "    # Calculate MAE.\n",
            "    mae = np.mean(np.abs(y - x))\n",
            "    stats_list.append(mae)\n",
            "\n",
            "    # Add MAE\n",
            "    # plt.text(0.3, pipeline_offset, f'MAE = {mae:.4f} for {heading}', transform=plt.gca().transAxes)\n",
            "\n",
            "    # Add the aitchison distance.\n",
            "    try: \n",
            "        a_d = euclidean(df['RA_expected_clr'].values, df['RA_observed_clr'].values)\n",
            "        # plt.text(0.7, pipeline_offset, f'AD = {a_d:.4f} for {heading}', transform=plt.gca().transAxes)\n",
            "        stats_list.append(a_d)\n",
            "\n",
            "    except ValueError as e:\n",
            "        print(f\"ValueError: {e}\")\n",
            "        # If there is a zero in the data, we cannot calculate the aitchison distance.\n",
            "        # plt.text(0.65, pipeline_offset, f'AD = N/A for {heading}', transform=plt.gca().transAxes)\n",
            "        stats_list.append(np.NaN)\n",
            "\n",
            "    # Add the bray-curtis distance.\n",
            "    try: \n",
            "        bc = 1 - braycurtis(df['RA_expected'].values, df['RA_observed'].values)\n",
            "        # plt.text(1.1, pipeline_offset, f'1-BC = {bc:.4f} for {heading}', transform=plt.gca().transAxes)\n",
            "        stats_list.append(bc)\n",
            "    except ValueError:\n",
            "        # plt.text(1.1, pipeline_offset, f'1-BC = N/A for {heading}', transform=plt.gca().transAxes)\n",
            "        stats_list.append(np.NaN)\n",
            "\n",
            "    # Add the RMSE distance.\n",
            "    rms = mean_squared_error(y, x, squared=False)\n",
            "    # plt.text(1.5, pipeline_offset, f'RMSE = {rms:.4f} for {heading}', transform=plt.gca().transAxes)\n",
            "    stats_list.append(rms)\n",
            "\n",
            "    # Add the amos stats.\n",
            "    stats_list.extend(calc_amos_stats(df))\n",
            "\n",
            "    stats_df = (pd.DataFrame(stats_list, index=['Source/Pipeline', 'Diversity', 'R^2', 'MAE', 'AD', '1-BC', 'RMSE', 'Sens', 'FPRA']).T).set_index('Source/Pipeline')\n",
            "\n",
            "    return stats_df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 273,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "def linear_plot(input_df: pd.DataFrame, title: str, sample_id: str, diversity_dict: dict, hue_category=\"Source_observed\", save_path=None, inset=False, colors=color_palette, linear=False) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    Plot a linear regression of the expected vs. observed relative abundance. Also calculates the R^2 value, MAE, RMSE, and Aitchison distance.\n",
            "    Parameters:\n",
            "        input_df: A dataframe with the expected and observed relative abundance. The third column should be the source of the data.\n",
            "        title: The title of the plot.\n",
            "        sample_id: The sample ID of the plot.\n",
            "        diversity_dict: A dictionary of the diversity values by grouping variable (hue category).\n",
            "        hue_category: The category to use for the hue (default: \"Source_observed\").\n",
            "        save_path: The path to save the plot to (if None, the plot will not be saved).\n",
            "        inset: Whether or not to plot the inset.\n",
            "        linear: Whether add plots of the non-CLR data.\n",
            "    Returns:\n",
            "        A dataframe with the R^2, MAE, Aitchison distance, BC, and RMSE with pipeline and sampleID.\n",
            "    \"\"\"\n",
            "    stats_df = pd.DataFrame()\n",
            "\n",
            "    fig = plt.figure(figsize=(10, 10))\n",
            "\n",
            "    # We need to transform the data to clr space before we plot.\n",
            "    plot_df = pd.DataFrame()\n",
            "    for heading, dataframe in input_df.groupby(hue_category):\n",
            "        # The constant value should be one order of magnitude smaller than the smallest NONZERO in the dataframe.\n",
            "        minimum = dataframe['RA_expected']\n",
            "        minimum = minimum[minimum > 0]\n",
            "        minimum = minimum.min() / 10\n",
            "\n",
            "        append_df = pd.DataFrame({\n",
            "            # \"RA_expected_clr\": clr(multiplicative_replacement(dataframe[\"RA_expected\"], minimum)), \n",
            "            # \"RA_observed_clr\": clr(multiplicative_replacement(dataframe[\"RA_observed\"], minimum)), \n",
            "            \"RA_expected_clr\": clr(uniform_replace_zeroes(dataframe[\"RA_expected\"], minimum)), \n",
            "            \"RA_observed_clr\": clr(uniform_replace_zeroes(dataframe[\"RA_observed\"], minimum)),\n",
            "            \"Source_observed\": heading,\n",
            "            \"RA_expected\": dataframe[\"RA_expected\"],\n",
            "            \"RA_observed\": dataframe[\"RA_observed\"],\n",
            "            \"SampleID_observed\": dataframe[\"SampleID_observed\"]}\n",
            "        )\n",
            "\n",
            "        plot_df = pd.concat([plot_df, append_df], axis=0)\n",
            "\n",
            "    ax = sns.lmplot(x=\"RA_expected_clr\", y=\"RA_observed_clr\", hue=hue_category, col=hue_category, col_wrap=3, data=plot_df, fit_reg=False, height=7, aspect=1, ci=None, palette=colors)\n",
            "\n",
            "    # Plot the non-CLR data if linear is wanted.\n",
            "    if linear:\n",
            "        non_clr_plot(plot_df, hue_category, colors, title)\n",
            "    \n",
            "    pipeline_offset = -0.1\n",
            "    for heading, dataframe in plot_df.groupby(hue_category):\n",
            "        # print(heading, diversity_dict[heading])\n",
            "        # Convert the stats list to a dataframe, transpose it for rows, then concat it to the stats_df.\n",
            "        stats_data = linear_plot_stats(dataframe, heading, pipeline_offset, diversity_dict[heading])\n",
            "        stats_df = pd.concat([stats_df, stats_data], axis=0)\n",
            "        pipeline_offset -= 0.05\n",
            "\n",
            "    row_colors = None\n",
            "    # Get the colors of the rows by referencing the color palette with the index names.\n",
            "    try:\n",
            "        # Try to assign to the color palette.\n",
            "        row_colors = [colors[x] for x in stats_df.index.tolist()]\n",
            "    except TypeError:\n",
            "        # If we can't (replicate communities), use the default color.\n",
            "        row_colors = colors\n",
            "        \n",
            "        # Convert all of the columns to floats.\n",
            "        stats_df = stats_df.astype(float)\n",
            "\n",
            "        # Add the average for the replicates.\n",
            "        stats_df.loc['Average'] = stats_df.mean(numeric_only=True, axis=0)\n",
            "\n",
            "    stats_df.update(stats_df[['R^2', 'MAE', 'AD', '1-BC', 'RMSE', 'Diversity', 'Sens', 'FPRA']].applymap('{:.4f}'.format))\n",
            "    plt.table(cellText=stats_df.values,\n",
            "        colLabels=stats_df.columns,\n",
            "        rowLabels=stats_df.index, \n",
            "        rowColours=row_colors,\n",
            "        loc='bottom', \n",
            "        fontsize=14, \n",
            "        bbox=[0.0, -0.65, 1.0, 0.5], \n",
            "        colWidths=[0.50]*len(stats_df.columns))\n",
            "\n",
            "    # Add title.\n",
            "    ax.fig.suptitle(title, fontsize=16, y=1.05)\n",
            "\n",
            "    # Plot a line from (min-0.1, min-0.1) to (max_x+0.1, max_y+0.1) on all facets.\n",
            "    max_val, min_val = get_min_and_max_values(plot_df)\n",
            "    for a in plt.gcf().axes:\n",
            "        a.plot([min_val - 0.01, max_val + 0.01], [min_val - 0.01, max_val + 0.01], ls=\"--\", c=\".3\")\n",
            "\n",
            "    # Add an inset for the x values between 0 and 0.05.\n",
            "    if inset:\n",
            "        pass\n",
            "        # left, bottom, width, height = [0.65, 0.15, 0.25, 0.25]\n",
            "        # ax2 = ax.fig.add_axes([left, bottom, width, height])\n",
            "        # ax2 = sns.scatterplot(x=\"RA_expected\", y=\"RA_observed\", hue=hue_category, data=input_df, ax=ax2, legend=False, palette=colors)\n",
            "        # ax2.set_xlim(-0.001, 0.02)\n",
            "        # ax2.set_ylim(-0.001, 0.02)\n",
            "        # ax2.set_title(\"Zoomed In\")\n",
            "\n",
            "    # display(stats_df)\n",
            "\n",
            "    if save_path is not None:\n",
            "        pdf_output.savefig(ax.figure, bbox_inches='tight', dpi=300)\n",
            "        plt.close(ax.figure)\n",
            "        \n",
            "\n",
            "    return stats_df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 274,
         "metadata": {},
         "outputs": [],
         "source": [
            "def linear_plot_log(input_df: pd.DataFrame, title, sample_id, hue_category=\"Source_observed\", save_path=None):\n",
            "    # We want to plot the x and y axis on a log scale.\n",
            "    fig = plt.figure(figsize=(20, 20))\n",
            "    ax = sns.lmplot(x=\"RA_expected\", y=\"RA_observed\", hue=hue_category, data=input_df, fit_reg=True, height=7, aspect=11/7, ci=None, truncate=True)\n",
            "    # ax.set(xscale=\"symlog\", yscale=\"symlog\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 275,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Since the sampleID and pipeline are changed depending on the hue color (for replicates), we need to change it here.\n",
            "def cleanup_bivariate_stats(df: pd.DataFrame, sampleID: str):\n",
            "    # We need to add a column equal to sampleID.\n",
            "    df['SampleID'] = sampleID\n",
            "\n",
            "    # Pop the old index.\n",
            "    df.reset_index(inplace=True)\n",
            "\n",
            "    # Set the index as the sampleID.\n",
            "    df.set_index(['SampleID'], inplace=True)\n",
            "\n",
            "    # We need to rename the \"index\" column to pipeline.\n",
            "    df.rename(columns={'index': 'Pipeline'}, inplace=True)\n",
            "\n",
            "    return df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 276,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "# Convert from Genus, RA, Source, SampleID to Genus RA_x, RA_y, Source, SampleID.\n",
            "def convert_to_bivariate(root_dir: str, save_path=None, inset=False, rank=None, threshold = 0.0) -> pd.DataFrame:\n",
            "    if rank is None:\n",
            "        raise Exception(\"Rank cannot be None.\")\n",
            "\n",
            "    df = fully_combined(root_dir, rank=rank)\n",
            "    diversity_dict = {}\n",
            "\n",
            "    combined_stats = pd.DataFrame()\n",
            "    for sample, sample_df in df.groupby('SampleID'):\n",
            "        bivariate_df = pd.DataFrame()\n",
            "        # Get the expected dataframe.\n",
            "        expected = sample_df[sample_df['Source'] == 'Expected']\n",
            "\n",
            "        # Get the dirname of the root directory.\n",
            "        dirname = root_dir.split('/')[-1]\n",
            "        # print(dirname)\n",
            "\n",
            "        # Get the experimental dataframe.\n",
            "        experimental = sample_df[(sample_df['Source'] != 'Expected') & (sample_df['Source'] != dirname)]\n",
            "        # display(expected)\n",
            "        # display(experimental)\n",
            "\n",
            "        # We want to join outer on the expected dataframe after grouping by source. This will show the missing values from the expected.\n",
            "        for source, source_df in experimental.groupby('Source'):\n",
            "            # Merge the expected and experimental dataframes.\n",
            "\n",
            "            merged = join_left(expected, source_df, source_name=source, sampleID=sample)\n",
            "            # merged.to_csv(\"bivariate_debug.csv\", mode='a')\n",
            "\n",
            "            # We need the outer merge to calculate total diversity for the amos stat.\n",
            "            outer_merged = join_outer(expected, source_df, source_name=source, threshold=threshold)\n",
            "            # Now, diversity is number of rows in the outer merged dataframe.\n",
            "            # Add to dictionary. We will pass this to the stats function.\n",
            "            diversity_dict[source] = outer_merged.shape[0]\n",
            "            print(source, outer_merged.shape[0])\n",
            "\n",
            "            bivariate_df = pd.concat([bivariate_df, merged], axis=0)\n",
            "\n",
            "        # Add the merged dataframe to the bivariate dataframe.\n",
            "        # bivariate_df.to_csv('bmock12.csv', header=True)\n",
            "\n",
            "        title = f\"Bivariate Linear Regression for Sample {sample} in Experiment {dirname} ({rank})\"\n",
            "\n",
            "        # bivariate_df = bivariate_df.sort_values(by='RA_expected', ascending=False).head(30)\n",
            "        if save_path is not None:\n",
            "            stats_df = linear_plot(bivariate_df, title, sample, hue_category=\"Source_observed\", save_path=os.path.join(root_dir, f\"{sample}_linear_{rank}_all.png\"), inset=inset, linear=True, diversity_dict=diversity_dict)\n",
            "            stats_df = cleanup_bivariate_stats(stats_df, sample)\n",
            "\n",
            "            combined_stats = pd.concat([combined_stats, stats_df], axis=0)\n",
            "        else:\n",
            "            stats_df = linear_plot(bivariate_df, title, sample, hue_category=\"Source_observed\", save_path=None, linear=True, diversity_dict=diversity_dict)\n",
            "            stats_df = cleanup_bivariate_stats(stats_df, sample)\n",
            "\n",
            "            combined_stats = pd.concat([combined_stats, stats_df], axis=0)\n",
            "\n",
            "    return combined_stats\n",
            "# bivariate_df = convert_to_bivariate(fully_combined())"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Multiple Samples (Replicates) with One Expected"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 277,
         "metadata": {},
         "outputs": [],
         "source": [
            "# This function is used for multiple samples from the same pipeline against the same expected.\n",
            "def plot_many_versus_expected(root_dir, output_dir, in_df, rank, threshold):\n",
            "    expected = in_df[in_df['Source'] == 'Expected']\n",
            "    experimental = in_df[in_df['Source'] != 'Expected']\n",
            "\n",
            "    dirname = root_dir.split('/')[-1]\n",
            "    for pipeline, df in experimental.groupby('Source'):\n",
            "        # print(pipeline)\n",
            "        if pipeline == 'Expected':\n",
            "            raise Exception(\"Pipeline should not be Expected.\")\n",
            "        \n",
            "        if pipeline != dirname:\n",
            "            fig = plt.figure(figsize=(15, 12))\n",
            "\n",
            "            df = df.where(df['RA'] > threshold).dropna()\n",
            "\n",
            "            # display(df.head())\n",
            "\n",
            "            # Add the expected dataframe to the combined dataframe.\n",
            "            merged = pd.concat([expected, df], axis=0)\n",
            "\n",
            "            # Sample names change, so we can just use the same set of colors rather than having to change them.\n",
            "            ax = sns.barplot(x=merged.index, y='RA', hue=\"SampleID\", data=merged, errorbar=None, log=True, palette=cb_palette)\n",
            "            ticks = [0.001, 0.01, 0.10]\n",
            "            ax.set_yticks(ticks)\n",
            "            ax.set_yticklabels(ticks)\n",
            "            # ax.bar_label(ax.containers[0], fmt='%.2e', label_type='center')\n",
            "            # ax.bar_label(ax.containers[1], fmt='%.2e', label_type='edge')\n",
            "\n",
            "            ax.set_xticklabels(fix_x_labels(ax=ax, df=merged, rank=rank), rotation=45, horizontalalignment='right')\n",
            "            # ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
            "\n",
            "            title = f\"Expected vs. Observed Relative Abundance for {rank} using {pipeline} in Experiment {dirname}\"\n",
            "            ax.set_title(title)\n",
            "            ax.set_xlabel(rank)\n",
            "            ax.set_ylabel('Relative Abundance')\n",
            "\n",
            "            pdf_output.savefig(fig, bbox_inches='tight', dpi=300)\n",
            "            plt.close(fig)\n",
            "            # plt.savefig(os.path.join(output_dir, f\"{pipeline}\", f\"{pipeline}_bars.png\"), dpi=300, bbox_inches='tight')\n",
            "        else:\n",
            "            continue\n",
            "\n",
            "# plot_many_versus_expected(root_dir, fully_combined(), \"genus\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 278,
         "metadata": {},
         "outputs": [],
         "source": [
            "# This function will plot the expected vs. observed for each sample in each pipeline.\n",
            "# There is only one expected value for each sample since they are replicates.\n",
            "# This will be a bivariate plot.\n",
            "def plot_many_versus_expected_bivariate(root_dir, observed_df, expected_df, rank, inset, threshold) -> pd.DataFrame:\n",
            "    \"\"\"\n",
            "    This function will plot the expected vs. observed for each sample in each pipeline. \n",
            "    There is only one expected value for each sample since they are replicates. \n",
            "    This will be a bivariate plot that will include MAE, r^2 and Aitchison distance.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    root_dir : str\n",
            "        The root directory of the pipeline.\n",
            "    observed_df : pd.DataFrame\n",
            "        The observed dataframe.\n",
            "    expected_df : pd.DataFrame\n",
            "        The expected dataframe.\n",
            "    rank : str\n",
            "        The taxonomic rank.\n",
            "    inset : bool\n",
            "        Whether or not to include an inset plot.\n",
            "\n",
            "    Returns\n",
            "    -------\n",
            "    pd.DataFrame\n",
            "        A dataframe containing the statistics for each sample.\n",
            "    \"\"\"\n",
            "    # First, we need to make a df of observed vs. expected for each sample.\n",
            "    combined_stats = pd.DataFrame()\n",
            "    for pipeline, pipeline_df in observed_df.groupby(\"Source\"):\n",
            "            # Plotting expected vs expected is useless.\n",
            "            if pipeline == \"tourlousse\" or pipeline == \"Expected\":\n",
            "                continue\n",
            "\n",
            "            # We are going to merge outer so that we can see which organisms are missing from the expected or are not supposed to be there. \n",
            "            merged = join_left_replicates(expected_df, pipeline_df, source_name=pipeline)\n",
            "            merged.to_csv(\"left_debug.csv\", mode='a', header=True)\n",
            "\n",
            "            # Make the diversity dictionary.\n",
            "            outer_merged = join_outer(expected_df, pipeline_df, source_name=pipeline, threshold=threshold)\n",
            "            diversity_dict = {}\n",
            "            for sample, sample_df in outer_merged.groupby(\"SampleID_observed\"):\n",
            "                diversity_dict[sample] = sample_df.shape[0]\n",
            "\n",
            "            # merged.to_csv(\"bivariate_debug.csv\", mode='a')\n",
            "\n",
            "            save_path = os.path.join(root_dir, pipeline, f\"{pipeline}_bivariate_{rank}_all_samples.png\")\n",
            "            # merged.to_csv(os.path.join(root_dir, pipeline, f\"{pipeline}_bivariate_{rank}_all_samples.csv\"))\n",
            "\n",
            "            exp_name = os.path.basename(root_dir)\n",
            "\n",
            "            # Add amos to the mixed or hilo samples since they are nested.\n",
            "            if exp_name == \"mixed\" or exp_name == \"hilo\":\n",
            "                exp_name = \"Amos \" + exp_name\n",
            "\n",
            "            print(pipeline)\n",
            "            pipeline_stats = linear_plot(merged, f\"Expected vs. Observed Relative Abundance for {rank} using {pipeline} in Experiment {exp_name}\", pipeline, hue_category=\"SampleID_observed\", save_path=save_path, inset=inset, colors=cb_palette[1:], diversity_dict=diversity_dict)\n",
            "\n",
            "            # Add pipeline column to the stats dataframe equal to the pipeline name.\n",
            "            pipeline_stats['Pipeline'] = pipeline\n",
            "\n",
            "            # Add the pipeline stats to the combined stats dataframe.\n",
            "            combined_stats = pd.concat([combined_stats, pipeline_stats], axis=0)\n",
            "\n",
            "    return combined_stats\n",
            "\n",
            "# plot_many_versus_expected_bivariate(get_relabund_files(root_dir), get_all_expected(root_dir, \"genus\"), \"genus\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Run the code here: MAIN."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 279,
         "metadata": {},
         "outputs": [],
         "source": [
            "# We can initialize the dataclasses to hold the parameters for each pipeline.\n",
            "@dataclass\n",
            "class Pipeline:\n",
            "    root: str\n",
            "    inset: bool\n",
            "\n",
            "    def __init__(self, root: str, inset: bool):\n",
            "        self.root = root\n",
            "        self.inset = inset\n",
            "\n",
            "bmock12 = Pipeline(\"pipelines/bmock12\", False)\n",
            "camisim = Pipeline(\"pipelines/camisimGI\", True)\n",
            "tourlousse = Pipeline(\"pipelines/tourlousse\", False)\n",
            "amos_hilo = Pipeline(\"pipelines/amos/hilo\", False)\n",
            "amos_mixed = Pipeline(\"pipelines/amos/mixed\", False)\n",
            "\n",
            "# No ending slash for these directories.\n",
            "root_dirs_one_to_one = [bmock12, camisim]\n",
            "# root_dirs_one_to_one = [camisim]\n",
            "# root_dirs_one_to_many = [tourlousse, amos_hilo, amos_mixed]\n",
            "root_dirs_one_to_many = [amos_mixed]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 280,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Experiment:  pipelines/amos/mixed\n",
                  "ehre\n",
                  "bio4\n"
               ]
            },
            {
               "ename": "KeyError",
               "evalue": "'SampleID_observed'",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
                  "Cell \u001b[0;32mIn [280], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     many_to_one()\n\u001b[1;32m     25\u001b[0m     pdf_output\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 27\u001b[0m main()\n",
                  "Cell \u001b[0;32mIn [280], line 24\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         final_stats\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(exp\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_stats_replicates.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), index_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSampleID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# one_to_one()\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmany_to_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m pdf_output\u001b[38;5;241m.\u001b[39mclose()\n",
                  "Cell \u001b[0;32mIn [280], line 19\u001b[0m, in \u001b[0;36mmain.<locals>.many_to_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment: \u001b[39m\u001b[38;5;124m\"\u001b[39m, exp\u001b[38;5;241m.\u001b[39mroot)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# plot_many_versus_expected(exp.root, exp.root, fully_combined(exp.root, rank=\"genus\"), \"genus\", filtering_threshold)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# plot_many_versus_expected(exp.root, exp.root, fully_combined(exp.root, rank=\"species\"), \"species\", filtering_threshold)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m final_stats \u001b[38;5;241m=\u001b[39m \u001b[43mplot_many_versus_expected_bivariate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfully_combined\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_all_expected\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# final_stats = plot_many_versus_expected_bivariate(exp.root, fully_combined(exp.root, rank=\"species\"), get_all_expected(exp.root, \"species\"), \"species\", inset=exp.inset, threshold=0.0)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m final_stats\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(exp\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_stats_replicates.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), index_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSampleID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                  "Cell \u001b[0;32mIn [278], line 57\u001b[0m, in \u001b[0;36mplot_many_versus_expected_bivariate\u001b[0;34m(root_dir, observed_df, expected_df, rank, inset, threshold)\u001b[0m\n\u001b[1;32m     54\u001b[0m     exp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmos \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m exp_name\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(pipeline)\n\u001b[0;32m---> 57\u001b[0m pipeline_stats \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExpected vs. Observed Relative Abundance for \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrank\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m using \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpipeline\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m in Experiment \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mexp_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_category\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSampleID_observed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb_palette\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiversity_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiversity_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Add pipeline column to the stats dataframe equal to the pipeline name.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m pipeline_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pipeline\n",
                  "Cell \u001b[0;32mIn [273], line 22\u001b[0m, in \u001b[0;36mlinear_plot\u001b[0;34m(input_df, title, sample_id, diversity_dict, hue_category, save_path, inset, colors, linear)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# We need to transform the data to clr space before we plot.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m plot_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m heading, dataframe \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhue_category\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# The constant value should be one order of magnitude smaller than the smallest NONZERO in the dataframe.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     minimum \u001b[38;5;241m=\u001b[39m dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRA_expected\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m     minimum \u001b[38;5;241m=\u001b[39m minimum[minimum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
                  "File \u001b[0;32m~/coding/pipelines/.pipeline_venv/lib/python3.10/site-packages/pandas/core/frame.py:8392\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   8389\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to supply one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   8390\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8392\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[1;32m   8393\u001b[0m     obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   8394\u001b[0m     keys\u001b[39m=\u001b[39;49mby,\n\u001b[1;32m   8395\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   8396\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   8397\u001b[0m     as_index\u001b[39m=\u001b[39;49mas_index,\n\u001b[1;32m   8398\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m   8399\u001b[0m     group_keys\u001b[39m=\u001b[39;49mgroup_keys,\n\u001b[1;32m   8400\u001b[0m     squeeze\u001b[39m=\u001b[39;49msqueeze,\n\u001b[1;32m   8401\u001b[0m     observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[1;32m   8402\u001b[0m     dropna\u001b[39m=\u001b[39;49mdropna,\n\u001b[1;32m   8403\u001b[0m )\n",
                  "File \u001b[0;32m~/coding/pipelines/.pipeline_venv/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:959\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[39mif\u001b[39;00m grouper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgroupby\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrouper\u001b[39;00m \u001b[39mimport\u001b[39;00m get_grouper\n\u001b[0;32m--> 959\u001b[0m     grouper, exclusions, obj \u001b[39m=\u001b[39m get_grouper(\n\u001b[1;32m    960\u001b[0m         obj,\n\u001b[1;32m    961\u001b[0m         keys,\n\u001b[1;32m    962\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    963\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m    964\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    965\u001b[0m         observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[1;32m    966\u001b[0m         mutated\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmutated,\n\u001b[1;32m    967\u001b[0m         dropna\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropna,\n\u001b[1;32m    968\u001b[0m     )\n\u001b[1;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m obj\n\u001b[1;32m    971\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis_number(axis)\n",
                  "File \u001b[0;32m~/coding/pipelines/.pipeline_venv/lib/python3.10/site-packages/pandas/core/groupby/grouper.py:889\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    887\u001b[0m         in_axis, level, gpr \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, gpr, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    888\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m    890\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(gpr, Grouper) \u001b[39mand\u001b[39;00m gpr\u001b[39m.\u001b[39mkey \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[39m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     exclusions\u001b[39m.\u001b[39madd(gpr\u001b[39m.\u001b[39mkey)\n",
                  "\u001b[0;31mKeyError\u001b[0m: 'SampleID_observed'"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "<Figure size 1000x1000 with 0 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "def main():\n",
            "    # This is the minimum RA abundance.\n",
            "    filtering_threshold = 0.001\n",
            "\n",
            "    def one_to_one():\n",
            "        for exp in root_dirs_one_to_one:\n",
            "            print(\"Experiment: \", exp.root)\n",
            "            plot_by_sample(root=exp.root, output_dir=exp.root, hue_category=\"Source\", taxonomic_rank=\"Genus\", threshold=filtering_threshold)\n",
            "            plot_by_sample(root=exp.root, output_dir=exp.root, hue_category=\"Source\", taxonomic_rank=\"Species\", threshold=filtering_threshold)\n",
            "            final_stats = convert_to_bivariate(exp.root, save_path=exp.root, inset=exp.inset, rank=\"Genus\", threshold=filtering_threshold)\n",
            "            final_stats = convert_to_bivariate(exp.root, save_path=exp.root, inset=exp.inset, rank=\"Species\", threshold=filtering_threshold)\n",
            "            final_stats.to_csv(os.path.join(exp.root, \"all_stats_replicates.csv\"), index_label=\"SampleID\")\n",
            "\n",
            "    def many_to_one():\n",
            "        for exp in root_dirs_one_to_many:\n",
            "            print(\"Experiment: \", exp.root)\n",
            "            # plot_many_versus_expected(exp.root, exp.root, fully_combined(exp.root, rank=\"genus\"), \"genus\", filtering_threshold)\n",
            "            # plot_many_versus_expected(exp.root, exp.root, fully_combined(exp.root, rank=\"species\"), \"species\", filtering_threshold)\n",
            "            final_stats = plot_many_versus_expected_bivariate(exp.root, fully_combined(exp.root, rank=\"genus\"), get_all_expected(exp.root, \"genus\"), \"genus\", inset=exp.inset, threshold=0.0)\n",
            "            # final_stats = plot_many_versus_expected_bivariate(exp.root, fully_combined(exp.root, rank=\"species\"), get_all_expected(exp.root, \"species\"), \"species\", inset=exp.inset, threshold=0.0)\n",
            "            final_stats.to_csv(os.path.join(exp.root, \"all_stats_replicates.csv\"), index_label=\"SampleID\")\n",
            "\n",
            "    # one_to_one()\n",
            "    many_to_one()\n",
            "    pdf_output.close()\n",
            "    \n",
            "main()\n",
            "\n",
            "# fc = fully_combined(\"pipelines/bmock12/\")\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3.10.6 ('.pipeline_venv': venv)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.6"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "3ac10c2973d938bf4f101ae2299756abbb7e00dac649f0769819439ff384650d"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
