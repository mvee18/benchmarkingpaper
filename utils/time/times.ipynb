{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sys  # noqa\n",
    "sys.path.append('../..')  # noqa\n",
    "\n",
    "from utils.data_paths import make_data_list\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from datetime import datetime, timedelta\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OutputLogs:\n",
    "    \"\"\" Holds the paths to the output logs for each pipeline. \"\"\"\n",
    "    bio3: str = \"\"\n",
    "    bio4: str = \"\"\n",
    "    jams: list[str] = field(default_factory=list)\n",
    "    wgsa2: str = \"\"\n",
    "    woltka: list[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TimeData:\n",
    "    \"\"\" Holds the average or total time for each pipeline.\"\"\"\n",
    "    bio3: timedelta = timedelta()\n",
    "    bio4: timedelta = timedelta()\n",
    "    jams: timedelta = timedelta()\n",
    "    wgsa2: timedelta = timedelta()\n",
    "    woltka: timedelta = timedelta()\n",
    "\n",
    "@dataclass\n",
    "class ThreadData:\n",
    "    \"\"\" Holds the number of CPUS used in each run. \"\"\"\n",
    "    bio3: int = 0\n",
    "    bio4: int = 0\n",
    "    jams: int = 0\n",
    "    wgsa2: int = 0\n",
    "    woltka: int = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_files(path: str):\n",
    "    \"\"\" Look for anadama.log, JAMS logs, WGSA logs, and Wolka logs.\"\"\"\n",
    "    data_obj = OutputLogs()\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # print(files)\n",
    "        for f in files:\n",
    "            if f == \"anadama.log\":\n",
    "                if \"bio4\" in root:\n",
    "                    data_obj.bio4 = os.path.join(root, f)\n",
    "                elif \"bio3\" in root:\n",
    "                    data_obj.bio3 = os.path.join(root, f)\n",
    "            elif f.endswith(\"JAMS.log\"):\n",
    "                # Don't add the beta log or the negative control log.\n",
    "                if \"beta\" in root or \"Neg\" in f:\n",
    "                    continue\n",
    "                else:\n",
    "                    data_obj.jams.append(os.path.join(root, f))\n",
    "            elif f == \"logfile.txt\":\n",
    "                data_obj.wgsa2 = os.path.join(root, f)\n",
    "\n",
    "            elif f == \"classify_time.log\" or f == \"bowtie_time.log\":\n",
    "                data_obj.woltka.append(os.path.join(root, f))\n",
    "\n",
    "    return data_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anadama_format = \"%Y-%m-%d %H:%M:%S,%f\"\n",
    "wanted_time_fmt = \"%H:%M:%S\"\n",
    "\n",
    "\n",
    "def parse_bio_time(log_path: str):\n",
    "    \"\"\"Parse the anadama.log file to get the start and end times.\"\"\"\n",
    "    # First line is the start time, last line is the end time.\n",
    "\n",
    "    with open(log_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        start = datetime.strptime(\n",
    "            lines[0].strip().split('\\t')[0], anadama_format)\n",
    "        end = datetime.strptime(\n",
    "            lines[-1].strip().split('\\t')[0], anadama_format)\n",
    "        \n",
    "        # Find the line that has \"threads\" in it.\n",
    "        threads = 0\n",
    "        for line in lines:\n",
    "            if \"threads\" in line:\n",
    "                # Get the number of threads used.\n",
    "                threads = int(line.split(' ')[-1])\n",
    "                break\n",
    "\n",
    "        elapsed = end - start\n",
    "\n",
    "        return elapsed, threads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jams_format_time = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "\n",
    "def average_time(times: list):\n",
    "    \"\"\" Get the average time for each pipeline. \"\"\"\n",
    "    average = sum(times, timedelta()) / len(times)\n",
    "    return average\n",
    "\n",
    "\n",
    "def parse_jams_time(logs: list):\n",
    "    \"\"\" Parse all of the JAMS logs. \"\"\"\n",
    "\n",
    "    cpu_regex = \"Saving project workspace image using fastSave package with \\d+ CPUs\"\n",
    "\n",
    "    times = []\n",
    "\n",
    "    # They all ran with the same number of threads, so we can overwrite this in the for loop.\n",
    "    threads = 0\n",
    "    for l in logs:\n",
    "        # open the file\n",
    "        with open(l, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            # First line is start time.\n",
    "            start = \" \".join(lines[0].strip().split()[1:3]).strip(\"[]\")\n",
    "            start_time = datetime.strptime(start, jams_format_time)\n",
    "            # Last line is end time.\n",
    "            end = \" \".join(lines[-1].strip().split()[1:3]).strip(\"[]\")\n",
    "            end_time = datetime.strptime(end, jams_format_time)\n",
    "\n",
    "            for line in lines:\n",
    "                if \"Saving project workspace image using fastSave package with\" in line:\n",
    "                    threads = int(line.split()[-2])\n",
    "                    break                  \n",
    "\n",
    "            elapsed = end_time - start_time\n",
    "\n",
    "            times.append(elapsed)\n",
    "\n",
    "    return average_time(times), threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wgsa2(file_path: str):\n",
    "    \"\"\" Parse the WGSA output log for the time.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        start = \" \".join(lines[0].strip().split()[0:2]).strip(\"[]\")\n",
    "        start_time = datetime.strptime(start, anadama_format)\n",
    "        # Last line is end time.\n",
    "        end = \" \".join(lines[-1].strip().split()[0:2]).strip(\"[]\")\n",
    "        end_time = datetime.strptime(end, anadama_format)\n",
    "\n",
    "        threads = 0\n",
    "        for line in lines:\n",
    "            if 'Provided cores:' in line:\n",
    "                threads = int(line.split()[-1])\n",
    "                break\n",
    "\n",
    "        elapsed = end_time - start_time\n",
    "        return elapsed, threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_woltka_time(logs: list):\n",
    "    times = []\n",
    "    for log in logs:\n",
    "        # Second line gives column widths.\n",
    "        with open(log, 'r') as f:\n",
    "            dash_line = f.readlines()[1].strip().split()\n",
    "            widths = [len(x)+1 for x in dash_line]\n",
    "\n",
    "            df = pd.read_fwf(log, widths=widths, skiprows=[1], header=0)\n",
    "            df = df.loc[df[\"JobName\"] == \"swarm\"]\n",
    "\n",
    "            # Split along the colon. This is of the format DD:HH:MM.\n",
    "            df[\"Elapsed\"] = df[\"Elapsed\"].str.split(\":\").apply(\n",
    "                lambda x: timedelta(days=int(x[0]), hours=int(x[1]), minutes=int(x[2])))\n",
    "\n",
    "            avg_td = df[\"Elapsed\"].mean()\n",
    "\n",
    "            times.append(avg_td)\n",
    "\n",
    "    # Sum of the bowtie and classify times.\n",
    "    total_time = sum(times, timedelta())\n",
    "    return total_time.to_pytimedelta()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_all_times(log_paths: OutputLogs) -> TimeData:\n",
    "    \"\"\" Parses all time data, returns a TimeData object.\"\"\"\n",
    "    times = TimeData()\n",
    "    threads_data = ThreadData()\n",
    "\n",
    "    # Parse bio3 time.\n",
    "    bio3_time, threads = parse_bio_time(log_paths.bio3)\n",
    "    times.bio3 = bio3_time / threads\n",
    "    # threads_data.bio3 = threads\n",
    "    # Parse bio4 time.\n",
    "    bio4_time, threads = parse_bio_time(log_paths.bio4)\n",
    "    times.bio4 = bio4_time / threads\n",
    "    # threads_data.bio4 = threads\n",
    "    # Parse JAMS time.\n",
    "    jams_times, threads = parse_jams_time(log_paths.jams)\n",
    "    times.jams = jams_times / threads\n",
    "    # threads_data.jams = threads\n",
    "    # Parse WGSA2 time.\n",
    "    wgsa2_time, threads = parse_wgsa2(log_paths.wgsa2)\n",
    "    times.wgsa2 = wgsa2_time / threads\n",
    "    # threads_data.wgsa2 = threads\n",
    "    # Parse Woltka time.\n",
    "    woltka_times = parse_woltka_time(log_paths.woltka)\n",
    "    # Woltka used 16 threads. This could be automated but since we are doing just this one pipeline, it's fine.\n",
    "    times.woltka = woltka_times / 16\n",
    "\n",
    "    # threads_data.woltka = 16\n",
    "\n",
    "    return times, threads_data\n",
    "\n",
    "\n",
    "def analyze_times(log_paths: OutputLogs):\n",
    "    times, threads = parse_all_times(log_paths)\n",
    "\n",
    "    print(asdict(times))\n",
    "    print(threads)\n",
    "\n",
    "    time_df = pd.DataFrame(asdict(times), index=[0])\n",
    "    # Drop the second row.\n",
    "\n",
    "    display(time_df)\n",
    "\n",
    "    # Relative difference is (x2 - min) / min\n",
    "    relative_times = time_df.apply(lambda x: (\n",
    "        x - time_df.min(axis=1)) / time_df.min(axis=1) * 100)\n",
    "\n",
    "    # Set index value to be: \"Relative Time (Factor of Smallest Time)\"\n",
    "    relative_times.index = [\"Relative Time Per CPU (%)\"]\n",
    "\n",
    "    return time_df, relative_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_dict = {\n",
    "    \"nist\": \"/Volumes/TBHD_share/valencia/pipelines/NIST/\"\n",
    "}\n",
    "\n",
    "def main():\n",
    "    log_paths = search_for_files(paths_dict[\"nist\"])\n",
    "    # print(log_paths)\n",
    "\n",
    "    time_df, relative_time_df = analyze_times(log_paths)\n",
    "\n",
    "    display(time_df)\n",
    "\n",
    "    # Format timedf to HH:MM:SS\n",
    "    time_df = time_df.applymap(lambda x: str(x).split(\".\")[0])\n",
    "\n",
    "    # Strip the days from the raw time_df.\n",
    "    time_df = time_df.applymap(lambda x: x.split(\" \")[-1])\n",
    "    time_df.index = [\"Time (HH:MM:SS)\"]\n",
    "\n",
    "    # Concat the two dataframes.\n",
    "    time_df = pd.concat([time_df, relative_time_df], axis=0)\n",
    "\n",
    "    display(time_df)\n",
    "\n",
    "    time_df.to_latex(\"time_table.tex\", index=True, escape=True)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nist_data():\n",
    "    # Assert that the NIST output is TimeData(bio3=datetime.timedelta(seconds=28415, microseconds=815000), bio4=datetime.timedelta(seconds=4478, microseconds=860000), jams=datetime.timedelta(seconds=5213, microseconds=800000), wgsa2=datetime.timedelta(seconds=4963, microseconds=465000), woltka=datetime.timedelta(seconds=25692))).\n",
    "\n",
    "    wanted = TimeData(bio3=timedelta(seconds=28415, microseconds=815000), bio4=timedelta(seconds=4478, microseconds=860000), jams=timedelta(\n",
    "        seconds=5213, microseconds=800000), wgsa2=timedelta(seconds=4963, microseconds=465000), woltka=timedelta(seconds=25692))\n",
    "    log_path = search_for_files(paths_dict[\"nist\"])\n",
    "    times, times_df = analyze_times(log_path)\n",
    "\n",
    "    assert times == wanted\n",
    "\n",
    "\n",
    "test_nist_data()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.pipeline_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ac10c2973d938bf4f101ae2299756abbb7e00dac649f0769819439ff384650d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
