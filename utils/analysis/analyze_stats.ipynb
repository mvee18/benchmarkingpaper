{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import os\n",
    "\n",
    "run_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_palette = sns.color_palette(as_cmap=True)\n",
    "\n",
    "color_palette = {\n",
    "    \"Expected\": cb_palette[0], \n",
    "    \"expected\": cb_palette[0], \n",
    "    \"woltka\": cb_palette[1], \n",
    "    \"wol\": cb_palette[1], \n",
    "    \"jams\": cb_palette[2], \n",
    "    \"wgsa\": cb_palette[3], \n",
    "    \"wgsa2\": cb_palette[3], \n",
    "    \"biobakery3\": cb_palette[4], \n",
    "    \"bio3\": cb_palette[4], \n",
    "    \"biobakery4\": cb_palette[5], \n",
    "    \"bio4\": cb_palette[5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.abspath(\"../../pipelines/\")\n",
    "# threshold = 0.0\n",
    "\n",
    "# First, we load the data from the CSV file.\n",
    "def find_stats_files(rank: str, threshold: bool):\n",
    "    for root, dirs, files in os.walk(project_root):\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            if f\"all_stats_replicates_{rank}\" in file and file.endswith('.csv'):\n",
    "                stats_path = os.path.join(root, file)\n",
    "\n",
    "                df = pd.read_csv(stats_path)\n",
    "\n",
    "                if threshold:\n",
    "                    # Add the threshold to the dataframe\n",
    "                    threshold = file.split(\"_\")[0]\n",
    "                    if threshold == \"all\":\n",
    "                        continue\n",
    "                    elif threshold == \"0.1\":\n",
    "                        continue\n",
    "                    else:   \n",
    "                        df[\"threshold\"] = threshold\n",
    "                \n",
    "                yield stats_path, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_stats(rank: str, threshold_files: bool) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        rank: str\n",
    "            The rank to combine the stats for.\n",
    "        threshold_files: bool\n",
    "            Whether the files are thresholded or not.\n",
    "    Returns:\n",
    "        df: pd.DataFrame\n",
    "            The combined dataframe.\n",
    "\n",
    "    Combines the stats from all the different pipelines into one dataframe.\n",
    "    \"\"\"\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for path, df in find_stats_files(rank, threshold_files):\n",
    "        df[\"Source\"] = path.split(\"/\")[-2]\n",
    "        combined_df = pd.concat([combined_df, df])\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_catplot(df: pd.DataFrame, id_var: str, src: str, plot_type: str, pdf_output: PdfPages, title: str):\n",
    "    melted = df.melt(id_vars=[\"SampleID\", id_var, \"Source\"], var_name=\"Metric\", value_name=\"Value\").dropna()\n",
    "    # Cast value column to float\n",
    "    melted[\"Value\"] = melted[\"Value\"].astype(float)\n",
    "\n",
    "    ax = sns.catplot(data=melted, x=id_var, y=\"Value\", col=\"Metric\", col_wrap=3, kind=plot_type, sharey=False, palette=color_palette)\n",
    "    ax.fig.suptitle(title, y=1.05)\n",
    "\n",
    "    pdf_output.savefig(ax.figure, bbox_inches='tight', dpi=300)\n",
    "    plt.close(ax.figure)\n",
    "\n",
    "def plot_stats(df: pd.DataFrame, output_pdf: str):\n",
    "    pdf_output = PdfPages(output_pdf)\n",
    "    # display(df.head(30))\n",
    "    for th, th_df in df.groupby(\"threshold\"):\n",
    "        for src, df in th_df.groupby(\"Source\"):\n",
    "            title = f'Summary of Statistics for {src} at {th} Threshold'\n",
    "            if src == \"bmock12\" or src == \"camisimGI\" or src ==\"nist\":\n",
    "                make_catplot(df, \"Pipeline\", src, \"bar\", pdf_output, title)\n",
    "\n",
    "            else:\n",
    "                no_average = df.loc[df['SampleID'] != 'Average']\n",
    "                make_catplot(no_average, \"Pipeline\", src, \"box\", pdf_output, title)\n",
    "\n",
    "    pdf_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_pipeline(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    recode_dict = {\"wgsa\": \"wgsa2\", \"wol\": \"woltka\", \"bio4\": \"biobakery4\"}\n",
    "\n",
    "    df[\"Pipeline\"].replace(recode_dict, inplace=True)\n",
    "\n",
    "\n",
    "def cleanup_combined_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    display(df.head(30))\n",
    "    # If the Pipeline value is empty, then replace it with the value in \"Source/Pipeline\".\n",
    "    try:\n",
    "        df.loc[df[\"Pipeline\"].isna(), \"Pipeline\"] = df.loc[df[\"Pipeline\"].isna(), \"Source/Pipeline\"]\n",
    "        # Drop the \"Source/Pipeline\" column.\n",
    "        df = df.drop(columns=[\"Source/Pipeline\"])\n",
    "    except KeyError:\n",
    "        print(\"Warning: No Source/Pipeline column found. Skipping...\")\n",
    "        pass\n",
    "\n",
    "    # Drop rows where the SampleID is \"Average\".\n",
    "    df = df.loc[df[\"SampleID\"] != \"Average\"]\n",
    "\n",
    "    # Recode the pipeline names.\n",
    "    recode_pipeline(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_stats(combine_stats(\"genus\"), \"stats_summary_genus.pdf\")\n",
    "\n",
    "# combined_df = cleanup_combined_df(combine_stats(\"genus\", True))\n",
    "# combined_df.to_csv(\"all_stats_genus.csv\", index=False)\n",
    "\n",
    "combined_df = cleanup_combined_df(combine_stats(\"species\", True))\n",
    "combined_df.to_csv(\"all_stats_species.csv\", index=False)\n",
    "\n",
    "# plot_stats(combined_df, \"stats_summary_species.pdf\")\n",
    "# display(combined_df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only gather stats of interest...\n",
    "pipelines_subset = [\"biobakery4\", \"jams202212\"]\n",
    "threshold_subset = [0.0001]\n",
    "source_subset = [\"mixed\", \"hilo\", \"tourlousse\"]\n",
    "metrics_subset = [\"AD\", \"Sens\", \"FPRA\"]\n",
    "\n",
    "# Open the stats files:\n",
    "stats_files = [\"all_stats_species.csv\", \"all_stats_genus.csv\"]\n",
    "\n",
    "def subset_df(save: bool = False):\n",
    "    for sf in stats_files:\n",
    "        df = pd.read_csv(sf)\n",
    "        # display(df)\n",
    "        df = df.loc[df[\"Pipeline\"].isin(pipelines_subset)]\n",
    "        df = df.loc[df[\"threshold\"].isin(threshold_subset)]\n",
    "        df = df.loc[df[\"Source\"].isin(source_subset)]\n",
    "        df[\"Run\"] = run_num\n",
    "\n",
    "        if save:\n",
    "            df.to_csv(f\"{sf.split('.')[0]}_subset.csv\", index=False)\n",
    "        \n",
    "        rank = sf.split(\"_\")[2].split(\".\")[0]\n",
    "    \n",
    "        yield df, rank\n",
    "    \n",
    "for df, rank in subset_df(save=True):\n",
    "    id_vars = [\"SampleID\", \"Pipeline\", \"Source\"] + metrics_subset\n",
    "    df = df[[\"SampleID\", \"Pipeline\", \"Source\", \"AD\", \"Sens\", \"FPRA\"]]\n",
    "    # display(df)\n",
    "    df = df.melt(id_vars=[\"SampleID\", \"Pipeline\", \"Source\"], var_name=\"Metrics\", value_vars=[\"AD\", \"Sens\", \"FPRA\"])\n",
    "    # display(df.head(30))\n",
    "    g = sns.catplot(data=df, kind=\"bar\", x=\"Pipeline\", col=\"Metrics\", y=\"value\", hue=\"Source\")\n",
    "    g.fig.suptitle(f\"Summary of {rank} stats\", y=1.05)\n",
    "\n",
    "    # Add data labels\n",
    "    for ax in g.axes.flat:\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                        ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.pipeline_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ac10c2973d938bf4f101ae2299756abbb7e00dac649f0769819439ff384650d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
