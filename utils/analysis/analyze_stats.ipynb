{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import os\n",
    "import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import tarfile\n",
    "\n",
    "run_num = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_palette = sns.color_palette(as_cmap=True)\n",
    "\n",
    "color_palette = {\n",
    "    \"Expected\": cb_palette[0],\n",
    "    \"expected\": cb_palette[0],\n",
    "    \"woltka\": cb_palette[1],\n",
    "    \"wol\": cb_palette[1],\n",
    "    \"jams\": cb_palette[2],\n",
    "    \"wgsa\": cb_palette[3],\n",
    "    \"wgsa2\": cb_palette[3],\n",
    "    \"biobakery3\": cb_palette[4],\n",
    "    \"bio3\": cb_palette[4],\n",
    "    \"biobakery4\": cb_palette[5],\n",
    "    \"bio4\": cb_palette[5]\n",
    "}\n",
    "\n",
    "today = datetime.date.today()\n",
    "date = today.strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering and Generating Combined Stats Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.abspath(\"../../pipelines/\")\n",
    "# threshold = 0.0\n",
    "\n",
    "# First, we load the data from the CSV file.\n",
    "\n",
    "\n",
    "def find_stats_files(rank: str, threshold: bool):\n",
    "    for root, dirs, files in os.walk(project_root):\n",
    "        for file in files:\n",
    "            # print(file)\n",
    "            if f\"all_stats_replicates_{rank}\" in file and file.endswith('.csv'):\n",
    "                stats_path = os.path.join(root, file)\n",
    "\n",
    "                df = pd.read_csv(stats_path)\n",
    "\n",
    "                if threshold:\n",
    "                    # Add the threshold to the dataframe\n",
    "                    threshold = file.split(\"_\")[0]\n",
    "                    if threshold == \"all\":\n",
    "                        continue\n",
    "                    elif threshold == \"0.1\":\n",
    "                        continue\n",
    "                    else:\n",
    "                        df[\"threshold\"] = threshold\n",
    "\n",
    "                yield stats_path, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_stats(rank: str, threshold_files: bool) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        rank: str\n",
    "            The rank to combine the stats for.\n",
    "        threshold_files: bool\n",
    "            Whether the files are thresholded or not.\n",
    "    Returns:\n",
    "        df: pd.DataFrame\n",
    "            The combined dataframe.\n",
    "\n",
    "    Combines the stats from all the different pipelines into one dataframe.\n",
    "    \"\"\"\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for path, df in find_stats_files(rank, threshold_files):\n",
    "        df[\"Source\"] = path.split(\"/\")[-2]\n",
    "        combined_df = pd.concat([combined_df, df])\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_catplot(df: pd.DataFrame, id_var: str, src: str, plot_type: str, pdf_output: PdfPages, title: str):\n",
    "    melted = df.melt(id_vars=[\"SampleID\", id_var, \"Source\"],\n",
    "                     var_name=\"Metric\", value_name=\"Value\").dropna()\n",
    "    # Cast value column to float\n",
    "    melted[\"Value\"] = melted[\"Value\"].astype(float)\n",
    "\n",
    "    ax = sns.catplot(data=melted, x=id_var, y=\"Value\", col=\"Metric\",\n",
    "                     col_wrap=3, kind=plot_type, sharey=False, palette=color_palette)\n",
    "    ax.fig.suptitle(title, y=1.05)\n",
    "\n",
    "    pdf_output.savefig(ax.figure, bbox_inches='tight', dpi=300)\n",
    "    plt.close(ax.figure)\n",
    "\n",
    "\n",
    "def plot_stats(df: pd.DataFrame, output_pdf: str):\n",
    "    pdf_output = PdfPages(output_pdf)\n",
    "    # display(df.head(30))\n",
    "    for th, th_df in df.groupby(\"threshold\"):\n",
    "        for src, df in th_df.groupby(\"Source\"):\n",
    "            title = f'Summary of Statistics for {src} at {th} Threshold'\n",
    "            if src == \"bmock12\" or src == \"camisimGI\" or src == \"nist\":\n",
    "                make_catplot(df, \"Pipeline\", src, \"bar\", pdf_output, title)\n",
    "\n",
    "            else:\n",
    "                no_average = df.loc[df['SampleID'] != 'Average']\n",
    "                make_catplot(no_average, \"Pipeline\", src,\n",
    "                             \"box\", pdf_output, title)\n",
    "\n",
    "    pdf_output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_pipeline(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    recode_dict = {\"wgsa\": \"wgsa2\", \"wol\": \"woltka\", \"bio4\": \"biobakery4\"}\n",
    "\n",
    "    df[\"Pipeline\"].replace(recode_dict, inplace=True)\n",
    "\n",
    "\n",
    "def cleanup_combined_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # display(df.head(30))\n",
    "    # If the Pipeline value is empty, then replace it with the value in \"Source/Pipeline\".\n",
    "    try:\n",
    "        df.loc[df[\"Pipeline\"].isna(\n",
    "        ), \"Pipeline\"] = df.loc[df[\"Pipeline\"].isna(), \"Source/Pipeline\"]\n",
    "        # Drop the \"Source/Pipeline\" column.\n",
    "        df = df.drop(columns=[\"Source/Pipeline\"])\n",
    "    except KeyError:\n",
    "        print(\"Warning: No Source/Pipeline column found. Skipping...\")\n",
    "        pass\n",
    "\n",
    "    # Drop rows where the SampleID is \"Average\".\n",
    "    df = df.loc[df[\"SampleID\"] != \"Average\"]\n",
    "\n",
    "    # Recode the pipeline names.\n",
    "    recode_pipeline(df)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_stats(combine_stats(\"genus\"), \"stats_summary_genus.pdf\")\n",
    "\n",
    "def make_stats():\n",
    "    # Check if today's files exist. If they do, we want to delete them because we are going to append to them.\n",
    "    today_genus = f\"results/all_stats_genus.csv\"\n",
    "    today_species = f\"results/all_stats_species.csv\"\n",
    "\n",
    "    if os.path.exists(today_genus):\n",
    "        os.remove(today_genus)\n",
    "    if os.path.exists(today_species):\n",
    "        os.remove(today_species)\n",
    "\n",
    "    combined_df = cleanup_combined_df(combine_stats(\"genus\", True))\n",
    "    combined_df.to_csv(today_genus, index=False, mode='a')\n",
    "\n",
    "    combined_df = cleanup_combined_df(combine_stats(\"species\", True))\n",
    "    combined_df.to_csv(today_species, index=False, mode='a')\n",
    "\n",
    "\n",
    "make_stats()\n",
    "# plot_stats(combined_df, \"stats_summary_species.pdf\")\n",
    "# display(combined_df.head(30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Plotting with Simple Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Subset:\n",
    "    pipelines: List[str]\n",
    "    thresholds: List[float]\n",
    "    sources: List[str]\n",
    "    metrics: List[str]\n",
    "\n",
    "\n",
    "# Only gather stats of interest... no longer using jams202212 as of 2023-03-01.\n",
    "replicates = Subset(\n",
    "    pipelines=[\"biobakery3\", \"biobakery4\", \"jams\", \"wgsa2\", \"woltka\"],\n",
    "    thresholds=[0.0001],\n",
    "    sources=[\"mixed\", \"hilo\", \"tourlousse\"],\n",
    "    metrics=[\"Diversity\", \"AD\", \"Sens\", \"FPRA\", \"Unclassified\"],\n",
    ")\n",
    "\n",
    "one_to_one = Subset(\n",
    "    pipelines=[\"biobakery3\", \"biobakery4\", \"jams\", \"wgsa2\", \"woltka\"],\n",
    "    thresholds=[0.0001],\n",
    "    sources=[\"bmock12\", \"camisimGI\", \"nist\"],\n",
    "    metrics=[\"Diversity\", \"AD\", \"Sens\", \"FPRA\", \"Unclassified\"],\n",
    ")\n",
    "\n",
    "rename_pipeline_dict = {\n",
    "    \"biobakery3\": \"Biobakery3\",\n",
    "    \"biobakery4\": \"Biobakery4\",\n",
    "    \"jams\": \"JAMS\",\n",
    "    \"wgsa2\": \"WGSA2\",\n",
    "    \"woltka\": \"Woltka\"\n",
    "}\n",
    "\n",
    "rename_source_dict = {\n",
    "    \"mixed\": \"Amos Mixed\",\n",
    "    \"hilo\": \"Amos HiLo\",\n",
    "    \"tourlousse\": \"Tourlousse\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the stats files:\n",
    "stats_files = [f\"results/all_stats_species.csv\",\n",
    "               f\"results/all_stats_genus.csv\"]\n",
    "\n",
    "\n",
    "def subset_df(sub: Subset, save: bool = False):\n",
    "    for sf in stats_files:\n",
    "        df = pd.read_csv(sf)\n",
    "        # display(df)\n",
    "        df = df.loc[df[\"Pipeline\"].isin(sub.pipelines)]\n",
    "        df = df.loc[df[\"threshold\"].isin(sub.thresholds)]\n",
    "        df = df.loc[df[\"Source\"].isin(sub.sources)]\n",
    "        df[\"Run\"] = run_num\n",
    "\n",
    "        if save:\n",
    "            df.to_csv(f\"{sf.split('.')[0]}_subset.csv\", index=False)\n",
    "\n",
    "        rank = sf.split(\"_\")[2].split(\".\")[0]\n",
    "\n",
    "        yield df, rank\n",
    "\n",
    "\n",
    "def plot_subset(sub: Subset, tag: str, pdf_output: PdfPages):\n",
    "    for df, rank in subset_df(sub, save=True):\n",
    "        id_vars = [\"SampleID\", \"Pipeline\", \"Source\"] + sub.metrics\n",
    "        df = df[[\"SampleID\", \"Pipeline\", \"Source\", \"AD\", \"Sens\", \"FPRA\"]]\n",
    "        # display(df)\n",
    "        df = df.melt(id_vars=[\"SampleID\", \"Pipeline\", \"Source\"],\n",
    "                     var_name=\"Metrics\", value_vars=[\"AD\", \"Sens\", \"FPRA\"])\n",
    "\n",
    "        # Rename the pipelines.\n",
    "        df[\"Pipeline\"].replace(rename_pipeline_dict, inplace=True)\n",
    "\n",
    "        # We need to make a figure object no bigger than 8.5x11 inches.\n",
    "        if tag == \"One-to-One\":\n",
    "            # df[\"Community\"] = df[\"SampleID\"] + \"_\" + df[\"Source\"]\n",
    "            # g = sns.catplot(data=df, kind=\"bar\", x=\"Pipeline\", col=\"Metrics\", row=\"Source\", y=\"value\", hue=\"Community\")\n",
    "\n",
    "            for src, src_df in df.groupby(\"Source\"):\n",
    "                print(src)\n",
    "                g = sns.catplot(data=src_df, kind=\"bar\", x=\"Pipeline\",\n",
    "                                col=\"Metrics\", y=\"value\", hue=\"SampleID\", sharey=True)\n",
    "                # Change y-axis label to \"Assessment Metric\"\n",
    "                g.set(ylabel=\"Assessment Metric\")\n",
    "                g.fig.suptitle(\n",
    "                    f\"Summary of {rank.capitalize()} Stats for {src} Community\", y=1.025)\n",
    "\n",
    "                # Add data labels\n",
    "                # for ax in g.axes.flat:\n",
    "                #     for p in ax.patches:\n",
    "                #         ax.annotate(f\"{p.get_height():.0f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                #             ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "                g.fig.savefig(\n",
    "                    f\"results/images/{rank}_stats_{src}.png\", bbox_inches='tight', dpi=300)\n",
    "                pdf_output.savefig(g.fig, bbox_inches='tight', dpi=300)\n",
    "                plt.close(g.fig)\n",
    "\n",
    "            # df[\"Community\"] = df[\"SampleID\"] + \"_\" + df[\"Source\"]\n",
    "            # g = sns.catplot(data=df, kind=\"bar\", x=\"Pipeline\", col=\"Metrics\", row=\"Community\", y=\"value\", hue=\"Source\") -- original\n",
    "            # g = sns.catplot(data=df, kind=\"bar\", x=\"Pipeline\", col=\"Community\", y=\"value\", hue=\"Metrics\", col_wrap=3)\n",
    "            # g = sns.catplot(data=df, kind=\"bar\", x=\"Pipeline\", col=\"Source\", row=\"SampleID\", y=\"value\", hue=\"Metrics\", sharey=False)\n",
    "\n",
    "        else:\n",
    "            # Rname the sources.\n",
    "            df[\"Source\"].replace(rename_source_dict, inplace=True)\n",
    "            g = sns.catplot(data=df, kind=\"bar\", x=\"Pipeline\",\n",
    "                            col=\"Metrics\", y=\"value\", hue=\"Source\", sharey=True)\n",
    "\n",
    "        g.fig.suptitle(\n",
    "            f\"Summary of {rank.capitalize()} Stats for {tag} Communities\", y=1.05)\n",
    "        g.set(ylabel=\"Assessment Metric\")\n",
    "\n",
    "        # Add data labels\n",
    "        # for ax in g.axes.flat:\n",
    "        #     for p in ax.patches:\n",
    "        #         ax.annotate(f\"{p.get_height():.0f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "        #                     ha='center', va='center', xytext=(0, 10), textcoords='offset points', fontsize=6)\n",
    "\n",
    "        # save to png as well\n",
    "        g.fig.savefig(\n",
    "            f\"results/images/{rank}_stats_{tag}.png\", bbox_inches='tight', dpi=300)\n",
    "        pdf_output.savefig(g.fig, bbox_inches='tight', dpi=300)\n",
    "        plt.close(g.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tar_images():\n",
    "    if len(os.listdir(\"results/images\")) == 0:\n",
    "        return\n",
    "\n",
    "    with tarfile.open(f\"results/images.tar.gz\", \"w:gz\") as tar:\n",
    "        tar.add(\"results/images\", arcname=os.path.basename(\"results/images\"))\n",
    "\n",
    "    # Clear the images in the directory\n",
    "    for f in os.listdir(\"results/images\"):\n",
    "        os.remove(os.path.join(\"results/images\", f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_output = PdfPages(f\"results/summary_stats_{today}.pdf\")\n",
    "plot_subset(sub=replicates, tag=\"Replicate\", pdf_output=pdf_output)\n",
    "plot_subset(sub=one_to_one, tag=\"One-to-One\", pdf_output=pdf_output)\n",
    "\n",
    "# tar_images()\n",
    "\n",
    "pdf_output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_images()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Boxplots for Total Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from statannotations.Annotator import Annotator\n",
    "from itertools import combinations\n",
    "from scipy.stats import f_oneway, kruskal, wilcoxon\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "\n",
    "wanted_sheets = [\"AD_Tidy_NF\", \"Sens_Tidy_NF\", \"FPRA_Tidy_NF\"]\n",
    "\n",
    "score_table = pd.read_excel(\n",
    "    \"../paper/ad_table_04062023.xlsx\", sheet_name=wanted_sheets)\n",
    "for df in score_table.values():\n",
    "    display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_df(df: pd.DataFrame):\n",
    "    \"\"\" Takes input of corrected p-values and returns a dataframe with correct rounding and bolding.\"\"\"\n",
    "\n",
    "    # If val < 0.05, then it is significant and it should be bolded. If it is \"-\", then is should be left alone.\n",
    "    df = df.style.applymap(\n",
    "        lambda val: \"font-weight: bold\" if float(val) < 0.05 else \"\")\n",
    "\n",
    "    display(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annontated_boxplot(stat: str):\n",
    "    # We need to convert to tidy format.\n",
    "    name = stat.split(\"_\")[0]\n",
    "    AD = score_table[stat].melt(\n",
    "        id_vars=[\"Community\"], var_name=\"SampleID\", value_name=name)\n",
    "\n",
    "    display(AD.head())\n",
    "    # Get unique values in SampleID column\n",
    "    pipelines = AD[\"SampleID\"].unique()\n",
    "    display(pipelines)\n",
    "\n",
    "    # Make a list of tuples of all pairs of pipelines\n",
    "    pairs = list(combinations(pipelines, 2))\n",
    "\n",
    "    # Boxplot of AD\n",
    "    g = sns.boxplot(data=AD, x=\"SampleID\", y=name)\n",
    "\n",
    "    ann = Annotator(g, pairs, data=AD, x=\"SampleID\", y=name)\n",
    "    ann.configure(test=\"Wilcoxon\", text_format=\"star\", loc=\"inside\", verbose=2)\n",
    "    ann.apply_and_annotate()\n",
    "\n",
    "    g.set_title(f\"Wilcoxon Test of {name} Between Pipelines\")\n",
    "\n",
    "    plt.savefig(\n",
    "        f\"results/stats_tex/{name}_boxplot.png\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pairwise_analysis(df: pd.DataFrame, stat: str):\n",
    "    print(stat)\n",
    "    stat_short = stat.split(\"_\")[0]\n",
    "    values_df = df[stat]\n",
    "    display(values_df.head())\n",
    "\n",
    "    # Get the columns, drop the community column\n",
    "    pipelines = values_df.columns[1:]\n",
    "\n",
    "    # Make a dataframe of the values on both the x and y axis\n",
    "    a = pd.DataFrame(np.zeros((len(pipelines), len(pipelines))),\n",
    "                     index=pipelines, columns=pipelines)\n",
    "\n",
    "    # Make a list of tuples of all pairs of pipelines\n",
    "    pairs = list(combinations(pipelines, 2))\n",
    "\n",
    "    # Now do Wilcoxon on all the pairs\n",
    "    for pair in pairs:\n",
    "        h, p = wilcoxon(values_df[pair[0]],\n",
    "                        values_df[pair[1]], correction=False)\n",
    "        a.loc[pair[1], pair[0]] = p\n",
    "\n",
    "    # Fill 0 with \"nan\"\n",
    "    a = a.replace(0, np.NAN)\n",
    "\n",
    "    # Flatten a to a 1D array\n",
    "    b = pd.DataFrame(a.stack(), columns=[\"p-val\"])\n",
    "\n",
    "    display(b)\n",
    "\n",
    "    # Apply the Bonferroni correction\n",
    "    b_corrected = multipletests(b[\"p-val\"], method=\"fdr_bh\")\n",
    "\n",
    "    b[\"corrected_p-val\"] = b_corrected[1]\n",
    "    b.drop(\"p-val\", axis=1, inplace=True)\n",
    "\n",
    "    # Reshape b to a square matrix\n",
    "    b = b.unstack()\n",
    "    b = b.droplevel(0, axis=1)\n",
    "\n",
    "    b = cleanup_df(b)\n",
    "\n",
    "    caption = f\"Pairwise Wilcoxon test for {stat_short} between all pipelines. The FDR-BH corrected p-values are shown in the table. Bold values indicate < 0.05.\"\n",
    "    label = f\"tab:pairwise_{stat_short}\"\n",
    "    b.to_latex(f\"results/stats_tex/{stat_short}_pairwise.tex\",\n",
    "               convert_css=True, caption=caption, label=label)\n",
    "\n",
    "\n",
    "def group_analysis(df: pd.DataFrame, stat: str):\n",
    "    print(stat)\n",
    "    values_df = df[stat]\n",
    "    display(values_df.head())\n",
    "    # f, p = f_oneway(values_df[\"Biobakery4\"], values_df[\"Biobakery3\"], values_df[\"JAMS\"], values_df[\"WGSA2\"], values_df[\"Woltka\"])\n",
    "    # print(f, p)\n",
    "\n",
    "    # Now do Kruskal-Wallis\n",
    "    h, p = kruskal(values_df[\"Biobakery4\"], values_df[\"Biobakery3\"],\n",
    "                   values_df[\"JAMS\"], values_df[\"WGSA2\"], values_df[\"Woltka\"])\n",
    "    print(\"Kruskal-Wallis: \", h, p)\n",
    "    return h, p\n",
    "\n",
    "\n",
    "def do_group_stats():\n",
    "    stats = []\n",
    "    for stat in wanted_sheets:\n",
    "        h, p = group_analysis(score_table, stat)\n",
    "        stats.append([stat, h, p])\n",
    "\n",
    "        pairwise_analysis(score_table, stat)\n",
    "        annontated_boxplot(stat)\n",
    "\n",
    "    stats_df = pd.DataFrame(stats, columns=[\"Stat\", \"H\", \"P\"])\n",
    "    stats_df.to_csv(\"kw_results.csv\", index=False)\n",
    "\n",
    "\n",
    "do_group_stats()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.pipeline_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ac10c2973d938bf4f101ae2299756abbb7e00dac649f0769819439ff384650d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
