{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import os\n",
    "import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import tarfile\n",
    "\n",
    "run_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_palette = sns.color_palette(as_cmap=True)\n",
    "\n",
    "color_palette = {\n",
    "    \"Expected\": cb_palette[0], \n",
    "    \"expected\": cb_palette[0], \n",
    "    \"woltka\": cb_palette[1], \n",
    "    \"wol\": cb_palette[1], \n",
    "    \"jams\": cb_palette[2], \n",
    "    \"wgsa\": cb_palette[3], \n",
    "    \"wgsa2\": cb_palette[3], \n",
    "    \"biobakery3\": cb_palette[4], \n",
    "    \"bio3\": cb_palette[4], \n",
    "    \"biobakery4\": cb_palette[5], \n",
    "    \"bio4\": cb_palette[5]\n",
    "}\n",
    "\n",
    "today = datetime.date.today()\n",
    "date = today.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering and Generating Combined Stats Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.abspath(\"../../pipelines/\")\n",
    "# threshold = 0.0\n",
    "\n",
    "# First, we load the data from the CSV file.\n",
    "def find_stats_files(rank: str, threshold: bool):\n",
    "    for root, dirs, files in os.walk(project_root):\n",
    "        for file in files:\n",
    "            # print(file)\n",
    "            if f\"all_stats_replicates_{rank}\" in file and file.endswith('.csv'):\n",
    "                stats_path = os.path.join(root, file)\n",
    "\n",
    "                df = pd.read_csv(stats_path)\n",
    "\n",
    "                if threshold:\n",
    "                    # Add the threshold to the dataframe\n",
    "                    threshold = file.split(\"_\")[0]\n",
    "                    if threshold == \"all\":\n",
    "                        continue\n",
    "                    elif threshold == \"0.1\":\n",
    "                        continue\n",
    "                    else:   \n",
    "                        df[\"threshold\"] = threshold\n",
    "                \n",
    "                yield stats_path, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_stats(rank: str, threshold_files: bool) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        rank: str\n",
    "            The rank to combine the stats for.\n",
    "        threshold_files: bool\n",
    "            Whether the files are thresholded or not.\n",
    "    Returns:\n",
    "        df: pd.DataFrame\n",
    "            The combined dataframe.\n",
    "\n",
    "    Combines the stats from all the different pipelines into one dataframe.\n",
    "    \"\"\"\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for path, df in find_stats_files(rank, threshold_files):\n",
    "        df[\"Source\"] = path.split(\"/\")[-2]\n",
    "        combined_df = pd.concat([combined_df, df])\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_catplot(df: pd.DataFrame, id_var: str, src: str, plot_type: str, pdf_output: PdfPages, title: str):\n",
    "    melted = df.melt(id_vars=[\"SampleID\", id_var, \"Source\"], var_name=\"Metric\", value_name=\"Value\").dropna()\n",
    "    # Cast value column to float\n",
    "    melted[\"Value\"] = melted[\"Value\"].astype(float)\n",
    "\n",
    "    ax = sns.catplot(data=melted, x=id_var, y=\"Value\", col=\"Metric\", col_wrap=3, kind=plot_type, sharey=False, palette=color_palette)\n",
    "    ax.fig.suptitle(title, y=1.05)\n",
    "\n",
    "    pdf_output.savefig(ax.figure, bbox_inches='tight', dpi=300)\n",
    "    plt.close(ax.figure)\n",
    "\n",
    "def plot_stats(df: pd.DataFrame, output_pdf: str):\n",
    "    pdf_output = PdfPages(output_pdf)\n",
    "    # display(df.head(30))\n",
    "    for th, th_df in df.groupby(\"threshold\"):\n",
    "        for src, df in th_df.groupby(\"Source\"):\n",
    "            title = f'Summary of Statistics for {src} at {th} Threshold'\n",
    "            if src == \"bmock12\" or src == \"camisimGI\" or src ==\"nist\":\n",
    "                make_catplot(df, \"Pipeline\", src, \"bar\", pdf_output, title)\n",
    "\n",
    "            else:\n",
    "                no_average = df.loc[df['SampleID'] != 'Average']\n",
    "                make_catplot(no_average, \"Pipeline\", src, \"box\", pdf_output, title)\n",
    "\n",
    "    pdf_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_pipeline(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    recode_dict = {\"wgsa\": \"wgsa2\", \"wol\": \"woltka\", \"bio4\": \"biobakery4\"}\n",
    "\n",
    "    df[\"Pipeline\"].replace(recode_dict, inplace=True)\n",
    "\n",
    "\n",
    "def cleanup_combined_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # display(df.head(30))\n",
    "    # If the Pipeline value is empty, then replace it with the value in \"Source/Pipeline\".\n",
    "    try:\n",
    "        df.loc[df[\"Pipeline\"].isna(), \"Pipeline\"] = df.loc[df[\"Pipeline\"].isna(), \"Source/Pipeline\"]\n",
    "        # Drop the \"Source/Pipeline\" column.\n",
    "        df = df.drop(columns=[\"Source/Pipeline\"])\n",
    "    except KeyError:\n",
    "        print(\"Warning: No Source/Pipeline column found. Skipping...\")\n",
    "        pass\n",
    "\n",
    "    # Drop rows where the SampleID is \"Average\".\n",
    "    df = df.loc[df[\"SampleID\"] != \"Average\"]\n",
    "\n",
    "    # Recode the pipeline names.\n",
    "    recode_pipeline(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_stats(combine_stats(\"genus\"), \"stats_summary_genus.pdf\")\n",
    "\n",
    "def make_stats():\n",
    "    # Check if today's files exist. If they do, we want to delete them because we are going to append to them.\n",
    "    today_genus = f\"results/all_stats_genus_{today}.csv\"\n",
    "    today_species = f\"results/all_stats_species_{today}.csv\"\n",
    "\n",
    "    if os.path.exists(today_genus):\n",
    "        os.remove(today_genus)\n",
    "    if os.path.exists(today_species):\n",
    "        os.remove(today_species)\n",
    "    \n",
    "    combined_df = cleanup_combined_df(combine_stats(\"genus\", True))\n",
    "    combined_df.to_csv(today_genus, index=False, mode='a')\n",
    "\n",
    "    combined_df = cleanup_combined_df(combine_stats(\"species\", True))\n",
    "    combined_df.to_csv(today_species, index=False, mode='a')\n",
    "\n",
    "make_stats()\n",
    "# plot_stats(combined_df, \"stats_summary_species.pdf\")\n",
    "# display(combined_df.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Plotting with Simple Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Subset:\n",
    "    pipelines: List[str]\n",
    "    thresholds: List[float]\n",
    "    sources: List[str]\n",
    "    metrics: List[str]\n",
    "\n",
    "# Only gather stats of interest... no longer using jams202212 as of 2023-03-01.\n",
    "replicates = Subset(\n",
    "    pipelines = [\"biobakery3\", \"biobakery4\", \"jams\", \"wgsa2\", \"woltka\"],\n",
    "    thresholds = [0.0001],\n",
    "    sources = [\"mixed\", \"hilo\", \"tourlousse\"],\n",
    "    metrics = [\"AD\", \"Sens\", \"FPRA\"],\n",
    ")\n",
    "\n",
    "one_to_one = Subset(\n",
    "    pipelines = [\"biobakery3\", \"biobakery4\", \"jams\", \"wgsa2\", \"woltka\"],\n",
    "    thresholds = [0.0001],\n",
    "    sources = [\"bmock12\", \"camisimGI\", \"nist\"],\n",
    "    metrics = [\"AD\", \"Sens\", \"FPRA\"],\n",
    ")\n",
    "\n",
    "rename_pipeline_dict = {\n",
    "    \"biobakery3\": \"Biobakery3\", \n",
    "    \"biobakery4\": \"Biobakery4\", \n",
    "    \"jams\": \"JAMS\", \n",
    "    \"wgsa2\": \"WGSA2\", \n",
    "    \"woltka\": \"Woltka\"\n",
    "}\n",
    "\n",
    "rename_source_dict = {\n",
    "    \"mixed\": \"Amos Mixed\",\n",
    "    \"hilo\": \"Amos HiLo\",\n",
    "    \"tourlousse\": \"Tourlousse\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the stats files:\n",
    "stats_files = [f\"results/all_stats_species_{today}.csv\", f\"results/all_stats_genus_{today}.csv\"]\n",
    "\n",
    "def subset_df(sub: Subset, save: bool = False):\n",
    "    for sf in stats_files:\n",
    "        df = pd.read_csv(sf)\n",
    "        # display(df)\n",
    "        df = df.loc[df[\"Pipeline\"].isin(sub.pipelines)]\n",
    "        df = df.loc[df[\"threshold\"].isin(sub.thresholds)]\n",
    "        df = df.loc[df[\"Source\"].isin(sub.sources)]\n",
    "        df[\"Run\"] = run_num\n",
    "\n",
    "        if save:\n",
    "            df.to_csv(f\"{sf.split('.')[0]}_subset.csv\", index=False)\n",
    "        \n",
    "        rank = sf.split(\"_\")[2].split(\".\")[0]\n",
    "    \n",
    "        yield df, rank\n",
    "\n",
    "\n",
    "def plot_subset(sub: Subset, tag: str, pdf_output: PdfPages):\n",
    "    for df, rank in subset_df(sub, save=True):\n",
    "        id_vars = [\"SampleID\", \"Pipeline\", \"Source\"] + sub.metrics\n",
    "        df = df[[\"SampleID\", \"Pipeline\", \"Source\", \"AD\", \"Sens\", \"FPRA\"]]\n",
    "        # display(df)\n",
    "        df = df.melt(id_vars=[\"SampleID\", \"Pipeline\", \"Source\"], var_name=\"Metrics\", value_vars=[\"AD\", \"Sens\", \"FPRA\"])\n",
    "\n",
    "        # Rename the pipelines.\n",
    "        df[\"Pipeline\"].replace(rename_pipeline_dict, inplace=True)\n",
    "\n",
    "        # We need to make a figure object no bigger than 8.5x11 inches.\n",
    "        if tag == \"One-to-One\":\n",
    "            # df[\"Community\"] = df[\"SampleID\"] + \"_\" + df[\"Source\"]\n",
    "            # g = sns.catplot(data=df, kind=\"bar\", x=\"Pipeline\", col=\"Metrics\", row=\"Source\", y=\"value\", hue=\"Community\")\n",
    "\n",
    "            for src, src_df in df.groupby(\"Source\"):\n",
    "                print(src)\n",
    "                g = sns.catplot(data=src_df, kind=\"bar\", x=\"Pipeline\", col=\"Metrics\", y=\"value\", hue=\"SampleID\", sharey=True)\n",
    "                # Change y-axis label to \"Assessment Metric\"\n",
    "                g.set(ylabel=\"Assessment Metric\")\n",
    "                g.fig.suptitle(f\"Summary of {rank.capitalize()} Stats for {src} Community\", y=1.025)\n",
    "\n",
    "                # Add data labels\n",
    "                # for ax in g.axes.flat:\n",
    "                #     for p in ax.patches:\n",
    "                #         ax.annotate(f\"{p.get_height():.0f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                #             ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "                g.fig.savefig(f\"results/images/{rank}_stats_{src}.png\", bbox_inches='tight', dpi=300) \n",
    "                pdf_output.savefig(g.fig, bbox_inches='tight', dpi=300)\n",
    "                plt.close(g.fig)\n",
    "\n",
    "            # df[\"Community\"] = df[\"SampleID\"] + \"_\" + df[\"Source\"]\n",
    "            # g = sns.catplot(data=df, kind=\"bar\", x=\"Pipeline\", col=\"Metrics\", row=\"Community\", y=\"value\", hue=\"Source\") -- original\n",
    "            # g = sns.catplot(data=df, kind=\"bar\", x=\"Pipeline\", col=\"Community\", y=\"value\", hue=\"Metrics\", col_wrap=3)\n",
    "            # g = sns.catplot(data=df, kind=\"bar\", x=\"Pipeline\", col=\"Source\", row=\"SampleID\", y=\"value\", hue=\"Metrics\", sharey=False)\n",
    "\n",
    "        else:\n",
    "            # Rname the sources.\n",
    "            df[\"Source\"].replace(rename_source_dict, inplace=True)\n",
    "            g = sns.catplot(data=df, kind=\"bar\", x=\"Pipeline\", col=\"Metrics\", y=\"value\", hue=\"Source\", sharey=True)\n",
    "\n",
    "\n",
    "        g.fig.suptitle(f\"Summary of {rank.capitalize()} Stats for {tag} Communities\", y=1.05)\n",
    "        g.set(ylabel=\"Assessment Metric\")\n",
    "\n",
    "        # Add data labels\n",
    "        # for ax in g.axes.flat:\n",
    "        #     for p in ax.patches:\n",
    "        #         ax.annotate(f\"{p.get_height():.0f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "        #                     ha='center', va='center', xytext=(0, 10), textcoords='offset points', fontsize=6)\n",
    "\n",
    "        # save to png as well\n",
    "        g.fig.savefig(f\"results/images/{rank}_stats_{tag}.png\", bbox_inches='tight', dpi=300) \n",
    "        pdf_output.savefig(g.fig, bbox_inches='tight', dpi=300)\n",
    "        plt.close(g.fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tar_images():\n",
    "    if len(os.listdir(\"results/images\")) == 0:\n",
    "        return\n",
    "\n",
    "    with tarfile.open(f\"results/images.tar.gz\", \"w:gz\") as tar:\n",
    "        tar.add(\"results/images\", arcname=os.path.basename(\"results/images\"))\n",
    "    \n",
    "    # Clear the images in the directory\n",
    "    for f in os.listdir(\"results/images\"):\n",
    "        os.remove(os.path.join(\"results/images\", f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bmock12\n",
      "camisimGI\n",
      "nist\n",
      "bmock12\n",
      "camisimGI\n",
      "nist\n"
     ]
    }
   ],
   "source": [
    "pdf_output = PdfPages(f\"results/summary_stats_{today}.pdf\")\n",
    "plot_subset(sub=replicates, tag=\"Replicate\", pdf_output=pdf_output)\n",
    "plot_subset(sub=one_to_one, tag=\"One-to-One\", pdf_output=pdf_output)\n",
    "\n",
    "# tar_images()\n",
    "\n",
    "pdf_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_images()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.pipeline_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ac10c2973d938bf4f101ae2299756abbb7e00dac649f0769819439ff384650d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
