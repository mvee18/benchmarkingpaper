{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "from functools import reduce\n",
    "from numpy.typing import ArrayLike\n",
    "import warnings\n",
    "import openpyxl\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\") # noqa\n",
    "\n",
    "from typing import Generator, Tuple\n",
    "\n",
    "from python_src.figures_utils import get_all_expected, generate_experimental_df, get_relabund_files, fully_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIST Sample Sensitivity Analysis\n",
    "\n",
    "Our goal is to use the five NIST samples to conduct an analysis of TP, TN, FP, and FN scores across the bacterial species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should read in the expected data and make a large dataframe.\n",
    "\n",
    "expected_df = get_all_expected(\"../../expected_pipelines/nist/\", rank=\"species\")\n",
    "expected_df.drop(columns=[\"Source\"], inplace=True)\n",
    "display(expected_df)\n",
    "\n",
    "expected_df.reset_index(inplace=True)\n",
    "df = expected_df.pivot(index=[\"species\", \"TAX_ID\"], columns=\"SampleID\", values=\"RA\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = fully_combined(root_dir=\"../../pipelines/nist\", expected_root=\"../../expected_pipelines/nist\", rank=\"species\")\n",
    "display(fc)\n",
    "\n",
    "negatives = get_relabund_files(\"negatives\", rank=\"species\")\n",
    "display(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframes():\n",
    "    for smpl, smpl_df in fc.groupby(\"SampleID\"):\n",
    "        exp = smpl_df.loc[smpl_df[\"Source\"] == \"Expected\"]\n",
    "        obs = smpl_df.loc[smpl_df[\"Source\"] != \"Expected\"]\n",
    "\n",
    "        # Take from the expected dataframe the index, species, and RA.\n",
    "        final_df = pd.DataFrame()\n",
    "        final_df.index = exp.index\n",
    "        final_df[\"species\"] = exp[\"species\"]\n",
    "        final_df[\"RA_exp\"] = exp[\"RA\"]\n",
    "\n",
    "        data_frames = smpl_df.groupby(\"Source\")\n",
    "        for pl, pl_df in obs.groupby(\"Source\"):\n",
    "            # left join with exp\n",
    "            merged = exp.merge(pl_df, on=\"TAX_ID\", how=\"left\", suffixes=(\"_exp\", \"_obs\"))\n",
    "\n",
    "            # Don't need most of these columns, want final table to be TAXID, Species Name, SampleID_exp, BB3, BB4, etc.\n",
    "            merged.drop(columns=[\"Source_exp\", \"Source_obs\", \"species_obs\", \"SampleID_exp\", \"SampleID_obs\"], inplace=True)\n",
    "\n",
    "            # Rename the columns to be SampleID_exp, SampleID_obs\n",
    "            merged = merged.rename(columns={\"RA_exp\": f\"{smpl}_exp\", \"RA_obs\": f\"{pl}_obs\"})\n",
    "\n",
    "            final_df = pd.concat([final_df, merged[f\"{pl}_obs\"]], axis=1, sort=False)\n",
    "\n",
    "        final_df.index.name = \"TAX_ID\"\n",
    "        final_df[\"SampleID\"] = smpl\n",
    "        # display(final_df.head())\n",
    "        yield final_df\n",
    "\n",
    "\"\"\"\n",
    "def calculate_stats(df: pd.DataFrame, pl: str):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    # TP condition: species is in expected and observed = 0\n",
    "    # FP condition: species is in observed but not expected = 1\n",
    "    # FN condition: species is in expected but not observed = 2\n",
    "    # TN condition: species is not in expected or observed = 3\n",
    "\n",
    "    conditions = [\n",
    "        (df[\"RA_exp\"] > 0) & (df[f\"{pl}_obs\"] > 0),\n",
    "        (df[\"RA_exp\"] == 0) & (df[f\"{pl}_obs\"] > 0),\n",
    "        (df[\"RA_exp\"] > 0) & (df[f\"{pl}_obs\"] == 0),\n",
    "        (df[\"RA_exp\"] == 0) & (df[f\"{pl}_obs\"] == 0),\n",
    "    ]\n",
    "\n",
    "    choices = [0, 1, 2, 3]\n",
    "\n",
    "    df[\"score\"] = np.select(conditions, choices)\n",
    "\n",
    "    # Now, summarize the number of each condition\n",
    "\n",
    "    summary = df[\"score\"].value_counts()\n",
    "    print(summary)\n",
    "\n",
    "    display(df.head()) \n",
    "\"\"\"\n",
    "\n",
    "def pivot_df(dataframes: Generator[pd.DataFrame, None, None]) -> Generator[Tuple[pd.DataFrame, str], None, None]:\n",
    "    \"\"\"\n",
    "    Takes the dataframes from make_dataframes and pivots them so that the SampleID values are the columns.\n",
    "\n",
    "    Parameters:\n",
    "        dataframes: Generator[pd.DataFrame]\n",
    "            The dataframes from make_dataframes\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame as a generator\n",
    "    \n",
    "    \"\"\"\n",
    "    pipelines = [\"biobakery3\", \"biobakery4\", \"jams\", \"wgsa2\", \"woltka\"]\n",
    "\n",
    "    for pl in pipelines:\n",
    "        merged = pd.DataFrame()\n",
    "        for dfs in make_dataframes():\n",
    "            pl_df = dfs[[\"species\", \"RA_exp\", f\"{pl}_obs\", \"SampleID\"]]\n",
    "            merged = pd.concat([merged, pl_df], axis=0, sort=False)\n",
    "            # calculate_stats(pl_df, pl)\n",
    "\n",
    "        # display(merged.head())\n",
    "        # print(merged.shape)\n",
    "\n",
    "        # Make the sampleID values the columns\n",
    "        print(f\"Pipeline: {pl}\")\n",
    "        piv = merged.pivot(index=[\"species\", \"RA_exp\"], columns=\"SampleID\", values=f\"{pl}_obs\")\n",
    "        display(piv)\n",
    "\n",
    "        yield (piv, pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have several conditions. \n",
    "# One set is if both values are above the treshold. If so, we can check if:\n",
    "# 1. Both are above the threshold (TP)\n",
    "# 2. Both are zero. (TN)\n",
    "# 3. Observed is above the threshold and the expected is zero. (FP)\n",
    "# 4. Observed is below the threshold and the expected is above the threshold. (FN)\n",
    "\n",
    "# Unfortunately, sometimes the expected is below the threshold. In this case, we need to relax the observed threshold to above 0.\n",
    "# Thus, a TP would be if the observed is above 0 and the expected is above 0. A TN would be if the observed is 0 and the expected is 0.\n",
    "\n",
    "def confusion_matrix(df: pd.DataFrame, threshold: float):\n",
    "    output_list = []\n",
    "    for spec, spec_df in df.groupby(\"species\"):\n",
    "        # Initialize the confusion matrix\n",
    "        tp, fp, fn, tn = 0, 0, 0, 0\n",
    "\n",
    "        for ind in spec_df.index:\n",
    "            ra_exp = ind[1]\n",
    "            # display(spec_df.loc[[ind]])\n",
    "\n",
    "            # Get index of True value in the row\n",
    "            row_values = spec_df.loc[[ind]].values.reshape(-1)\n",
    "            truth_array = spec_df.loc[[ind]].notna().values.reshape(-1)\n",
    "            # print(truth_array)\n",
    "            idx = [i for i, x in enumerate(truth_array) if x]\n",
    "\n",
    "            if len(idx) > 1:\n",
    "                raise Exception(\"More than one value in row\")\n",
    "            \n",
    "            # Means that the pipeline did not find the value at all. This can either be a true negative or a false negative.\n",
    "            ra_obs: float = None\n",
    "\n",
    "            try:\n",
    "                # display(spec, spec_df.loc[[ind]].values.reshape(-1))\n",
    "                ra_obs = spec_df.loc[[ind]].values.reshape(-1)[idx][0]\n",
    "            except IndexError:\n",
    "                print(\"Could not find spec: \", spec)\n",
    "                # If we can't find the value, then the pipeline did not find it.\n",
    "                ra_obs = 0\n",
    "\n",
    "            # If the value is less than the threshold, it is not considered to be present. \n",
    "            \n",
    "            # True positive\n",
    "            if ra_exp > threshold and ra_obs > threshold:\n",
    "                tp += 1\n",
    "            # False negative\n",
    "            elif ra_exp > threshold and ra_obs <= threshold:\n",
    "                fn += 1\n",
    "            # False positive\n",
    "            elif ra_exp == 0 and ra_obs > threshold:\n",
    "                fp += 1\n",
    "            # True negative\n",
    "            elif ra_exp == 0 and ra_obs <= threshold:\n",
    "                tn += 1\n",
    "            else:\n",
    "                if ra_exp < threshold and ra_exp != 0.0:\n",
    "                    # This warning is necessary because sometimes the expected value is below the threshold. If so, the observed\n",
    "                    # threshold needs to be relaxed to some other value.\n",
    "                    warnings.warn(f\"WARN: Expected value for {spec} is below threshold of {threshold} with value: {ra_exp}. The filtering criteria for the observed value will be relaxed to above 0 for a TP.\")\n",
    "                    # If the expected value is below the threshold, we need to relax the observed threshold to above 0.\n",
    "                    if ra_obs > 0:\n",
    "                        tp += 1\n",
    "                    # If the expected value is then zero, we have a false negative.\n",
    "                    if ra_obs == 0:\n",
    "                        fn += 1\n",
    "                else:\n",
    "                    raise Exception(f\"Unexpected condition: {ra_exp}, {ra_obs}\")\n",
    "\n",
    "        # Verify that the sum is five since we have five samples.\n",
    "        if tp + fp + fn + tn != 5:\n",
    "            print(f\"Results for {spec}: {tp}, {fp}, {fn}, {tn}\")\n",
    "            raise Exception(\"Sum of confusion matrix is not 5\")\n",
    "\n",
    "        # print(f\"Results for {spec}: {tp}, {fp}, {fn}, {tn}\")\n",
    "        output_list.append([spec, tp, fp, fn, tn])\n",
    "\n",
    "    output_df = pd.DataFrame(output_list, columns=[\"species\", \"tp\", \"fp\", \"fn\", \"tn\"])\n",
    "    return output_df\n",
    "\n",
    "def performance_metrics(df: pd.DataFrame, pipeline: str):\n",
    "    # We need to calculate the following metrics:\n",
    "    # Sens = TP / (TP + FN)\n",
    "    # Spec = TN / (TN + FP)\n",
    "    # Prec = TP / (TP + FP)\n",
    "    # Acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    # F1 = 2 * (Prec * Sens) / (Prec + Sens)\n",
    "    # DOR = Sens * Spec / ((1 - Sens) * (1 - Spec))\n",
    "\n",
    "    def ignore_nan_hmean(x: ArrayLike) -> float:\n",
    "        # Drop NaN values\n",
    "        x = x[~np.isnan(x)]\n",
    "        # Then, compute hmean.\n",
    "        return hmean(x)\n",
    "\n",
    "    df[\"sens\"] = df[\"tp\"] / (df[\"tp\"] + df[\"fn\"])\n",
    "    df[\"spec\"] = df[\"tn\"] / (df[\"tn\"] + df[\"fp\"])\n",
    "    df[\"prec\"] = df[\"tp\"] / (df[\"tp\"] + df[\"fp\"])\n",
    "    df[\"acc\"] = (df[\"tp\"] + df[\"tn\"]) / (df[\"tp\"] + df[\"tn\"] + df[\"fp\"] + df[\"fn\"])\n",
    "    df[\"f1\"] = 2 * (df[\"prec\"] * df[\"sens\"]) / (df[\"prec\"] + df[\"sens\"])\n",
    "    # df[\"dor\"] = df[\"sens\"] * df[\"spec\"] / ((1 - df[\"sens\"]) * (1 - df[\"spec\"]))\n",
    "\n",
    "    # We want to calculate the harmonic mean of each column. \n",
    "    # Unfortunately, there can be NaN values, so we need to ignore those if present in a column.\n",
    "    df.loc[\"Harmonic Mean\"] = df[[\"sens\", \"spec\", \"prec\", \"acc\", \"f1\"]].apply(lambda x: ignore_nan_hmean(x), axis=0)\n",
    "    df.loc[\"Mean\"] = df[[\"sens\", \"spec\", \"prec\", \"acc\", \"f1\"]].mean(axis=0, skipna=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(threshold: float):\n",
    "    dfs = {}\n",
    "    for piv in pivot_df(make_dataframes()):\n",
    "        pipeline_name = piv[1]\n",
    "        cf_df = confusion_matrix(piv[0], threshold)\n",
    "        cf_df.set_index(\"species\", inplace=True)\n",
    "        cf_df = performance_metrics(cf_df, pipeline_name)\n",
    "        cf_df[\"Threshold\"] = threshold\n",
    "\n",
    "        display(cf_df)\n",
    "\n",
    "        dfs[pipeline_name] = cf_df\n",
    "\n",
    "    with pd.ExcelWriter(f\"confusion_matrix_{threshold}.xlsx\") as writer:\n",
    "        for key in dfs.keys():\n",
    "            data: pd.DataFrame = dfs[key]\n",
    "\n",
    "            # Drop index name and make it a column.\n",
    "            data.reset_index(inplace=True)\n",
    "\n",
    "            data.to_excel(writer, sheet_name=key)\n",
    "\n",
    "            tex_output = os.path.join(\"tex\", str(threshold), f\"confusion_matrix_{key}.tex\")\n",
    "            tex_caption = f\"Confusion matrix for {key} at threshold {threshold}.\"\n",
    "            tex_label = f\"tab:conf_{key}_{threshold}\"\n",
    "\n",
    "            # We need to split the species names into two words on the underscore.\n",
    "            data[\"species\"] = data[\"species\"].str.split(\"_\").str.join(\" \")\n",
    "\n",
    "            # Then, capitalize the first letter of each word.\n",
    "            data[\"species\"] = data[\"species\"].str.title()\n",
    "\n",
    "            # All of the index names should be in italics. We should use the CSS styler to do this.\n",
    "            # data.index = data.index.to_frame().style.set_properties(**{\"text-align\": \"left\", \"font-style\": \"italic\"}).render()\n",
    "            s = data.style.format_index(lambda x: f\"\\\\textbf{{{x}}}\", axis=1)\n",
    "\n",
    "            # We want a rule between the header and the data.\n",
    "            s.set_table_styles([\n",
    "                {\"selector\": 'midrule', 'props': ':hline;'},\n",
    "                {\"selector\": 'toprule', 'props': ':hline;'},\n",
    "            ])\n",
    "\n",
    "            # s = data.style.format(lambda x: f\"\\\\textit{{{x}}}\", subset=[\"species\"])\n",
    "            s.format(\"{:.0f}\", subset=[\"tp\", \"fp\", \"fn\", \"tn\"], na_rep=\"-\")\n",
    "            s.format(\"{:.3f}\", subset=[\"sens\", \"spec\", \"prec\", \"acc\", \"f1\"], na_rep=\"-\")\n",
    "            s.format(\"{:.4f}\", subset=[\"Threshold\"], na_rep=\"-\")\n",
    "\n",
    "            # We need to unitalicize and bold the Harmonic Mean and Mean rows. These are always the last two rows.\n",
    "            # s.set_properties(**{\"text-align\": \"left\", \"font-style\": \"normal\", \"font-weight\": \"bold\"}, subset=pd.IndexSlice[-2:, :])\n",
    "            def bold_means(v):\n",
    "                if v == \"Harmonic Mean\" or v == \"Mean\":\n",
    "                    return \"font-weight: bold; font-style: normal;\"\n",
    "                else:\n",
    "                    return \"font-style: italic;\"\n",
    "\n",
    "            s.applymap(bold_means, subset=[\"species\"])\n",
    "\n",
    "            s.hide(axis=\"index\")\n",
    "\n",
    "            # Replace NaN values with a dash.\n",
    "            # Make the table small size.\n",
    "\n",
    "            display(data)\n",
    "\n",
    "            s.to_latex(tex_output, position_float=\"centering\", convert_css=True, position=\"H\")\n",
    "\n",
    "            with open(tex_output, \"r+\") as f:\n",
    "                # text = f.read().replace(\"\\\\begin{tabular}\", \"\\small\\n\\\\begin{tabular}\")\n",
    "                replacement_caption = f\"\\\\end{{tabular}}\\n\\\\caption{{{tex_caption}}}\\n\\\\label{{{tex_label}}}\"\n",
    "                caption = f.read().replace(\"\\\\end{tabular}\", replacement_caption)\n",
    "                # f.seek(0)\n",
    "                # f.write(text)\n",
    "                f.seek(0)\n",
    "                f.write(caption)\n",
    "                f.close()\n",
    "\n",
    "main(0.0)\n",
    "main(1E-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.pipeline_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ac10c2973d938bf4f101ae2299756abbb7e00dac649f0769819439ff384650d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
