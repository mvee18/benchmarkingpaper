{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.ncbi.names import split_bio, map_and_add_tax_ids, standardize_core, generate_names_df, names_db_path, find_tax_id\n",
    "from typing import Tuple\n",
    "from utils.data_paths import *\n",
    "\n",
    "taxonomy_dict = {\"s\": \"species\", \"g\": \"genus\", \"f\": \"family\", \"o\": \"order\", \"c\": \"class\", \"p\": \"phylum\", \"k\": \"kingdom\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = {\"Clostridium_clostridioforme\": \"Clostridium_clostridiiforme\"}\n",
    "\n",
    "split_name_replacement_dict = {\n",
    "    \"LACHNOSPIRACEAE_UNCLASSIFIED\": \"UNCLASSIFIED_LACHNOSPIRACEAE\",\n",
    "    \"CLOSTRIDIALES_FAMILY_XIII-INCERTAE-SEDIS-UNCLASSIFIED\":\"CLOSTRIDIALES_FAMILY_XIII--INCERTAE-SEDIS\",\n",
    "    \"ACTINOMYCES_ODONTOLYTICUS\": \"SCHAALIA_ODONTOLYTICA\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a csv file with genus, relative abundance.\n",
    "def clean_biobakery_with_taxid(df, rank=\"g\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    data = df.copy()\n",
    "\n",
    "    # Get the indices so we can split then. They are of the form ex: ...|g__Bacteroides|s__Bacteroides_vulgatus\n",
    "    indices = data.index.to_list()\n",
    "    # Tax_ids are of the form #|#|# ...\n",
    "    tax_ids = data[\"TAX_ID\"].to_list()\n",
    "\n",
    "    # This makes a list of lists, where each sublist is the taxonomy split by the rank.\n",
    "    splitted = [i.split(\"|\") for i in indices]\n",
    "    splitted_ids = [i.split(\"|\") for i in tax_ids]\n",
    "\n",
    "    new_index = []\n",
    "    new_ids = []\n",
    "    # Traverse all of the rows.\n",
    "    for c, i in enumerate(splitted):\n",
    "        # Traverse the sublists.\n",
    "        for c2, j in enumerate(i):\n",
    "            # If it matches the rank we want, then append it to the new index.\n",
    "            if j.startswith(f\"{rank}__\"):\n",
    "                new_index.append(j.replace(f\"{rank}__\", \"\"))\n",
    "                # The taxid is the same index as the taxonomy.\n",
    "                new_ids.append(splitted_ids[c][c2])\n",
    "\n",
    "    # Set the new index.\n",
    "    data.index = new_index\n",
    "\n",
    "    # Make a new dataframe with the new index and the new_ids. We will use this to merge since we do not want to sum the tax_ids.\n",
    "    taxid_df = pd.DataFrame(index=new_index, data=new_ids, columns=[\"TAX_ID\"])\n",
    "\n",
    "    # Sum the rows where the genus/species is the same.\n",
    "    grouped = data.groupby(data.index).sum(numeric_only=True)\n",
    "\n",
    "    # Now, we want to rename the columns to be the sample names, since they are of the form sampleid_###.\n",
    "    columns = grouped.columns.to_list()\n",
    "    new_cols = [i.split(\"_\")[0] for i in columns]\n",
    "    grouped.columns = new_cols\n",
    "\n",
    "    # Divide all the values by 100.\n",
    "    grouped = grouped / 100\n",
    "\n",
    "    # We should now remove duplicates in the taxIDs (since genera can be equivalent). \n",
    "    # If there are duplicates, then we should make it equal to the first value.\n",
    "    taxid_df = taxid_df[~taxid_df.index.duplicated(keep=\"first\")]\n",
    "\n",
    "    return grouped, taxid_df\n",
    "\n",
    "# We want to save to csv, but we want a csv for each column.\n",
    "def save_to_csv(df, taxid_df, output_path, rank=\"s\"):\n",
    "    # Get the columns.\n",
    "    columns = df.columns.to_list()\n",
    "\n",
    "    # Iterate over the columns.\n",
    "    for c, i in enumerate(columns):\n",
    "        # Get the column.\n",
    "        col = df[[i]]\n",
    "\n",
    "        # Join the tax_ids to the column.\n",
    "        # Now, we want to add the new_ids to the dataframe by joining on the index.\n",
    "        col = col.join(taxid_df, how=\"left\")\n",
    "\n",
    "        # TODO: Figure out why I did this in the first place, removing for now.\n",
    "        # We need to multiply each column by the reciprocal of the minimum non-zero value.\n",
    "\n",
    "        # # Get the minimum non-zero value.\n",
    "        # minimum = col[col > 0].min().min()\n",
    "        # col[i] = col[i] / minimum\n",
    "\n",
    "        # # Round to closest integer.\n",
    "        # col[i] = col[i].round()\n",
    "\n",
    "        # display(col.head())\n",
    "\n",
    "        # Save to csv.\n",
    "        col.to_csv(os.path.join(output_path, f\"{i.upper()}_{taxonomy_dict[rank]}_relabund_annotated.csv\"), index_label=f\"{taxonomy_dict[rank]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In their great wisdom, they decided that the taxid should not be incldued in the file. \n",
    "# Therefore, we will need to standardize the names and attach the taxids.\n",
    "def clean_biobakery_merged(df: pd.DataFrame, rank=\"g\"):\n",
    "    \"\"\"\n",
    "    Cleans the merged biobakery file (after cutting into species_relab.txt) and attached TAX_IDs.\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "\n",
    "    # Get the indices so we can split then. They are of the form ex: ...|g__Bacteroides|s__Bacteroides_vulgatus\n",
    "    indices = data.index.to_list()\n",
    "\n",
    "    # This makes a list of lists, where each sublist is the taxonomy split by the rank.\n",
    "    splitted = [i.split(\"|\") for i in indices]\n",
    "    # print(\"splitted: \", splitted)\n",
    "\n",
    "    new_index = []\n",
    "    # Traverse all of the rows.\n",
    "    for c, i in enumerate(splitted):\n",
    "        # Traverse the sublists.\n",
    "        for c2, j in enumerate(i):\n",
    "            # If it matches the rank we want, then append it to the new index.\n",
    "            if j.startswith(f\"{rank}__\"):\n",
    "                new_index.append(j.replace(f\"{rank}__\", \"\"))\n",
    "\n",
    "    # print(new_index)\n",
    "    # Set the new index.\n",
    "    data.index = new_index\n",
    "\n",
    "    # Sum the rows where the genus/species is the same.\n",
    "    grouped = data.groupby(data.index).sum(numeric_only=True)\n",
    "\n",
    "    # Now, we want to rename the columns to be the sample names, since they are of the form sampleid_###.\n",
    "    columns = grouped.columns.to_list()\n",
    "    new_cols = [i.split(\"_\")[0] for i in columns]\n",
    "    grouped.columns = new_cols\n",
    "\n",
    "    # Divide all the values by 100.\n",
    "    grouped = grouped / 100\n",
    "\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_merged(df: pd.DataFrame):\n",
    "    \"\"\" \n",
    "    This function takes the merged dataframe (converted to RA with clean_biobakery_merged) \\  \n",
    "    and standardizes the names plus adds the taxids.\n",
    "    \"\"\"\n",
    "    names_df = generate_names_df(names_db_path, load_pickle=True)\n",
    "\n",
    "    # Get the index. Replace names with those in replacement_dict.\n",
    "    index = df.index.to_list()\n",
    "    index = [replacement_dict[i] if i in replacement_dict else i for i in index]\n",
    "    df.index = index\n",
    "\n",
    "    split_names = split_bio(df)\n",
    "\n",
    "    standard_df = standardize_core(input_df=df, split_names=split_names)\n",
    "\n",
    "    # Replace the split_name using the replacement_dict.\n",
    "    standard_df.set_index(\"split_name\", inplace=True)\n",
    "\n",
    "    # Replace any split_names that are in the replacement_dict.\n",
    "    standard_df.index = [split_name_replacement_dict[i] if i in split_name_replacement_dict else i for i in standard_df.index]\n",
    "\n",
    "    annotated = map_and_add_tax_ids(df=standard_df, names_df=names_df)\n",
    "\n",
    "    # We want to split the tax_id column off the dataframe.\n",
    "    taxid_df = annotated[[\"tax_id\"]]\n",
    "\n",
    "    # If any rows contain nan, then we raise an error.\n",
    "    if taxid_df.isna().any().any():\n",
    "        # Print the rows that contain nan.\n",
    "        display(taxid_df[taxid_df.isna().any(axis=1)])\n",
    "        raise ValueError(\"There are nan values in the tax_id column.\")\n",
    "\n",
    "    annotated.drop(columns=[\"tax_id\"], inplace=True)\n",
    "\n",
    "    return annotated, taxid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code\n",
    "Run the below function for cleaning of biobakery. Change rank for the desired rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = hmpTongue.biobakery4\n",
    "# # data_path = \"pipelines/bmock12/biobakery4/species_relab.txt\"\n",
    "# output_dir = \"pipelines/hmp/tongue/bio4/\"\n",
    "# if not os.path.exists(data_path):\n",
    "#     raise Exception(\"Data file does not exist!\")\n",
    "# # data_path = \"pipelines/bmock12/biobakery4/species_relab.txt\"\n",
    "# data = pd.read_csv(data_path, sep=\"\\t\", index_col=0)\n",
    "# # display(data.head())\n",
    "# # print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank = \"s\"\n",
    "# names, taxids = clean_biobakery_with_taxid(data, rank=rank)\n",
    "# save_to_csv(names, taxids, output_dir, rank=rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/valenciaem/coding/pipelines/pipelines/bmock12/biobakery4\n",
      "/Volumes/TBHD_share/valencia/pipelines/bmock12/biobakery4/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Volumes/TBHD_share/valencia/pipelines/bmock12/biobakery4/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Users/valenciaem/coding/pipelines/pipelines/camisimGI/biobakery4\n",
      "/Volumes/TBHD_share/cami_data/bio4/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Volumes/TBHD_share/cami_data/bio4/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Users/valenciaem/coding/pipelines/pipelines/tourlousse/biobakery4\n",
      "/Volumes/TBHD_share/valencia/pipelines/microbio_spectrum/CLEANED/pipelines/bio4/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Volumes/TBHD_share/valencia/pipelines/microbio_spectrum/CLEANED/pipelines/bio4/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Users/valenciaem/coding/pipelines/pipelines/amos/mixed/biobakery4\n",
      "/Volumes/TBHD_share/valencia/pipelines/amos/nibsc/mixed/bio4/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Volumes/TBHD_share/valencia/pipelines/amos/nibsc/mixed/bio4/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Users/valenciaem/coding/pipelines/pipelines/amos/hilo/biobakery4\n",
      "/Volumes/TBHD_share/valencia/pipelines/amos/nibsc/hilo/bio4/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Volumes/TBHD_share/valencia/pipelines/amos/nibsc/hilo/bio4/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Users/valenciaem/coding/pipelines/pipelines/hmp/gut/biobakery4\n",
      "/Users/valenciaem/coding/pipelines/pipelines/nist/biobakery4\n",
      "/Volumes/TBHD_share/valencia/pipelines/NIST/pipelines/bio4/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Volumes/TBHD_share/valencia/pipelines/NIST/pipelines/bio4/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Users/valenciaem/coding/pipelines/pipelines/bmock12/biobakery3\n",
      "/Volumes/TBHD_share/valencia/pipelines/bmock12/biobakery3/metaphlan/main/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Volumes/TBHD_share/valencia/pipelines/bmock12/biobakery3/metaphlan/main/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Users/valenciaem/coding/pipelines/pipelines/camisimGI/biobakery3\n",
      "/Volumes/TBHD_share/cami_data/NOADAPTERS/pipelines/bio3/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Volumes/TBHD_share/cami_data/NOADAPTERS/pipelines/bio3/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Users/valenciaem/coding/pipelines/pipelines/tourlousse/biobakery3\n",
      "/Volumes/TBHD_share/valencia/pipelines/microbio_spectrum/CLEANED/pipelines/bio3/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Volumes/TBHD_share/valencia/pipelines/microbio_spectrum/CLEANED/pipelines/bio3/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Users/valenciaem/coding/pipelines/pipelines/amos/mixed/biobakery3\n",
      "/Volumes/TBHD_share/valencia/pipelines/amos/nibsc/mixed/bio3/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Volumes/TBHD_share/valencia/pipelines/amos/nibsc/mixed/bio3/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Users/valenciaem/coding/pipelines/pipelines/amos/hilo/biobakery3\n",
      "/Volumes/TBHD_share/valencia/pipelines/amos/nibsc/hilo/bio3/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Volumes/TBHD_share/valencia/pipelines/amos/nibsc/hilo/bio3/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Users/valenciaem/coding/pipelines/pipelines/hmp/gut/biobakery3\n",
      "/Volumes/TBHD_share/valencia/pipelines/HMP/gut/bio3/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Volumes/TBHD_share/valencia/pipelines/HMP/gut/bio3/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Users/valenciaem/coding/pipelines/pipelines/nist/biobakery3\n",
      "/Volumes/TBHD_share/valencia/pipelines/NIST/pipelines/bio3/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Volumes/TBHD_share/valencia/pipelines/NIST/pipelines/bio3/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n"
     ]
    }
   ],
   "source": [
    "dpths = make_data_list()\n",
    "\n",
    "# We have an issue with the HMP. Let's make a replacement dictionary to fix this for now.\n",
    "hmp_dict = {\n",
    "    \"Clostridiales_unclassified\": \"Unclassified\",\n",
    "    \"Firmicutes_unclassified\": \"Unclassified\",\n",
    "    \"Lachnospiraceae_unclassified\": \"Unclassified\",\n",
    "    \"Ruminococcaceae_unclassified\": \"Unclassified\",\n",
    "    # \"Ruminococcaceae_unclassified\": \"Unclassified\",\n",
    "}\n",
    "\n",
    "amos_dict = {\n",
    "    \"Prevotella_copri_clade_A\": \"Prevotella_copri\",\n",
    "}\n",
    "\n",
    "def main(data_path: str, output_dir: str, rank: str, replace_dict: dict = None) -> None:\n",
    "    print(data_path)\n",
    "    data = pd.read_csv(data_path, sep=\"\\t\", index_col=0)\n",
    "    # display(data)\n",
    "\n",
    "    cleaned = clean_biobakery_merged(data, rank=rank)\n",
    "        \n",
    "    if replace_dict is not None:\n",
    "        cleaned.rename(index=replace_dict, inplace=True)\n",
    "\n",
    "    annotated, ids = standardize_merged(cleaned)\n",
    "\n",
    "    # display(annotated)\n",
    "\n",
    "    save_to_csv(annotated, ids, output_path=output_dir, rank=rank)\n",
    "\n",
    "def clean_all():\n",
    "    for i in dpths:\n",
    "        output_path = os.path.join(i.path, \"bio4\")\n",
    "        if not os.path.exists(output_path):\n",
    "            output_path = os.path.join(i.path, \"biobakery4\")\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        print(output_path)\n",
    "        if i.biobakery4 == \"\":\n",
    "            continue\n",
    "        \n",
    "        if \"hmp\" in i.path:\n",
    "            replace_dict = hmp_dict\n",
    "        elif \"mixed\" in i.path or \"hilo\" in i.path:\n",
    "            replace_dict = amos_dict\n",
    "        else:\n",
    "            replace_dict = None\n",
    "\n",
    "        main(i.biobakery4, output_dir=output_path, rank=\"g\", replace_dict=replace_dict)\n",
    "        main(i.biobakery4, output_dir=output_path, rank=\"s\", replace_dict=replace_dict)\n",
    "\n",
    "    for i in dpths:\n",
    "        output_path = os.path.join(i.path, \"bio3\")\n",
    "        if not os.path.exists(output_path):\n",
    "            output_path = os.path.join(i.path, \"biobakery3\")\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        print(output_path)\n",
    "        if i.biobakery3 == \"\":\n",
    "            continue\n",
    "        \n",
    "        if \"hmp\" in i.path:\n",
    "            replace_dict = hmp_dict\n",
    "        elif \"mixed\" in i.path or \"hilo\" in i.path:\n",
    "            replace_dict = amos_dict\n",
    "        else:\n",
    "            replace_dict = None\n",
    "\n",
    "        main(i.biobakery3, output_dir=output_path, rank=\"g\", replace_dict=replace_dict)\n",
    "        main(i.biobakery3, output_dir=output_path, rank=\"s\", replace_dict=replace_dict)\n",
    "\n",
    "clean_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/TBHD_share/valencia/pipelines/NIST/pipelines/bio3_eg/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n",
      "/Volumes/TBHD_share/valencia/pipelines/NIST/pipelines/bio3_eg/metaphlan/merged/species_relab.txt\n",
      "The pkl file was last modified (and hopefully generated) on 2023-02-14 18:37:13.183117+00:00\n"
     ]
    }
   ],
   "source": [
    "def clean_eg():\n",
    "    # We also have to add the bio_eg.\n",
    "    bio_eg_path = \"/Volumes/TBHD_share/valencia/pipelines/NIST/pipelines/bio3_eg/metaphlan/merged/species_relab.txt\"\n",
    "    output_eg = \"../../pipelines/nist/biobakery3/\"\n",
    "\n",
    "    main(bio_eg_path, output_eg, rank=\"g\")\n",
    "    main(bio_eg_path, output_eg, rank=\"s\")\n",
    "\n",
    "clean_eg()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.pipeline_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ac10c2973d938bf4f101ae2299756abbb7e00dac649f0769819439ff384650d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
