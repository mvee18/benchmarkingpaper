{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import openpyxl\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from utils.ncbi.jams_convert import convert_jams_to_taxid, generate_names_df, names_db_path\n",
    "from utils.data_paths import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = {\"LKT__s__Anaerobutyricum_hallii\": \"LKT__s__Eubacterium_hallii\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update replacement dict for JAMS.\n",
    "\n",
    "camisim_replacement_dict = {\n",
    "    \"Acetivibrio_thermocellus\": \"Ruminiclostridium_thermocellum\", \n",
    "    \"Thermoclostridium_stercorarium\": \"Ruminiclostridium_stercorarium\",\n",
    "}\n",
    "\n",
    "def clean_jams(input_file: str, rank: str = \"Genus\", input_type=\"csv\"):\n",
    "    \"\"\"\n",
    "    This function cleans the output from JAMSalpha in the bmock12 dataset. From now on, use the JAMSbeta function.\n",
    "    \"\"\"\n",
    "\n",
    "    output_dir = os.path.dirname(input_file)\n",
    "    file_name = os.path.basename(input_file).split(\".\")[0]\n",
    "    csv_path = os.path.join(output_dir, f\"{file_name.upper()}_{rank.lower()}_relabund.csv\")\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    if input_type == \"csv\":\n",
    "        df = pd.read_csv(input_file, index_col=0)\n",
    "    elif input_type == \"excel\":\n",
    "        df = pd.read_excel(input_file, index_col=0)\n",
    "    else:\n",
    "        raise Exception(\"Input type not recognized.\")\n",
    "    \n",
    "    df[\"RA\"] = df[\"NumBases\"] / df[\"NumBases\"].sum()\n",
    "    # display(df.head())\n",
    "    species_df = df[[\"Species\", \"RA\"]].groupby(\"Species\").sum()\n",
    "    species_df.sort_values(\"RA\", ascending=False, inplace=True)\n",
    "\n",
    "    # We need to remove g__ and s__ from the index names\n",
    "    # genus_df.index = genus_df.index.str.replace(\"g__\", \"\")\n",
    "    species_df.index = species_df.index.str.replace(\"s__\", \"\")\n",
    "\n",
    "    names_df = generate_names_df(names_db_path, load_pickle=True)\n",
    "\n",
    "    if rank == \"Genus\":\n",
    "        # We need to split the species names into genus and species on the _ character.\n",
    "        species_names = species_df.index.to_list()\n",
    "        genus_names = [x.split(\"_\")[0] for x in species_names]\n",
    "\n",
    "        species_df[\"Genus\"] = genus_names\n",
    "\n",
    "        genus_df = species_df[[\"Genus\", \"RA\"]].groupby(\"Genus\").sum()\n",
    "\n",
    "        genus_df.sort_values(\"RA\", ascending=False, inplace=True)\n",
    "\n",
    "        # genus_df.to_csv(csv_path)\n",
    "\n",
    "        annotated, unannotated = convert_jams_to_taxid(genus_df, names_df)\n",
    "        annotated.to_csv(csv_path.replace(\".csv\", \"_annotated.csv\"), index_label=rank)\n",
    "\n",
    "        return\n",
    "\n",
    "    annotated, unannotated = convert_jams_to_taxid(species_df, names_df)\n",
    "    annotated.to_csv(csv_path.replace(\".csv\", \"_annotated.csv\"), index_label=rank)\n",
    "\n",
    "# clean_jams(input_file = \"../../pipelines/bmock12/jams/s1.csv\", rank = \"Species\", input_type = \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_jams_alpha(input_file: str, rank: str = \"Genus\", input_type=\"csv\"):\n",
    "    \"\"\"\n",
    "    This function cleans the output from JAMSalpha in the bmock12 dataset. From now on, use the JAMSbeta function.\n",
    "    \"\"\"\n",
    "\n",
    "    output_dir = os.path.dirname(input_file)\n",
    "    file_name = os.path.basename(input_file).split(\".\")[0]\n",
    "    csv_path = os.path.join(output_dir, f\"{file_name.upper()}_{rank.lower()}_relabund.csv\")\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    if input_type == \"csv\":\n",
    "        df = pd.read_csv(input_file, index_col=0)\n",
    "    elif input_type == \"excel\":\n",
    "        df = pd.read_excel(input_file, index_col=0)\n",
    "    else:\n",
    "        raise Exception(\"Input type not recognized.\")\n",
    "    \n",
    "    df[\"RA\"] = df[\"NumBases\"] / df[\"NumBases\"].sum()\n",
    "\n",
    "    rank_df = df[[rank, \"RA\"]].groupby(rank).sum()\n",
    "\n",
    "    cleaned_names = [i.split(\"__\")[1] for i in rank_df.index]\n",
    "    rank_df.index = cleaned_names\n",
    "    rank_df.index.name = rank\n",
    "\n",
    "    rank_df.sort_values(\"RA\", ascending=False, inplace=True)\n",
    "\n",
    "    names_df = generate_names_df(names_db_path, load_pickle=True)\n",
    "    annotated, unannotated = convert_jams_to_taxid(rank_df, names_df)\n",
    "    annotated.to_csv(csv_path.replace(\".csv\", \"_annotated.csv\"), index_label=rank)\n",
    "\n",
    "    display(annotated)\n",
    "\n",
    "clean_jams_alpha(input_file = \"../../pipelines/bmock12/jams/s1.csv\", rank = \"Genus\", input_type = \"csv\")\n",
    "clean_jams_alpha(input_file = \"../../pipelines/bmock12/jams/s1.csv\", rank = \"Species\", input_type = \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jams_to_csv(df: pd.DataFrame, taxid_df: pd.DataFrame, output_dir: str, rank: str):\n",
    "    # Save each column as a separate file.\n",
    "    columns = df.columns.to_list()\n",
    "    for c, i in enumerate(columns):\n",
    "        col = df[[i]]\n",
    "        col = col.join(taxid_df, how=\"left\")\n",
    "\n",
    "        # col.astype({\"tax_id\": \"int64\"})\n",
    "        col[\"tax_id\"] = col[\"tax_id\"].astype(\"int64\")\n",
    "\n",
    "        col.sort_values(i, ascending=False, inplace=True)\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"{i.upper()}_{rank}_relabund_annotated.csv\")\n",
    "\n",
    "        col.to_csv(output_path, index_label=rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_jams_join(input_file: str, rank: str, output_dir=\"\") -> None:\n",
    "    \"\"\" \n",
    "    This function cleans jams, but uses the LKT_featuretable sheet to join the taxonomy to the relabund.\n",
    "    Parameters:\n",
    "        input_file: The path to the input file.\n",
    "        rank: The rank to use for the output file.\n",
    "        output_dir: The directory to save the output file to.\n",
    "    \"\"\"\n",
    "    rank = rank.capitalize()\n",
    "    print(input_file)\n",
    "    relabund_df = pd.read_excel(input_file, index_col=0, sheet_name=1)\n",
    "    featuretable_df = pd.read_excel(input_file, index_col=0, sheet_name=\"LKT_featuretable\")\n",
    "    \n",
    "    # Make everything into relative abundances (i.e. pct).\n",
    "    relabund_df = relabund_df / relabund_df.sum(axis=0)\n",
    "\n",
    "    joined = relabund_df.join(featuretable_df, how=\"inner\")\n",
    "\n",
    "    # The two dataframes should have the same number of rows.\n",
    "    assert relabund_df.shape[0] == joined.shape[0], \"The relabund and featuretable dfs should have the same number of rows.\"\n",
    "\n",
    "    # Set the index to the rank.\n",
    "    joined.set_index(rank, inplace=True)\n",
    "\n",
    "    # Drop any columns that are not from the relabund_df.\n",
    "    joined.drop(columns=[i for i in joined.columns if i not in relabund_df.columns], inplace=True)\n",
    "\n",
    "    # Reset the index so that we can groupby the rank.\n",
    "    joined.reset_index(inplace=True)\n",
    "\n",
    "    # Now, we can groupby the rank and sum the relabund, while keeping the genus as first.\n",
    "    agg_dict = {i: \"sum\" for i in relabund_df.columns}\n",
    "    agg_dict[rank] = \"first\"\n",
    "\n",
    "    joined = joined.groupby(rank).agg(agg_dict)\n",
    "    joined.set_index(rank, inplace=True)\n",
    "\n",
    "    # Now, we need to split off the first three characters from the index.\n",
    "    # This is the cleaned name.\n",
    "    cleaned_names = [i.split(\"__\")[1] for i in joined.index]\n",
    "    joined.index = cleaned_names\n",
    "    joined.index.name = rank\n",
    "\n",
    "    # Now, we can run it through the annotation pipeline.\n",
    "    names_df = generate_names_df(names_db_path, load_pickle=True)\n",
    "    annotated, unannotated = convert_jams_to_taxid(joined.copy(), names_df)\n",
    "\n",
    "    taxid_df = annotated[[\"tax_id\"]]\n",
    "    annotated.drop(columns=[\"tax_id\"], inplace=True)\n",
    "\n",
    "    save_jams_to_csv(annotated, taxid_df, output_dir, rank.lower())\n",
    "\n",
    "# clean_jams_join(\"/Volumes/TBHD_share/valencia/pipelines/amos/nibsc/hilo/jams2022/beta_output/hilo_Relabund_PPM.xlsx\", \"Genus\")\n",
    "# clean_jams_join(\"/Volumes/TBHD_share/valencia/pipelines/amos/nibsc/hilo/jams2022/beta_output/hilo_Relabund_PPM.xlsx\", \"Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "# rank = \"species\"\n",
    "paths = make_data_list()\n",
    "for p in paths:\n",
    "    # bmock12 uses a single sample, so jamsbeta was not run. \n",
    "    # Use the clean_jams function on the csv dump from the R session.\n",
    "    if \"bmock12\" in p.path:\n",
    "        continue\n",
    "\n",
    "    if p.jams != \"\":\n",
    "        clean_jams_join(p.jams, rank=\"genus\", output_dir=os.path.join(p.path, \"jams\"))\n",
    "        clean_jams_join(p.jams, rank=\"species\", output_dir=os.path.join(p.path, \"jams\"))\n",
    "\n",
    "    if p.jams202212 != \"\":\n",
    "        clean_jams_join(p.jams202212, rank=\"genus\", output_dir=os.path.join(p.path, \"jams202212\"))\n",
    "        clean_jams_join(p.jams202212, rank=\"species\", output_dir=os.path.join(p.path, \"jams202212\"))\n",
    "\n",
    "# data_path = hmpTongue.jams\n",
    "# output_dir = \"pipelines/hmp/tongue/jams\"\n",
    "\n",
    "# clean_jams_beta(data_path, rank=\"species\", output_dir=output_dir)\n",
    "# clean_jams_beta(data_path, rank=\"genus\", output_dir=output_dir)\n",
    "# clean_jams_beta_higher(\"/Volumes/NRTS_share/SMS_NIAID_0162/fqfiles/Batch1/jams/brain_jams/brainjams_Relabund_PPM.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.pipeline_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ac10c2973d938bf4f101ae2299756abbb7e00dac649f0769819439ff384650d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
