{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from biom.table import Table\n",
    "# from biom import load_table\n",
    "import pandas as pd\n",
    "import os\n",
    "from utils.ncbi.names import generate_names_df, names_db_path, standardize_core\n",
    "from utils.ncbi.jams_convert import fix_name, convert_jams_to_taxid\n",
    "from utils.data_paths import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the TEDreadsTAX reports in TAXprofiles/TEDreadsTAX/reports.\n",
    "data = hmpTongue.wgsa\n",
    "output_path = \"pipelines/hmp/tongue/wgsa2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The genus was changed, but the Amos paper uses the original genus name.\n",
    "# replacement_dict = {\"Anaerobutyricum hallii\": \"Eubacterium hallii\", \"Anaerobutyricum\": \"Eubacterium\"}\n",
    "# For the Amos paper, the genus was changed to Eubacterium. This is the only result, so we can just replace it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_wgsa(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Standardizes the WGSA data using the standardize_core function.\n",
    "    \"\"\"\n",
    "    names_df = generate_names_df(names_db_path, load_pickle=True)\n",
    "\n",
    "    # We can use the convert_jams_to_taxid function from the utils.ncbi.convert_jams since the format is the same.\n",
    "    ann, unann = convert_jams_to_taxid(df.head(50), names_df)\n",
    "\n",
    "    return ann, unann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_dict = {\"Genus\": \"G\", \"Species\": \"S\", \"Family\": \"F\", \"Order\": \"O\", \"Class\": \"C\", \"Phylum\": \"P\", \"Kingdom\": \"K\"}\n",
    "\n",
    "def clean_and_parse_wgsa(data_path, output_dir, rank=\"Genus\", left_prefix=\"\"):\n",
    "    df = pd.read_csv(data_path, sep=\"\\t\", header=None, usecols=[1, 3, 4, 5])\n",
    "\n",
    "    # Split off the first two rows.\n",
    "    total_counts = df.iloc[:2, 0].sum()\n",
    "\n",
    "    df = df.where(df[3] == tax_dict[rank]).dropna()\n",
    "    df.sort_values(by=1, ascending=False, inplace=True)\n",
    "\n",
    "    clean_genus = df[[5, 1, 4]].copy(deep=True)\n",
    "    clean_genus.columns = [rank, \"RA\", \"TAX_ID\"]\n",
    "\n",
    "    # Convert the RA column from counts to RA.\n",
    "    clean_genus[\"RA\"] = clean_genus[\"RA\"].apply(lambda x: x / total_counts)\n",
    "\n",
    "    clean_genus.set_index(rank, inplace=True)\n",
    "\n",
    "    indices = clean_genus.index\n",
    "    indices = [i.lstrip() for i in indices]\n",
    "\n",
    "    # Remove any [ and ] characters from the indices.\n",
    "    indices = [i.replace(\"[\", \"\") for i in indices]\n",
    "    indices = [i.replace(\"]\", \"\") for i in indices]\n",
    "\n",
    "    clean_genus.index = indices\n",
    "\n",
    "    # Since the WGSA data already contains the TAXID, it is much faster than this function.\n",
    "    # ann, uann = standardize_wgsa(clean_genus)\n",
    "\n",
    "    clean_genus = clean_genus.astype({\"TAX_ID\": int})\n",
    "\n",
    "    prefix = os.path.basename(data_path).split(\"_\")[0]\n",
    "    # left_prefix = \"s\"\n",
    "    output_file = os.path.join(output_dir, left_prefix + prefix + \"_\" + rank.lower() + \"_\" + \"relabund_annotated.csv\")\n",
    "\n",
    "    clean_genus.to_csv(output_file, sep=\",\", header=True, index_label=rank)\n",
    "\n",
    "# clean_and_parse_wgsa(cami_sim_data, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There may be more than one output file, so we need to combine them.\n",
    "def combine_files(data_path: str, rank: str):\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(\"The data path does not exist.\")\n",
    "\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        print(files)\n",
    "        if len(files) == 0:\n",
    "            raise Exception(\"No files found in output directory.\")\n",
    "            \n",
    "        for file in files:\n",
    "            if \"REPORT\" in file:\n",
    "                print(os.path.join(root, file))\n",
    "                clean_and_parse_wgsa(os.path.join(root, file), output_path, rank=rank)\n",
    "\n",
    "combine_files(data, \"Genus\")\n",
    "combine_files(data, \"Species\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.pipeline_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ac10c2973d938bf4f101ae2299756abbb7e00dac649f0769819439ff384650d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
